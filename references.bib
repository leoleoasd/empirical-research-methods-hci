@article{abdelghaniGPT3drivenPedagogicalAgents2023,
  title = {{{GPT-3-driven}} Pedagogical Agents for Training Children's Curious Question-Asking Skills},
  author = {Abdelghani, Rania and Wang, Yen-Hsiang and Yuan, Xingdi and Wang, Tong and Lucas, Pauline and Sauz{\'e}on, H{\'e}l{\`e}ne and Oudeyer, Pierre-Yves},
  year = {2023},
  month = jun,
  journal = {International Journal of Artificial Intelligence in Education},
  eprint = {2211.14228},
  primaryclass = {cs},
  issn = {1560-4292, 1560-4306},
  doi = {10.1007/s40593-023-00340-7},
  urldate = {2023-07-24},
  abstract = {In order to train children's ability to ask curiosity-driven questions, previous research has explored designing specific exercises relying on providing semantic and linguistic cues to help formulate such questions. But despite showing pedagogical efficiency, this method is still limited as it relies on generating the said cues by hand, which can be a very costly process. In this context, we propose to leverage advances in the natural language processing field (NLP) and investigate the efficiency of using a large language model (LLM) for automating the production of the pedagogical content of a curious question-asking (QA) training. We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text. We evaluate the output using human experts annotations and comparisons with hand-generated content. Results suggested indeed the relevance and usefulness of this content. We also conduct a field study in primary school (75 children aged 9-10), where we evaluate children's QA performance when having this training. We compare 3 types of content : 1) hand-generated content that proposes "closed" cues leading to predefined questions; 2) GPT-3-generated content that proposes the same type of cues; 3) GPT-3-generated content that proposes "open" cues leading to several possible questions. We see a similar QA performance between the two "closed" trainings (showing the scalability of the approach using GPT-3), and a better one for participants with the "open" training. These results suggest the efficiency of using LLMs to support children in generating more curious questions, using a natural language prompting approach that affords usability by teachers and other users not specialists of AI techniques. Furthermore, results also show that open-ended content may be more suitable for training curious question-asking skills.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{ACLARR2024,
  title = {{{ACL ARR}} 2024 {{February Authors}}},
  journal = {OpenReview},
  url = {https://openreview.net/group?id=aclweb.org/ACL/ARR/2024/February/Authors},
  urldate = {2024-03-26},
  abstract = {Welcome to the OpenReview homepage for ACL ARR 2024 February Authors},
  langid = {english}
}

@inproceedings{agarwalProcessAwareDecisionSupport2022,
  title = {A {{Process-Aware Decision Support System}} for {{Business Processes}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Agarwal, Prerna and Gao, Buyu and Huo, Siyu and Reddy, Prabhat and Dechu, Sampath and Obeidi, Yazan and Muthusamy, Vinod and Isahagian, Vatche and Carbajales, Sebastian},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {2673--2681},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3534678.3539088},
  urldate = {2023-02-01},
  abstract = {Business processes in workflows comprise of an ordered sequence of tasks and decisions to accomplish certain business goals. Each decision point requires the input of a decision-maker to distill complex case information and make an optimal decision given their experience, organizational policy, and external contexts. Overlooking some of the essential factors or lack of knowledge can impact the throughput and business outcomes. Therefore, we propose an end-to-end automated decision support system with explanation for business processes. The system uses the proposed process-aware feature engineering methodology that extracts features from process and business data attributes. The system helps a decision-maker to make quick and quality decisions by predicting the decision and providing an explanation of the factors which led to the prediction. We provide offline and online training methods robust to data drift that can also incorporate user feedback. The system also support predictions with live instance data i.e., allow decision-makers to conduct trials on current data instance by modifying its business data attribute values. We evaluate our system on real-world and synthetic datasets and benchmark the performance, achieving an average of 15\% improvement over baselines.},
  isbn = {978-1-4503-9385-0},
  keywords = {business processes,decision support systems,process-awareness}
}

@inproceedings{aggarwalExplanationsCommonsenseQANew2021,
  title = {Explanations for {{CommonsenseQA}}: {{New Dataset}} and {{Models}}},
  shorttitle = {Explanations for {{CommonsenseQA}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Aggarwal, Shourya and Mandowara, Divyanshu and Agrawal, Vishwajeet and Khandelwal, Dinesh and Singla, Parag and Garg, Dinesh},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  month = aug,
  pages = {3050--3065},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.238},
  urldate = {2023-11-07},
  abstract = {CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for 11K QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100\% in F\_1 score, property generation model achieves a respectable F\_1 score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.}
}

@inproceedings{agrawalLargeLanguageModels2022,
  title = {Large Language Models Are Few-Shot Clinical Information Extractors},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Agrawal, Monica and Hegselmann, Stefan and Lang, Hunter and Kim, Yoon and Sontag, David},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {1998--2022},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.130},
  urldate = {2023-11-29},
  abstract = {A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.}
}

@misc{aguirreSelectingShotsDemographic2023,
  title = {Selecting {{Shots}} for {{Demographic Fairness}} in {{Few-Shot Learning}} with {{Large Language Models}}},
  author = {Aguirre, Carlos and Sasse, Kuleen and Cachola, Isabel and Dredze, Mark},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08472},
  eprint = {2311.08472},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.08472},
  urldate = {2023-11-18},
  abstract = {Recently, work in NLP has shifted to few-shot (in-context) learning, with large language models (LLMs) performing well across a range of tasks. However, while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems. Further, common standard methods for fairness involve access to models weights or are applied during finetuning, which are not applicable in few-shot learning. Do LLMs exhibit prediction biases when used for standard NLP tasks? In this work, we explore the effect of shots, which directly affect the performance of models, on the fairness of LLMs as NLP classification systems. We consider how different shot selection strategies, both existing and new demographically sensitive methods, affect model fairness across three standard fairness datasets. We discuss how future work can include LLM fairness evaluations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{aguirreSelectingShotsDemographic2023a,
  title = {Selecting {{Shots}} for {{Demographic Fairness}} in {{Few-Shot Learning}} with {{Large Language Models}}},
  author = {Aguirre, Carlos and Sasse, Kuleen and Cachola, Isabel and Dredze, Mark},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08472},
  eprint = {2311.08472},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.08472},
  urldate = {2023-12-29},
  abstract = {Recently, work in NLP has shifted to few-shot (in-context) learning, with large language models (LLMs) performing well across a range of tasks. However, while fairness evaluations have become a standard for supervised methods, little is known about the fairness of LLMs as prediction systems. Further, common standard methods for fairness involve access to models weights or are applied during finetuning, which are not applicable in few-shot learning. Do LLMs exhibit prediction biases when used for standard NLP tasks? In this work, we explore the effect of shots, which directly affect the performance of models, on the fairness of LLMs as NLP classification systems. We consider how different shot selection strategies, both existing and new demographically sensitive methods, affect model fairness across three standard fairness datasets. We discuss how future work can include LLM fairness evaluations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{ainslieCoLT5FasterLongRange2023,
  title = {{{CoLT5}}: {{Faster Long-Range Transformers}} with {{Conditional Computation}}},
  shorttitle = {{{CoLT5}}},
  author = {Ainslie, Joshua and Lei, Tao and {de Jong}, Michiel and Onta{\~n}{\'o}n, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and {Lee-Thorp}, James and Tay, Yi and Sung, Yun-Hsuan and Sanghai, Sumit},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09752},
  eprint = {2303.09752},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2303.09752},
  urldate = {2023-03-21},
  abstract = {Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{albalakEfficientOnlineData2023,
  title = {Efficient {{Online Data Mixing For Language Model Pre-Training}}},
  author = {Albalak, Alon and Pan, Liangming and Raffel, Colin and Wang, William Yang},
  year = {2023},
  month = dec,
  number = {arXiv:2312.02406},
  eprint = {2312.02406},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.02406},
  urldate = {2023-12-07},
  abstract = {The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19{\textbackslash}\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9\% relative accuracy, while adding negligible wall-clock time during pretraining.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{alemanCardiovascularDiseaseCancer2014,
  title = {Cardiovascular Disease after Cancer Therapy},
  author = {Aleman, Berthe M. P. and Moser, Elizabeth C. and Nuver, Janine and Suter, Thomas M. and Maraldo, Maja V. and Specht, Lena and Vrieling, Conny and Darby, Sarah C.},
  year = {2014},
  month = jun,
  journal = {European Journal of Cancer Supplements},
  series = {1st {{EORTC Cancer Survivorship Summit}}},
  volume = {12},
  number = {1},
  pages = {18--28},
  issn = {1359-6349},
  doi = {10.1016/j.ejcsup.2014.03.002},
  urldate = {2024-03-28},
  abstract = {Improvements in treatment and earlier diagnosis have both contributed to increased survival for many cancer patients. Unfortunately, many treatments carry a risk of late effects including cardiovascular diseases (CVDs), possibly leading to significant morbidity and mortality. In this paper we describe current knowledge of the cardiotoxicity arising from cancer treatments, outline gaps in knowledge, and indicate directions for future research and guideline development, as discussed during the 2014 Cancer Survivorship Summit organised by the European Organisation for Research and Treatment of Cancer (EORTC). Better knowledge is needed of the late effects of modern systemic treatments and of radiotherapy to critical structures of the heart, including the effect of both radiation dose and volume of the heart exposed. Research elucidating the extent to which treatments interact in causing CVD, and the mechanisms involved, as well as the extent to which treatments may increase CVD indirectly by increasing cardiovascular risk factors is also important. Systematic collection of data relating treatment details to late effects is needed, and great care is needed to obtain valid and generalisable results. Better knowledge of these cardiac effects will contribute to both primary and secondary prevention of late complications where exposure to cardiotoxic treatment is unavoidable. Also surrogate markers would help to identify patients at increased risk of cardiotoxicity. Evidence-based screening guidelines for CVD following cancer are also needed. Finally, risk prediction models should be developed to guide primary treatment choice and appropriate follow up after cancer treatment.},
  keywords = {Cancer,Cardiovascular,Disease,Oncology,Therapy}
}

@inproceedings{alemumogesMultiPerspectiveReasoningTransformers2021,
  title = {Multi-{{Perspective Reasoning Transformers}}},
  booktitle = {2021 13th {{International Conference}} on {{Machine Learning}} and {{Computing}}},
  author = {Alemu Moges, Dagmawi and Andre Niyongabo, Rubungo and Qu, Hong},
  year = {2021},
  pages = {503--508},
  doi = {10.1145/3457682.3457759}
}

@inproceedings{amershiGuidelinesHumanAIInteraction2019,
  title = {Guidelines for {{Human-AI Interaction}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and {Kikin-Gil}, Ruth and Horvitz, Eric},
  year = {2019},
  month = may,
  pages = {1--13},
  publisher = {ACM},
  address = {Glasgow Scotland Uk},
  doi = {10.1145/3290605.3300233},
  urldate = {2023-01-13},
  abstract = {Advances in artifcial intelligence (AI) frame opportunities and challenges for user interface design. Principles for humanAI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of guidelines for human-AI interaction design.},
  isbn = {978-1-4503-5970-2},
  langid = {english}
}

@inproceedings{bachSnorkelDryBellCase2019,
  title = {Snorkel {{DryBell}}: {{A Case Study}} in {{Deploying Weak Supervision}} at {{Industrial Scale}}},
  shorttitle = {Snorkel {{DryBell}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Bach, Stephen H. and Rodriguez, Daniel and Liu, Yintao and Luo, Chong and Shao, Haidong and Xia, Cassandra and Sen, Souvik and Ratner, Alex and Hancock, Braden and Alborzi, Houman and Kuchhal, Rahul and R{\'e}, Chris and Malkin, Rob},
  year = {2019},
  month = jun,
  pages = {362--375},
  publisher = {ACM},
  address = {Amsterdam Netherlands},
  doi = {10.1145/3299869.3314036},
  urldate = {2023-08-02},
  isbn = {978-1-4503-5643-5},
  langid = {english}
}

@article{baksiRecentAdvancesAutomated2021,
  title = {Recent {{Advances}} in {{Automated Question Answering In Biomedical Domain}}},
  author = {Baksi, Krishanu Das},
  year = {2021},
  journal = {arXiv preprint arXiv:2111.05937},
  eprint = {2111.05937},
  archiveprefix = {arxiv}
}

@inproceedings{baoContextualizedRewritingText2021,
  title = {Contextualized {{Rewriting}} for {{Text Summarization}}},
  booktitle = {{{AAAI}}},
  author = {Bao, Guangsheng and Zhang, Yue},
  year = {2021},
  abstract = {Extractive summarization suffers from irrelevance, redundancy and incoherence. Existing work shows that abstractive rewriting for extractive summaries can improve the conciseness and readability. These rewriting systems consider extracted summaries as the only input, which is relatively focused but can lose important background knowledge. In this paper, we investigate contextualized rewriting, which ingests the entire original document. We formalize contextualized rewriting as a seq2seq problem with group alignments, introducing group tag as a solution to model the alignments, identifying extracted summaries through content-based addressing. Results show that our approach significantly outperforms non-contextualized rewriting systems without requiring reinforcement learning, achieving strong improvements on ROUGE scores upon multiple extractive summarizers.}
}

@article{baradaranSurveyMachineReading2020,
  title = {A Survey on Machine Reading Comprehension Systems},
  author = {Baradaran, Razieh and Ghiasi, Razieh and Amirkhani, Hossein},
  year = {2020},
  journal = {arXiv preprint arXiv:2001.01582},
  eprint = {2001.01582},
  archiveprefix = {arxiv}
}

@inproceedings{bastanBioNLIGeneratingBiomedical2022,
  title = {{{BioNLI}}: {{Generating}} a {{Biomedical NLI Dataset Using Lexico-semantic Constraints}} for {{Adversarial Examples}}},
  shorttitle = {{{BioNLI}}},
  author = {Bastan, Mohaddeseh and Surdeanu, Mihai and Balasubramanian, Niranjan},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.14814},
  urldate = {2023-10-23},
  abstract = {Natural language inference (NLI) is critical for complex decision-making in biomedical domain. One key question, for example, is whether a given biomedical mechanism is supported by experimental evidence. This can be seen as an NLI problem but there are no directly usable datasets to address this. The main challenge is that manually creating informative negative examples for this task is difficult and expensive. We introduce a novel semi-supervised procedure that bootstraps an NLI dataset from existing biomedical dataset that pairs mechanisms with experimental evidence in abstracts. We generate a range of negative examples using nine strategies that manipulate the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, as perturbations via logical constraints in a neuro-logical decoding system. We use this procedure to create a novel dataset for NLI in the biomedical domain, called BioNLI and benchmark two state-of-the-art biomedical classifiers. The best result we obtain is around mid 70s in F1, suggesting the difficulty of the task. Critically, the performance on the different classes of negative examples varies widely, from 97\% F1 on the simple role change negative examples, to barely better than chance on the negative examples generated using neuro-logic decoding.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Information Retrieval (cs.IR),Machine Learning (cs.LG)}
}

@article{beltagySciBERTPretrainedLanguage2019,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  year = {2019},
  month = sep,
  journal = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  eprint = {1903.10676},
  pages = {3615--3620},
  doi = {10.18653/v1/D19-1371},
  urldate = {2022-01-17},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{bholaRetrievingSkillsJob2020,
  title = {Retrieving {{Skills}} from {{Job Descriptions}}: {{A Language Model Based Extreme Multi-label Classification Framework}}},
  shorttitle = {Retrieving {{Skills}} from {{Job Descriptions}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Bhola, Akshay and Halder, Kishaloy and Prasad, Animesh and Kan, Min-Yen},
  year = {2020},
  month = dec,
  pages = {5832--5842},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.513},
  urldate = {2022-07-12},
  abstract = {We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65\% of job descriptions miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9\% and 7\% absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our model publicly available.}
}

@misc{biancoImprovingImageCaptioning2023,
  title = {Improving {{Image Captioning Descriptiveness}} by {{Ranking}} and {{LLM-based Fusion}}},
  author = {Bianco, Simone and Celona, Luigi and Donzella, Marco and Napoletano, Paolo},
  year = {2023},
  month = jun,
  number = {arXiv:2306.11593},
  eprint = {2306.11593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.11593},
  urldate = {2024-03-25},
  abstract = {State-of-The-Art (SoTA) image captioning models often rely on the Microsoft COCO (MS-COCO) dataset for training. This dataset contains annotations provided by human annotators, who typically produce captions averaging around ten tokens. However, this constraint presents a challenge in effectively capturing complex scenes and conveying detailed information. Furthermore, captioning models tend to exhibit bias towards the ``average'' caption, which captures only the more general aspects. What would happen if we were able to automatically generate longer captions, thereby making them more detailed? Would these captions, evaluated by humans, be more or less representative of the image content compared to the original MS-COCO captions? In this paper, we present a novel approach to address previous challenges by showcasing how captions generated from different SoTA models can be effectively fused, resulting in richer captions. Our proposed method leverages existing models from the literature, eliminating the need for additional training. Instead, it utilizes an image-text based metric to rank the captions generated by SoTA models for a given image. Subsequently, the top two captions are fused using a Large Language Model (LLM). Experimental results demonstrate the effectiveness of our approach, as the captions generated by our model exhibit higher consistency with human judgment when evaluated on the MS-COCO test set. By combining the strengths of various SoTA models, our method enhances the quality and appeal of image captions, bridging the gap between automated systems and the rich, informative nature of human-generated descriptions. This advance opens up new possibilities for generating captions that are more suitable for the training of both vision-language and captioning models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Databases,Computer Science - Machine Learning}
}

@inproceedings{bikaunQuickGraphRapidAnnotation2022,
  title = {{{QuickGraph}}: {{A Rapid Annotation Tool}} for {{Knowledge Graph Extraction}} from {{Technical Text}}},
  shorttitle = {{{QuickGraph}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Bikaun, Tyler and Stewart, Michael and Liu, Wei},
  year = {2022},
  month = may,
  pages = {270--278},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-demo.27},
  urldate = {2023-04-03},
  abstract = {Acquiring high-quality annotated corpora for complex multi-task information extraction (MT-IE) is an arduous and costly process for human-annotators. Adoption of unsupervised techniques for automated annotation have thus become popular. However, these techniques rely heavily on dictionaries, gazetteers, and knowledge bases. While such resources are abundant for general domains, they are scarce for specialised technical domains. To tackle this challenge, we present QuickGraph, the first collaborative MT-IE annotation tool built with indirect weak supervision and clustering to maximise annotator productivity.QuickGraph's main contribution is a set of novel features that enable knowledge graph extraction through rapid and consistent complex multi-task entity and relation annotation. In this paper, we discuss these key features and qualitatively compare QuickGraph to existing annotation tools.}
}

@inproceedings{birdNLTKNaturalLanguage2004,
  title = {{{NLTK}}: {{The Natural Language Toolkit}}},
  shorttitle = {{{NLTK}}},
  booktitle = {Proceedings of the {{ACL Interactive Poster}} and {{Demonstration Sessions}}},
  author = {Bird, Steven and Loper, Edward},
  year = {2004},
  pages = {214--217},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  doi = {10.3115/1219044.1219075},
  urldate = {2022-01-10},
  annotation = {02186}
}

@inproceedings{bordesTranslatingEmbeddingsModeling2013,
  title = {Translating {{Embeddings}} for {{Modeling Multi-relational Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bordes, Antoine and Usunier, Nicolas and {Garcia-Duran}, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html},
  urldate = {2021-12-07}
}

@misc{borgeaudImprovingLanguageModels2022,
  title = {Improving Language Models by Retrieving from Trillions of Tokens},
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and van den Driessche, George and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  year = {2022},
  month = feb,
  number = {arXiv:2112.04426},
  eprint = {2112.04426},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.04426},
  urldate = {2022-09-06},
  abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\${\textbackslash}times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{bowmanLargeAnnotatedCorpus2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  editor = {M{\`a}rquez, Llu{\'i}s and {Callison-Burch}, Chris and Su, Jian},
  year = {2015},
  month = sep,
  pages = {632--642},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1075},
  urldate = {2023-12-15}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = dec,
  series = {{{NIPS}}'20},
  pages = {1877--1901},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-06-23},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  isbn = {978-1-71382-954-6}
}

@misc{bubeckSparksArtificialGeneral2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  urldate = {2023-03-24},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{caiGenerationPatientAfterVisit2022,
  title = {Generation of {{Patient After-Visit Summaries}} to {{Support Physicians}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Cai, Pengshan and Liu, Fei and Bajracharya, Adarsha and Sills, Joe and Kapoor, Alok and Liu, Weisong and Berlowitz, Dan and Levy, David and Pradhan, Richeek and Yu, Hong},
  editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  year = {2022},
  month = oct,
  pages = {6234--6247},
  publisher = {International Committee on Computational Linguistics},
  address = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.coling-1.544},
  urldate = {2024-03-18},
  abstract = {An after-visit summary (AVS) is a summary note given to patients after their clinical visit. It recaps what happened during their clinical visit and guides patients' disease self-management. Studies have shown that a majority of patients found after-visit summaries useful. However, many physicians face excessive workloads and do not have time to write clear and informative summaries. In this paper, we study the problem of automatic generation of after-visit summaries and examine whether those summaries can convey the gist of clinical visits. We report our findings on a new clinical dataset that contains a large number of electronic health record (EHR) notes and their associated summaries. Our results suggest that generation of lay language after-visit summaries remains a challenging task. Crucially, we introduce a feedback mechanism that alerts physicians when an automatic summary fails to capture the important details of the clinical notes or when it contains hallucinated facts that are potentially detrimental to the summary quality. Automatic and human evaluation demonstrates the effectiveness of our approach in providing writing feedback and supporting physicians.}
}

@misc{CalibratingImbalancedClassifiers,
  title = {Calibrating {{Imbalanced Classifiers}} with {{Focal Loss}}: {{An Empirical Study}} - {{ACL Anthology}}},
  url = {https://aclanthology.org/2022.emnlp-industry.14/},
  urldate = {2023-06-20}
}

@article{camposYAKEKeywordExtraction2020,
  title = {{{YAKE}}! {{Keyword}} Extraction from Single Documents Using Multiple Local Features},
  author = {Campos, Ricardo and Mangaravite, V{\'i}tor and Pasquali, Arian and Jorge, Al{\'i}pio and Nunes, C{\'e}lia and Jatowt, Adam},
  year = {2020},
  month = jan,
  journal = {Information Sciences},
  volume = {509},
  pages = {257--289},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2019.09.013},
  urldate = {2022-10-17},
  abstract = {As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains.},
  langid = {english},
  keywords = {Information extraction,Keyword extraction,Unsupervised Algorithm}
}

@misc{cassanoMultiPLEScalableExtensible2022,
  title = {{{MultiPL-E}}: {{A Scalable}} and {{Extensible Approach}} to {{Benchmarking Neural Code Generation}}},
  shorttitle = {{{MultiPL-E}}},
  author = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and {Phipps-Costin}, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q. and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
  year = {2022},
  month = dec,
  number = {arXiv:2208.08227},
  eprint = {2208.08227},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.08227},
  urldate = {2024-01-11},
  abstract = {Large language models have demonstrated the ability to generate both natural language and programming language text. Such models open up the possibility of multi-language code generation: could code generation models generalize knowledge from one language to another? Although contemporary code generation models can generate semantically correct Python code, little is known about their abilities with other languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages. We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages}
}

@inproceedings{chalkidisLargeScaleMultiLabelText2019,
  title = {Large-{{Scale Multi-Label Text Classification}} on {{EU Legislation}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chalkidis, Ilias and Fergadiotis, Emmanouil and Malakasiotis, Prodromos and Androutsopoulos, Ion},
  year = {2019},
  month = jul,
  pages = {6314--6322},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1636},
  urldate = {2023-10-23},
  abstract = {We consider Large-Scale Multi-Label Text Classification (LMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR-LEX, annotated with {$\sim$}4.3k EUROVOC labels, which is suitable for LMTC, few- and zero-shot learning. Experimenting with several neural classifiers, we show that BIGRUs with label-wise attention perform better than other current state of the art methods. Domain-specific WORD2VEC and context-sensitive ELMO embeddings further improve performance. We also find that considering only particular zones of the documents is sufficient. This allows us to bypass BERT's maximum text length limit and fine-tune BERT, obtaining the best results in all but zero-shot learning cases.},
  langid = {english}
}

@inproceedings{changAlloyClusteringCrowds2016,
  title = {Alloy: {{Clustering}} with {{Crowds}} and {{Computation}}},
  shorttitle = {Alloy},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chang, Joseph Chee and Kittur, Aniket and Hahn, Nathan},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {3180--3191},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858411},
  urldate = {2023-08-31},
  abstract = {Crowdsourced clustering approaches present a promising way to harness deep semantic knowledge for clustering complex information. However, existing approaches have difficulties supporting the global context needed for workers to generate meaningful categories, and are costly because all items require human judgments. We introduce Alloy, a hybrid approach that combines the richness of human judgments with the power of machine algorithms. Alloy supports greater global context through a new "sample and search" crowd pattern which changes the crowd's task from classifying a fixed subset of items to actively sampling and querying the entire dataset. It also improves efficiency through a two phase process in which crowds provide examples to help a machine cluster the head of the distribution, then classify low-confidence examples in the tail. To accomplish this, Alloy introduces a modular "cast and gather" approach which leverages a machine learning backbone to stitch together different types of judgment tasks.},
  isbn = {978-1-4503-3362-7},
  keywords = {computer supported cooperative work (CSCW),database access / informationretrieval,empirical methods,quantitative,worldwide web and hypermedia}
}

@inproceedings{changUsingExploringHierarchical2016,
  title = {Using and {{Exploring Hierarchical Data}} in {{Spreadsheets}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chang, Kerry Shih-Ping and Myers, Brad A.},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {2497--2507},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858430},
  urldate = {2023-08-31},
  abstract = {More and more data nowadays exist in hierarchical formats such as JSON due to the increasing popularity of web applications and web services. While many end-user systems support getting hierarchical data from databases without programming, they provide very little support for using hierarchical data beyond turning the data into a flat string or table. In this paper, we present a spreadsheet tool for using and exploring hierarchical datasets. We introduce novel interaction techniques and algorithms to manipulate and visualize hierarchical data in a spreadsheet using the data's relative hierarchical relationships with the data in its adjacent columns. Our tool leverages the data's structural information to support selecting, grouping, joining, sorting and filtering hierarchical data in spreadsheets. Our lab study showed that our tool helped spreadsheet users complete data exploration tasks nearly two times faster than using Excel and even outperform programmers in most tasks.},
  isbn = {978-1-4503-3362-7},
  keywords = {end-user programming,hierarchical data,spreadsheets}
}

@article{chanMangoMangoHow2018,
  title = {``{{Mango Mango}}, {{How}} to {{Let The Lettuce Dry Without A Spinner}}?'': {{Explore The Advantages And Challenges When Employing An LLM-Based Voice Assistant}} in {{Cooking Scenarios}}},
  author = {Chan, Szeyi and Li, Jiachen and Yao, Bingsheng and Mahmood, Amama and Huang, Chien-Ming and Jimison, Holly and Mynatt, Elizabeth D and Wang, Dakuo},
  year = {2018},
  abstract = {The rapid advancement of the Large Language Model (LLM) has created numerous potentials for integration with conversational assistants (CAs) assisting people in their daily tasks, particularly due to their extensive flexibility. However, users' real-world experiences interacting with these assistants remain unexplored. In this research, we chose cooking, a complex daily task, as a scenario to investigate people's successful and unsatisfactory experiences while receiving assistance from an LLM-based CA, Mango Mango. We discovered that participants value the system's ability to provide extensive information beyond the recipe, offer customized instructions based on context, and assist them in dynamically planning the task. However, they expect the system to be more adaptive to oral conversation and provide more suggestive responses to keep users actively involved. Recognizing that users began treating our LLM-CA as a personal assistant or even a partner rather than just a recipe-reading tool, we propose several design considerations for future development. CCS Concepts: {$\bullet$} Human-centered computing {$\rightarrow$} User studies; Sound-based input / output; Auditory feedback; Empirical studies in HCI.},
  langid = {english}
}

@misc{chaoEchoGPTLargeLanguage2024,
  title = {{{EchoGPT}}: {{A Large Language Model}} for {{Echocardiography Report Summarization}}},
  shorttitle = {{{EchoGPT}}},
  author = {Chao, Chieh-Ju and Banerjee, Imon and Arsanjani, Reza and Ayoub, Chadi and Tseng, Andrew and Kane, Garvan C. and Oh, Jae K. and Li, Fei-Fei and Adeli, Ehsan and Langlotz, Curtis P.},
  year = {2024},
  month = jan,
  pages = {2024.01.18.24301503},
  publisher = {medRxiv},
  doi = {10.1101/2024.01.18.24301503},
  urldate = {2024-01-20},
  abstract = {Background The increasing need for diagnostic echocardiography (echo) tests presents challenges in preserving the quality and promptness of reports. While Large Language Models (LLMs) have proven effective in summarizing clinical texts, their application in echo remains underexplored. To address this, we proposed EchoGPT, a dedicated, domain-specific LLM focused on echo report summarization. Methods Adult echo studies conducted at the Mayo Clinic from January 1, 2017, to December 31, 2017, were collected and categorized into two groups: development (all Mayo locations except Arizona) and external validation (Mayo Arizona). We adapted open-source LLMs (Llama-2, MedAlpaca, Zephyr, and Flan-T5) using In-Context Learning (ICL) and Quantized Low-Rank Adaptation (QLoRA) fine-tuning for echo text summarization. The models' performance was assessed both quantitatively with automatic metrics and qualitatively by cardiologists. Results The development dataset included 97,506 reports from 71,717 unique patients, predominantly male (54.3\%), with an average age of 64.1+/-16.1 years. The final split contains 95,506 for training, and 1,000 each for validation and testing. EchoGPT, a QLoRA fine-tuned Llama-2 model, outperformed other LLMs with about 90\% win rates in various metrics (BLEU, METEOR, ROUGE-L, BERT Score, and RadGraph F1 Score), and produced reports comparable to cardiologists in 30 randomly selected cases for qualitative human review (significantly preferred in conciseness (p{$<$} 0.001), with no significant preference in completeness, correctness, and clinical utility). In the external validation set (n=1,000), EchoGPT consistently outperformed fine-tuned Zephyr model across the same automatic metrics (all p {$<$} 0.0001). Conclusions Capable of generating echocardiography reports on par with human experts, EchoGPT could be used to generate draft reports for human review and approval, with significant workflow advantages.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@misc{chaoEchoGPTLargeLanguage2024a,
  title = {{{EchoGPT}}: {{A Large Language Model}} for {{Echocardiography Report Summarization}}},
  shorttitle = {{{EchoGPT}}},
  author = {Chao, Chieh-Ju and Banerjee, Imon and Arsanjani, Reza and Ayoub, Chadi and Tseng, Andrew and Kane, Garvan C. and Oh, Jae K. and {Fei-Fei}, Li and Adeli, Ehsan and Langlotz, Curtis},
  year = {2024},
  month = jan,
  pages = {2024.01.18.24301503},
  publisher = {medRxiv},
  doi = {10.1101/2024.01.18.24301503},
  urldate = {2024-02-20},
  abstract = {Background The increasing need for diagnostic echocardiography (echo) tests presents challenges in preserving the quality and promptness of reports. While Large Language Models (LLMs) have proven effective in summarizing clinical texts, their application in echo remains underexplored. To address this, we proposed EchoGPT, a dedicated, domain-specific LLM focused on echo report summarization. Methods Adult echo studies conducted at the Mayo Clinic from January 1, 2017, to December 31, 2017, were collected and categorized into two groups: development (all Mayo locations except Arizona) and external validation (Mayo Arizona). We adapted open-source LLMs (Llama-2, MedAlpaca, Zephyr, and Flan-T5) using In-Context Learning (ICL) and Quantized Low-Rank Adaptation (QLoRA) fine-tuning for echo text summarization. The models' performance was assessed both quantitatively with automatic metrics and qualitatively by cardiologists. Results The development dataset included 97,506 reports from 71,717 unique patients, predominantly male (54.3\%), with an average age of 64.1{\textpm}16.1 years. The final split contains 95,506 for training, and 1,000 each for validation and testing. EchoGPT, a QLoRA fine-tuned Llama-2 model, outperformed other LLMs with about 90\% win rates in various metrics (BLEU, METEOR, ROUGE-L, BERT Score, and RadGraph F1 Score), and produced reports comparable to cardiologists in 30 randomly selected cases for qualitative human review (significantly preferred in conciseness (p{$<$} 0.001), with no significant preference in completeness, correctness, and clinical utility). In the external validation set (n=1,000), EchoGPT consistently outperformed fine-tuned Zephyr model across the same automatic metrics (all p {$<$} 0.0001). Conclusions Capable of generating echocardiography reports on par with human experts, EchoGPT could be used to generate draft reports for human review and approval, with significant workflow advantages. Clinical PerspectiveWhat is new? \ding{108} This study is the first attempt to compare multiple open-source LLMs and different model adaptation methods in echocardiography report summarization.\ding{108} The resulting system, EchoGPT, can generate echo reports comparable in quality to cardiologists.\ding{108} Future metrics for echo report quality should emphasize factual correctness, especially on numerical measurements.What are the clinical implications? \ding{108} EchoGPT system demonstrated the potential of introducing LLMs into echocardiography practice.\ding{108} EchoGPT could be used as an AI co-pilot to generate echo reports.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@article{chaoPilotRandomizedControlled2017,
  title = {A {{Pilot Randomized Controlled Trial}} of a {{Technology-Based Approach}} for {{Preventing Excess Weight Gain}} during {{Pregnancy}} among {{Women}} with {{Overweight}}},
  author = {Chao, Ariana M. and Srinivas, Sindhu K. and Studt, Stacia K. and Diewald, Lisa K. and Sarwer, David B. and Allison, Kelly C.},
  year = {2017},
  journal = {Frontiers in Nutrition},
  volume = {4},
  issn = {2296-861X},
  doi = {10.3389/fnut.2017.00057},
  urldate = {2024-02-12},
  abstract = {ObjectiveOverweight/obesity and excess weight gain during pregnancy are associated with adverse maternal and neonatal outcomes. Few interventions have been effective in limiting gestational weight gain among women with overweight or obesity. This pilot, randomized clinical trial compared treatment as usual (TAU) to a lifestyle modification program delivered via phone for the prevention of excess gestational weight gain in women who had overweight or obesity.MethodsParticipants included 41 pregnant women with a body mass index (BMI)\,{$\geq$}\,25\,kg/m2 (mean age\,=\,28.7\,{\textpm}\,5.8\,years; mean pre-gravid BMI\,=\,31.2\,{\textpm}\,6.2\,kg/m2; 54\% black, 39\% white). The intervention group (n\,=\,20) received weekly telephone counseling sessions and used WiFi scales to monitor their weight from weeks 16 to 36 of pregnancy. We compared differences in weight and birth outcomes for the intervention vs. the TAU group (n\,=\,21).ResultsThe intervention and TAU groups did not differ with respect to: gestational weight gain (15.5\,{\textpm}\,5.3 vs. 13.3\,{\textpm}\,6.8\,kg, respectively); proportion gaining above the 2009 Institute of Medicine recommended weight range (83 vs. 70\%); and weight gain from pre-pregnancy weight to 6\,weeks postpartum (4.8\,{\textpm}\,4.6 vs. 3.0\,{\textpm}\,5.5\,kg). Other birth and health outcomes also did not differ.ConclusionA telemedicine intervention designed to decrease logistical burden on participants was not more successful in reducing excessive weight gain during pregnancy as compared to TAU. Future studies should examine more intensive forms of remote treatment beginning earlier in pregnancy as well as interventions promoting a healthy weight prior to pregnancy.}
}

@misc{chenControllableTextGeneration2022,
  title = {Controllable {{Text Generation}} with {{Language Constraints}}},
  author = {Chen, Howard and Li, Huihan and Chen, Danqi and Narasimhan, Karthik},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10466},
  eprint = {2212.10466},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10466},
  urldate = {2023-01-02},
  abstract = {We consider the task of text generation in language models with constraints specified in natural language. To this end, we first create a challenging benchmark Cognac that provides as input to the model a topic with example text, along with a constraint on text to be avoided. Unlike prior work, our benchmark contains knowledge-intensive constraints sourced from databases like Wordnet and Wikidata, which allows for straightforward evaluation while striking a balance between broad attribute-level and narrow lexical-level controls. We find that even state-of-the-art language models like GPT-3 fail often on this task, and propose a solution to leverage a language model's own internal knowledge to guide generation. Our method, called CognacGen, first queries the language model to generate guidance terms for a specified topic or constraint, and uses the guidance to modify the model's token generation probabilities. We propose three forms of guidance (binary verifier, top-k tokens, textual example), and employ prefix-tuning approaches to distill the guidance to tackle diverse natural language constraints. Through extensive empirical evaluations, we demonstrate that CognacGen can successfully generalize to unseen instructions and outperform competitive baselines in generating constraint conforming text.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.03374},
  urldate = {2024-01-11},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{chenInterpretingTrajectoriesMultiple2022,
  title = {Interpreting {{Trajectories}} from {{Multiple Views}}: {{A Hierarchical Self-Attention Network}} for {{Estimating}} the {{Time}} of {{Arrival}}},
  shorttitle = {Interpreting {{Trajectories}} from {{Multiple Views}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Zebin and Xiao, Xiaolin and Gong, Yue-Jiao and Fang, Jun and Ma, Nan and Chai, Hua and Cao, Zhiguang},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {2771--2779},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3534678.3539051},
  urldate = {2023-02-01},
  abstract = {Estimating the time of arrival is a crucial task in intelligent transportation systems. Although considerable efforts have been made to solve this problem, most of them decompose a trajectory into several segments and then compute the travel time by integrating the attributes from all segments. The segment view, though being able to depict the local traffic conditions straightforwardly, is insufficient to embody the intrinsic structure of trajectories on the road network. To overcome the limitation, this study proposes multi-view trajectory representation that comprehensively interprets a trajectory from the segment-, link-, and intersection-views. To fulfill the purpose, we design a hierarchical self-attention network (HierETA) that accurately models the local traffic conditions and the underlying trajectory structure. Specifically, a segment encoder is developed to capture the spatio-temporal dependencies at a fine granularity, within which an adaptive self-attention module is designed to boost performance. Further, a joint link-intersection encoder is developed to characterize the natural trajectory structure consisting of alternatively arranged links and intersections. Afterward, a hierarchy-aware attention decoder is designed to realize a tradeoff between the multi-view spatio-temporal features. The hierarchical encoders and the attentive decoder are simultaneously learned to achieve an overall optimality. Experiments on two large-scale practical datasets show the superiority of HierETA over the state-of-the-arts.},
  isbn = {978-1-4503-9385-0},
  keywords = {estimating the time of arrival,hierarchical representation learning,self-attention network}
}

@article{chenLearningQLargeScaleDataset2018,
  title = {{{LearningQ}}: {{A Large-Scale Dataset}} for {{Educational Question Generation}}},
  shorttitle = {{{LearningQ}}},
  author = {Chen, Guanliang and Yang, Jie and Hauff, Claudia and Houben, Geert-Jan},
  year = {2018},
  month = jun,
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {12},
  number = {1},
  issn = {2334-0770},
  doi = {10.1609/icwsm.v12i1.14987},
  urldate = {2023-11-07},
  abstract = {We present LearningQ, a challenging educational question generation dataset containing over 230K document-question pairs. It includes 7K instructor-designed questions assessing knowledge concepts being taught and 223K learner-generated questions seeking in-depth understanding of the taught concepts. We show that, compared to existing datasets that can be used to generate educational questions, LearningQ (i) covers a wide range of educational topics and (ii) contains long and cognitively demanding documents for which question generation requires reasoning over the relationships between sentences and paragraphs. As a result, a significant percentage of LearningQ questions ({\textasciitilde}30\%) require higher-order cognitive skills to solve (such as applying, analyzing), in contrast to existing question-generation datasets that are designed mostly for the lowest cognitive skill level (i.e. remembering). To understand the effectiveness of existing question generation methods in producing educational questions, we evaluate both rule-based and deep neural network based methods on LearningQ. Extensive experiments show that state-of-the-art methods which perform well on existing datasets cannot generate useful educational questions. This implies that LearningQ is a challenging test bed for the generation of high-quality educational questions and worth further investigation. We open-source the dataset and our codes at https://dataverse.mpi-sws.org/dataverse/icwsm18.},
  copyright = {Copyright (c) 2022 Proceedings of the International AAAI Conference on Web and Social Media},
  langid = {english},
  keywords = {Bloom's Revised Taxonomy}
}

@misc{chenMakingYourFirst2022a,
  title = {Making {{Your First Choice}}: {{To Address Cold Start Problem}} in {{Vision Active Learning}}},
  shorttitle = {Making {{Your First Choice}}},
  author = {Chen, Liangyu and Bai, Yutong and Huang, Siyu and Lu, Yongyi and Wen, Bihan and Yuille, Alan L. and Zhou, Zongwei},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02442},
  eprint = {2210.02442},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2210.02442},
  urldate = {2023-12-15},
  abstract = {Active learning promises to improve annotation efficiency by iteratively selecting the most important data to be annotated first. However, we uncover a striking contradiction to this promise: active learning fails to select data as efficiently as random selection at the first few choices. We identify this as the cold start problem in vision active learning, caused by a biased and outlier initial query. This paper seeks to address the cold start problem by exploiting the three advantages of contrastive learning: (1) no annotation is required; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3) typical data is determined by contrastive features to reduce outliers. Experiments are conducted on CIFAR-10-LT and three medical imaging datasets (i.e. Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial query not only significantly outperforms existing active querying strategies but also surpasses random selection by a large margin. We foresee our solution to the cold start problem as a simple yet strong baseline to choose the initial query for vision active learning. Code is available: https://github.com/c-liangyu/CSVAL},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{chenMultichoiceRelationalReasoning2020,
  title = {Multi-Choice {{Relational Reasoning}} for {{Machine Reading Comprehension}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Chen, Wuya and Quan, Xiaojun and Kit, Chunyu and Min, Zhengcheng and Wang, Jiahai},
  year = {2020},
  pages = {6448--6458},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.567},
  urldate = {2022-04-12},
  abstract = {This paper presents our study of cloze-style reading comprehension by imitating human reading comprehension, which normally involves tactical comparing and reasoning over candidates while choosing the best answer. We propose a multi-choice relational reasoning (McR\$\^{}2\$) model with an aim to enable relational reasoning on candidates based on fusion representations of document, query and candidates. For the fusion representations, we develop an efficient encoding architecture by integrating the schemes of bidirectional attention flow, self-attention and document-gated query reading. Then, comparing and inferring over candidates are executed by a novel relational reasoning network. We conduct extensive experiments on four datasets derived from two public corpora, Children's Book Test and Who DiD What, to verify the validity and advantages of our model. The results show that it outperforms all baseline models significantly on the four benchmark datasets. The effectiveness of its key components is also validated by an ablation study.}
}

@inproceedings{chenPersonalizedChitChatGeneration2022,
  title = {Personalized {{Chit-Chat Generation}} for {{Recommendation Using External Chat Corpora}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Changyu and Wang, Xiting and Yi, Xiaoyuan and Wu, Fangzhao and Xie, Xing and Yan, Rui},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {2721--2731},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3534678.3539215},
  urldate = {2023-02-01},
  abstract = {Chit-chat has been shown effective in engaging users in human-computer interaction. We find with a user study that generating appropriate chit-chat for news articles can help expand user interest and increase the probability that a user reads a recommended news article. Based on this observation, we propose a method to generate personalized chit-chat for news recommendation. Different from existing methods for personalized text generation, our method only requires an external chat corpus obtained from an online forum, which can be disconnected from the recommendation dataset from both the user and item (news) perspectives. This is achieved by designing a weak supervision method for estimating users' personalized interest in a chit-chat post by transferring knowledge learned by a news recommendation model. Based on the method for estimating user interest, a reinforcement learning framework is proposed to generate personalized chit-chat. Extensive experiments, including the automatic offline evaluation and user studies, demonstrate the effectiveness of our method.},
  isbn = {978-1-4503-9385-0},
  keywords = {chit-chat,news recommendation,personalized text generation,reinforcement learning}
}

@misc{chenRareBenchCanLLMs2024,
  title = {{{RareBench}}: {{Can LLMs Serve}} as {{Rare Diseases Specialists}}?},
  shorttitle = {{{RareBench}}},
  author = {Chen, Xuanzhong and Mao, Xiaohao and Guo, Qihan and Wang, Lun and Zhang, Shuyang and Chen, Ting},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06341},
  eprint = {2402.06341},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.06341},
  urldate = {2024-02-12},
  abstract = {Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic few-shot prompt methodology, leveraging a comprehensive rare disease knowledge graph synthesized from multiple knowledge bases, significantly enhancing LLMs' diagnostic performance. Moreover, we present an exhaustive comparative study of GPT-4's diagnostic capabilities against those of specialist physicians. Our experimental findings underscore the promising potential of integrating LLMs into the clinical diagnostic process for rare diseases. This paves the way for exciting possibilities in future advancements in this field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{chenSymbolicDiscoveryOptimization2023,
  title = {Symbolic {{Discovery}} of {{Optimization Algorithms}}},
  author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06675},
  eprint = {2302.06675},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.06675},
  urldate = {2023-02-20},
  abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \${\textbackslash}textbf\{Lion\}\$ (\${\textbackslash}textit\{Evo\${\textbackslash}textbf\{L\}\$ved S\${\textbackslash}textbf\{i\}\$gn M\${\textbackslash}textbf\{o\}\$me\${\textbackslash}textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2\% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3\% \${\textbackslash}textit\{zero-shot\}\$ and 91.1\% \${\textbackslash}textit\{fine-tuning\}\$ accuracy on ImageNet, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. The implementation of Lion is publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@misc{chiangCanLargeLanguage2023,
  title = {Can {{Large Language Models Be}} an {{Alternative}} to {{Human Evaluations}}?},
  author = {Chiang, Cheng-Han and Lee, Hung-yi},
  year = {2023},
  month = may,
  number = {arXiv:2305.01937},
  eprint = {2305.01937},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01937},
  urldate = {2023-10-26},
  abstract = {Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@inproceedings{chiEnhancingCrossDeviceInteraction2016,
  title = {Enhancing {{Cross-Device Interaction Scripting}} with {{Interactive Illustrations}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chi, Pei-Yu (Peggy) and Li, Yang and Hartmann, Bj{\"o}rn},
  year = {2016},
  month = may,
  series = {{{CHI}} '16},
  pages = {5482--5493},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2858036.2858382},
  urldate = {2023-08-31},
  abstract = {Cross-device interactions involve input and output on multiple computing devices. Implementing and reasoning about interactions that cover multiple devices with a diversity of form factors and capabilities can be complex. To assist developers in programming cross-device interactions, we created DemoScript, a technique that automatically analyzes a cross-device interaction program while it is being written. DemoScript visually illustrates the step-by-step execution of a selected portion or the entire program with a novel, automatically generated cross-device storyboard visualization. In addition to helping developers understand the behavior of the program, DemoScript also allows developers to revise their program by interactively manipulating the cross-device storyboard. We evaluated DemoScript with 8 professional programmers and found that DemoScript significantly improved development efficiency by helping developers interpret and manage cross-device interaction; it also encourages testing to think through the script in a development process.},
  isbn = {978-1-4503-3362-7},
  keywords = {cross-device interaction,interactive illustration,scripting,storyboards}
}

@misc{ChmodIgnoreUmask,
  title = {Chmod Ignore Umask - {{Google}} },
  url = {https://www.google.com/search?q=chmod+ignore+umask&newwindow=1&sca_esv=9fb3dbb1b45a590b&rlz=1C5GCCM_en&sxsrf=ACQVn0-u8VAa5Lk563Kuc-1gms5e4wqo_Q%3A1708901218054&ei=YsPbZefsAsWv5NoP4Y-XoA8&ved=0ahUKEwjnpbuQyceEAxXFF1kFHeHHBfQQ4dUDCBA&uact=5&oq=chmod+ignore+umask&gs_lp=Egxnd3Mtd2l6LXNlcnAiEmNobW9kIGlnbm9yZSB1bWFzazIFECEYoAFI_BVQzwRY_hRwAngBkAEBmAGJA6ABzg2qAQgxMy4zLjAuMbgBA8gBAPgBAZgCEqACuAzCAgoQABhHGNYEGLADwgIKEAAYgAQYigUYQ8ICBRAAGIAEwgILEC4YgAQYxwEY0QPCAgcQLhiABBgMwgIHEAAYgAQYCsICBBAAGB7CAgYQABgeGA_CAgYQABgFGB6YAwCIBgGQBgqSBwgxNS4yLjAuMQ&sclient=gws-wiz-serp},
  urldate = {2024-02-25}
}

@misc{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and {Castro-Ros}, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = dec,
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2023-08-15},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{cosmaApproachSourcecodePlagiarism2011,
  title = {An Approach to Source-Code Plagiarism Detection and Investigation Using Latent Semantic Analysis},
  author = {Cosma, Georgina and Joy, Mike},
  year = {2011},
  journal = {IEEE transactions on computers},
  volume = {61},
  number = {3},
  pages = {379--394},
  publisher = {IEEE},
  doi = {10.1109/TC.2011.223}
}

@article{cosmaDefinitionSourcecodePlagiarism2008,
  title = {Towards a Definition of Source-Code Plagiarism},
  author = {Cosma, Georgina and Joy, Mike},
  year = {2008},
  journal = {IEEE Transactions on Education},
  volume = {51},
  number = {2},
  pages = {195--200},
  publisher = {IEEE},
  doi = {10.1109/TE.2007.906776}
}

@misc{COVID19OpenResearch,
  title = {{{COVID-19 Open Research Dataset Challenge}} ({{CORD-19}})},
  url = {https://kaggle.com/allen-institute-for-ai/CORD-19-research-challenge},
  urldate = {2021-12-25},
  abstract = {An AI challenge with AI2, CZI, MSR, Georgetown, NIH \& The White House},
  langid = {english},
  annotation = {00000}
}

@article{CPLLMClinicalPrediction2023,
  title = {{{CPLLM}}: {{Clinical Prediction}} with {{Large Language Models}}},
  shorttitle = {{{CPLLM}}},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=fnBYPL5Ged},
  urldate = {2024-01-10},
  abstract = {We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models},
  langid = {english}
}

@misc{CSCW24LLM,
  title = {{{CSCW}}'24 {{LLM For Cancer Post-Op Care}} - {{Working}}},
  url = {https://www.overleaf.com/project/65236bd4ae117c85e45ed3f2/detached},
  urldate = {2024-01-14},
  abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  langid = {english}
}

@inproceedings{cuiAttentionoverAttentionNeuralNetworks2017,
  title = {Attention-over-{{Attention Neural Networks}} for {{Reading Comprehension}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Cui, Yiming and Chen, Zhipeng and Wei, Si and Wang, Shijin and Liu, Ting and Hu, Guoping},
  year = {2017},
  pages = {593--602},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1055},
  urldate = {2022-01-17},
  abstract = {Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. The proposed model aims to place another attention mechanism over the document-level attention and induces ``attended attention'' for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. In addition to the primary model, we also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test.},
  annotation = {00396}
}

@inproceedings{cuiConsensusAttentionbasedNeural2016,
  title = {Consensus {{Attention-based Neural Networks}} for {{Chinese Reading Comprehension}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Cui, Yiming and Liu, Ting and Chen, Zhipeng and Wang, Shijin and Hu, Guoping},
  year = {2016},
  pages = {1777--1786},
  publisher = {The COLING 2016 Organizing Committee},
  address = {Osaka, Japan},
  url = {https://aclanthology.org/C16-1167},
  urldate = {2022-01-15},
  abstract = {Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.},
  annotation = {00085}
}

@misc{daoFlashAttentionFastMemoryEfficient2022,
  title = {{{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  shorttitle = {{{FlashAttention}}},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14135},
  eprint = {2205.14135},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14135},
  urldate = {2023-03-13},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{dasDecoderonlyFoundationModel2024,
  title = {A Decoder-Only Foundation Model for Time-Series Forecasting},
  author = {Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  year = {2024},
  month = feb,
  number = {arXiv:2310.10688},
  eprint = {2310.10688},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.10688},
  urldate = {2024-02-14},
  abstract = {Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{dasigiDatasetInformationSeekingQuestions2021,
  title = {A {{Dataset}} of {{Information-Seeking Questions}} and {{Answers Anchored}} in {{Research Papers}}},
  author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.03011},
  eprint = {2105.03011},
  archiveprefix = {arxiv}
}

@misc{deluciaUsingNaturalLanguage2024,
  title = {Using {{Natural Language Inference}} to {{Improve Persona Extraction}} from {{Dialogue}} in a {{New Domain}}},
  author = {DeLucia, Alexandra and Zhao, Mengjie and Maeda, Yoshinori and Yoda, Makoto and Yamada, Keiichi and Wakaki, Hiromi},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06742},
  eprint = {2401.06742},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.06742},
  urldate = {2024-01-15},
  abstract = {While valuable datasets such as PersonaChat provide a foundation for training persona-grounded dialogue agents, they lack diversity in conversational and narrative settings, primarily existing in the "real" world. To develop dialogue agents with unique personas, models are trained to converse given a specific persona, but hand-crafting these persona can be time-consuming, thus methods exist to automatically extract persona information from existing character-specific dialogue. However, these persona-extraction models are also trained on datasets derived from PersonaChat and struggle to provide high-quality persona information from conversational settings that do not take place in the real world, such as the fantasy-focused dataset, LIGHT. Creating new data to train models on a specific setting is human-intensive, thus prohibitively expensive. To address both these issues, we introduce a natural language inference method for post-hoc adapting a trained persona extraction model to a new setting. We draw inspiration from the literature of dialog natural language inference (NLI), and devise NLI-reranking methods to extract structured persona information from dialogue. Compared to existing persona extraction models, our method returns higher-quality extracted persona and requires less human annotation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{dettmersLLMInt88bit2022,
  title = {{{LLM}}.Int8(): 8-Bit {{Matrix Multiplication}} for {{Transformers}} at {{Scale}}},
  shorttitle = {{{LLM}}.Int8()},
  author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  year = {2022},
  month = nov,
  number = {arXiv:2208.07339},
  eprint = {2208.07339},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.07339},
  urldate = {2023-04-03},
  abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  journal = {Proceedings of the 2019 Conference of the North},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  urldate = {2023-06-23},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{dingHPCGPTIntegratingLarge2023,
  title = {{{HPC-GPT}}: {{Integrating Large Language Model}} for {{High-Performance Computing}}},
  shorttitle = {{{HPC-GPT}}},
  booktitle = {Proceedings of the {{SC}} '23 {{Workshops}} of {{The International Conference}} on {{High Performance Computing}}, {{Network}}, {{Storage}}, and {{Analysis}}},
  author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto E. and Du, Wan},
  year = {2023},
  month = nov,
  eprint = {2311.12833},
  primaryclass = {cs},
  pages = {951--960},
  doi = {10.1145/3624062.3624172},
  urldate = {2024-01-20},
  abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT's potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Distributed Parallel and Cluster Computing}
}

@article{dingOpenPromptOpensourceFramework2021,
  title = {{{OpenPrompt}}: {{An Open-source Framework}} for {{Prompt-learning}}},
  shorttitle = {{{OpenPrompt}}},
  author = {Ding, Ning and Hu, Shengding and Zhao, Weilin and Chen, Yulin and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.01998 [cs]},
  eprint = {2111.01998},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2111.01998},
  urldate = {2022-04-15},
  abstract = {Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to \$cloze\$-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt-learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc. need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present \{OpenPrompt\}, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. OpenPrompt is publicly released at \{{\textbackslash}url\{ https://github.com/thunlp/OpenPrompt\}\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{dongSurveyIncontextLearning2023,
  title = {A {{Survey}} on {{In-context Learning}}},
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
  year = {2023},
  month = jun,
  number = {arXiv:2301.00234},
  eprint = {2301.00234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.00234},
  urldate = {2023-11-14},
  abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{doThatImportantHow2023,
  title = {``{{That}}'s Important, but...'': {{How Computer Science Researchers Anticipate Unintended Consequences}} of {{Their Research Innovations}}},
  shorttitle = {``{{That}}'s Important, But...''},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Do, Kimberly and Pang, Rock Yuren and Jiang, Jiachen and Reinecke, Katharina},
  year = {2023},
  month = apr,
  pages = {1--16},
  publisher = {ACM},
  address = {Hamburg Germany},
  doi = {10.1145/3544548.3581347},
  urldate = {2023-12-10},
  isbn = {978-1-4503-9421-5},
  langid = {english}
}

@misc{DREAMDomainInvariant,
  title = {{{DREAM}}: {{Domain Invariant}} and {{Contrastive Representation}} for {{Sleep Dynamics}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/10027713},
  urldate = {2024-04-05}
}

@misc{duanAttentionAllYou2020,
  title = {Attention {{Is All You Need}} for {{Chinese Word Segmentation}}},
  author = {Duan, Sufeng and Zhao, Hai},
  year = {2020},
  month = oct,
  number = {arXiv:1910.14537},
  eprint = {1910.14537},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.1910.14537},
  urldate = {2022-06-09},
  abstract = {Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@article{duBiomedicaldomainPretrainedLanguage2020,
  title = {Biomedical-Domain Pre-Trained Language Model for Extractive Summarization},
  author = {Du, Yongping and Li, Qingxiao and Wang, Lulin and He, Yanqing},
  year = {2020},
  month = jul,
  journal = {Knowledge-Based Systems},
  volume = {199},
  pages = {105964},
  issn = {09507051},
  doi = {10/gg286r},
  urldate = {2021-11-30},
  abstract = {In recent years, the performance of deep neural network in extractive summarization task has been improved significantly compared with traditional methods. However, in the field of biomedical extractive summarization, existing methods cannot make good use of the domain-aware external knowledge; furthermore, the document structural feature is omitted by existing deep neural network model. In this paper, we propose a novel model called BioBERTSum to better capture token-level and sentence-level contextual representation, which uses a domain-aware bidirectional language model pre-trained on large-scale biomedical corpora as encoder, and further fine-tunes the language model for extractive text summarization task on single biomedical document. Especially, we adopt a sentence position embedding mechanism, which enables the model to learn the position information of sentences and achieve the structural feature of document. To the best of our knowledge, this is the first work to use the pre-trained language model and fine-tuning strategy for extractive summarization task in the biomedical domain. Experiments on PubMed dataset show that our proposed model outperforms the recent SOTA (state-of-the-art) model by ROUGE-1/2/L.},
  langid = {english}
}

@inproceedings{duDualModelWeighting2021,
  title = {Dual {{Model Weighting Strategy}} and {{Data Augmentation}} in {{Biomedical Question Answering}}},
  booktitle = {Proceedings of the 2021 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Du, Yongping and Yan, Jingya and Zhao, Yiliang and Lu, Yuxuan and Jin, Xingnan},
  year = {2021},
  abstract = {Biomedical Question Answering aims to extract an answer to the given question from a biomedical context. Due to the strong professionalism of specific domain, it's more difficult to build large-scale datasets for specific domain question answering. Existing methods are limited by the lack of training data, and the performance is not as good as in open-domain settings. We propose a model weighting strategy for the final answer prediction in biomedical domain, which combines the advantage of two models, open-domain model QANet and BioBERT pre- trained in Biomedical domain data. Specially, we adopt effective data augmentation strategies to improve the model performance, including slide window, summarization and round-trip trans- lation. The public biomedical dataset collected from PubMed provided by BioASQ is used to evaluate our approach. The results show that the model performance has been improved significantly on BioASQ 6B, 7B and 8B datasets compared to the single model.},
  annotation = {00000}
}

@inproceedings{duGLMGeneralLanguage2022,
  title = {{{GLM}}: {{General Language Model Pretraining}} with {{Autoregressive Blank Infilling}}},
  shorttitle = {{{GLM}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  year = {2022},
  month = may,
  pages = {320--335},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.26},
  urldate = {2023-03-29},
  abstract = {There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\textbackslash}mbox\${\textbackslash}times\$ parameters of BERT Large , demonstrating its generalizability to different downstream tasks.}
}

@inproceedings{duHierarchicalQuestionAwareContext2019,
  title = {Hierarchical {{Question-Aware Context Learning}} with {{Augmented Data}} for {{Biomedical Question Answering}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Du, Yongping and Guo, Wenyang and Zhao, Yiliang},
  year = {2019},
  month = nov,
  pages = {370--375},
  publisher = {IEEE},
  address = {San Diego, CA, USA},
  doi = {10/gnm95v},
  urldate = {2021-11-30},
  abstract = {This paper is concerned with the task of biomedical Question Answering (QA) which refers to extracting an answer to the given question from a biomedical context. Current works have made progress on this task, but they are still severely restricted by the insufficient training data due to the domain-specific nature, which motivates us to further explore a powerful way to solve this problem. We propose a Hierarchical Question-Aware Context Learning (HQACL) model for the biomedical QA task constituted by multi-level attention. The interaction between the question and the context can be captured layer by layer, with multigrained embeddings to strengthen the ability of the language representation. A special training method called DA, including two parts namely domain adaptation and data augmentation, is also introduced to enhance the model performance. Domain adaptation can be defined as pre-training on a large-scale opendomain dataset and fine-tuning on the small training set of the target domain. As for the data augmentation, the Round-trip translation method is adopted to create new data with various expressions, which almost doubles the training set. The public biomedical dataset collected from PubMed provided by BioASQ is used to evaluate our model. The results show that our approach is superior to the best recent solution and achieves a new state of the art.},
  isbn = {978-1-72811-867-3},
  langid = {english}
}

@article{duImprovingBiomedicalQuestion2022,
  title = {Improving {{Biomedical Question Answering}} by {{Data Augmentation}} and {{Model Weighting}}},
  author = {Du, Yongping and Yan, Jingya and Lu, Yuxuan and Zhao, Yiliang and Jin, Xingnan},
  year = {2022},
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  pages = {1--1},
  issn = {1557-9964},
  doi = {10.1109/TCBB.2022.3171388},
  abstract = {Biomedical Question Answering aims to extract an answer to the given question from a biomedical context. Due to the strong professionalism of specific domain, its more difficult to build large-scale datasets for specific domain question answering. Existing methods are limited by the lack of training data, and the performance is not as good as in open-domain settings, especially degrading when facing to the adversarial sample. We try to resolve the above issues. Firstly, effective data augmentation strategies are adopted to improve the model training, including slide window, summarization and round-trip translation. Secondly, we propose a model weighting strategy for the final answer prediction in biomedical domain, which combines the advantage of two models, open-domain model QANet and BioBERT pre-trained in biomedical domain data. Finally, we give adversarial training to reinforce the robustness of the model. The public biomedical dataset collected from PubMed provided by BioASQ challenge is used to evaluate our approach. The results show that the model performance has been improved significantly compared to the single model and other models participated in BioASQ challenge. It can learn richer semantic expression from data augmentation and adversarial samples, which is beneficial to solve more complex question answering problems in biomedical domain.},
  keywords = {Biological system modeling,biomedical question answering,Context modeling,data augmentation,Data models,deep learning,model weighting,Predictive models,Task analysis,Training,Training data}
}

@inproceedings{durrNurseCareDesignInTheWild2020,
  title = {{{NurseCare}}: {{Design}} and '{{In-The-Wild}}' {{Evaluation}} of a {{Mobile System}} to {{Promote}} the {{Ergonomic Transfer}} of {{Patients}}},
  shorttitle = {{{NurseCare}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {D{\"u}rr, Maximilian and Gr{\"o}schel, Carla and Pfeil, Ulrike and Reiterer, Harald},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376851},
  urldate = {2023-08-31},
  abstract = {Nurses are frequently required to transfer patients as part of their daily duties. However, the manual transfer of patients is a major risk factor for injuries to the back. Although the Kinaesthetics Care Conception can help to address this issue, existing support for the integration of the concept into nursing-care practice is low. We present NurseCare, a mobile system that aims to promote the practical application of ergonomic patient transfers based on the Kinaesthetics Care Conception. NurseCare consists of a wearable and a smartphone app. Key features of NurseCare include mobile accessible instructions for ergonomic patient transfers, in-situ feedback for the risky bending of the back, and long-term feedback. We evaluated NurseCare in a nine participant 'in-the-wild' evaluation. Results indicate that NurseCare can facilitate ergonomic work while providing a high user experience adequate to the nurses' work domain, and reveal how NurseCare can be incorporated in given practices.},
  isbn = {978-1-4503-6708-0},
  keywords = {`in-the-wild' evaluation,mobile system,nursing care}
}

@misc{eloundouGPTsAreGPTs2023,
  title = {{{GPTs}} Are {{GPTs}}: {{An Early Look}} at the {{Labor Market Impact Potential}} of {{Large Language Models}}},
  shorttitle = {{{GPTs}} Are {{GPTs}}},
  author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
  year = {2023},
  month = mar,
  number = {arXiv:2303.10130},
  eprint = {2303.10130},
  primaryclass = {cs, econ, q-fin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.10130},
  urldate = {2023-03-21},
  abstract = {We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of GPTs, while around 19\% of workers may see at least 50\% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Economics - General Economics}
}

@article{fabbriSummEvalReevaluatingSummarization2021,
  title = {{{SummEval}}: {{Re-evaluating Summarization Evaluation}}},
  shorttitle = {{{SummEval}}},
  author = {Fabbri, Alexander R. and Kry{\'s}ci{\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  year = {2021},
  month = apr,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {391--409},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00373},
  urldate = {2022-08-02},
  abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.}
}

@misc{fanMuffinChihuahuaChallenging2024,
  title = {Muffin or {{Chihuahua}}? {{Challenging Large Vision-Language Models}} with {{Multipanel VQA}}},
  shorttitle = {Muffin or {{Chihuahua}}?},
  author = {Fan, Yue and Gu, Jing and Zhou, Kaiwen and Yan, Qianqi and Jiang, Shan and Kuo, Ching-Chen and Guan, Xinze and Wang, Xin Eric},
  year = {2024},
  number = {arXiv:2401.15847},
  eprint = {2401.15847},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.15847},
  urldate = {2024-04-08},
  abstract = {Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99{\textbackslash}\% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on LVLMs' multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of LVLMs in understanding multipanel images, we analyze the potential causes for LVLMs' performance and offer insights for enhancement with the synthetic data. Code and data are released at https://sites.google.com/view/multipanelvqa/home.},
  archiveprefix = {arxiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{fastEmergentCrowdscaleProgramming2014,
  title = {Emergent, Crowd-Scale Programming Practice in the {{IDE}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fast, Ethan and Steffee, Daniel and Wang, Lucy and Brandt, Joel R. and Bernstein, Michael S.},
  year = {2014},
  month = apr,
  series = {{{CHI}} '14},
  pages = {2491--2500},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2556288.2556998},
  urldate = {2023-08-31},
  abstract = {While emergent behaviors are uncodified across many domains such as programming and writing, interfaces need explicit rules to support users. We hypothesize that by codifying emergent programming behavior, software engineering interfaces can support a far broader set of developer needs. To explore this idea, we built Codex, a knowledge base that records common practice for the Ruby programming language by indexing over three million lines of popular code. Codex enables new data-driven interfaces for programming systems: statistical linting, identifying code that is unlikely to occur in practice and may constitute a bug; pattern annotation, automatically discovering common programming idioms and annotating them with metadata using expert crowdsourcing; and library generation, constructing a utility package that encapsulates and reflects emergent software practice. We evaluate these applications to find Codex captures a broad swatch of programming practice, statistical linting detects problematic code snippets, and pattern annotation discovers nontrivial idioms such as basic HTTP authentication and database migration templates. Our work suggests that operationalizing practice-driven knowledge in structured domains such as programming can enable a new class of user interfaces.},
  isbn = {978-1-4503-2473-1},
  keywords = {data mining,programming tools}
}

@article{fengPretrainedLanguageEmbeddingbased2020,
  title = {Pre-Trained {{Language Embedding-based Contextual Summary}} and {{Multi-scale Transmission Network}} for {{Aspect Extraction}}},
  author = {Feng, Cong and Rao, Yuan and Nazir, Ambreen and Wu, Lianwei and He, Long},
  year = {2020},
  month = jan,
  journal = {Procedia Computer Science},
  series = {2019 {{International Conference}} on {{Identification}}, {{Information}} and {{Knowledge}} in the {{Internet}} of {{Things}}},
  volume = {174},
  pages = {40--49},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.06.054},
  urldate = {2022-07-18},
  abstract = {With the development of IOT and 5G technology, people's demand for information acquisition is more inclined to accuracy, intelligence and timeliness. How to help designer obtain the real-time information of specific product reviews from the massive online consumers and upgrade the new design strategy has become a hot topic for research. In this paper, we define the problem as an aspect extraction task, and propose a novel deep learning model that comprises of three modules: pre-training language model embedding, multi-scale transmission network and contextual summary, which aims to provide an end-to-end solution without any additional supervision. To this end, we adopt BERT to overcome the disadvantage of traditional embedding methods, which cannot combine contextual information. Multi-scale transmission network is proposed to integrate the Bi-GRU and a group of CNN networks to extract sequential and local features of words respectively. Contextual summary is a tailor-made representation distilled from the input sentence, conditioned on each current word, and thus can assist aspect prediction. Experimental results over three benchmark SemEval datasets clearly illustrate that our model can achieve the state-of-the-art performance.},
  langid = {english},
  keywords = {contextual summary,multi-scale transmission network,pre-trained language embedding}
}

@article{fraserExtractingUMLSConcepts,
  title = {Extracting {{UMLS Concepts}} from {{Medical Text Using General}} and {{Domain-Specific Deep Learning Models}}},
  author = {Fraser, Kathleen C and Nejadgholi, Isar and Bruijn, Berry De and Li, Muqun and LaPlante, Astha and Abidine, Khaldoun Zine El},
  pages = {11},
  abstract = {Entity recognition is a critical first step to a number of clinical NLP applications, such as entity linking and relation extraction. We present the first attempt to apply state-of-theart entity recognition approaches on a newly released dataset, MedMentions. This dataset contains over 4000 biomedical abstracts, annotated for UMLS semantic types. In comparison to existing datasets, MedMentions contains a far greater number of entity types, and thus represents a more challenging but realistic scenario in a real-world setting. We explore a number of relevant dimensions, including the use of contextual versus non-contextual word embeddings, general versus domain-specific unsupervised pre-training, and different deep learning architectures. We contrast our results against the well-known i2b2 2010 entity recognition dataset, and propose a new method to combine general and domain-specific information. While producing a state-of-the-art result for the i2b2 2010 task (F1 = 0.90), our results on MedMentions are significantly lower (F1 = 0.63), suggesting there is still plenty of opportunity for improvement on this new data.},
  langid = {english}
}

@inproceedings{fuATNetAnsweringClozeStyle2019,
  title = {{{ATNet}}: {{Answering Cloze-Style Questions}} via {{Intra-attention}} and {{Inter-attention}}},
  shorttitle = {{{ATNet}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Fu, Chengzhen and Li, Yuntao and Zhang, Yan},
  editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {242--252},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-16145-3_19},
  abstract = {This paper proposes a novel framework, named ATNet, for answering cloze-style questions over documents. Our model, in the encoder phase, projects all contextual embeddings into multiple latent semantic spaces, with representations of each space attending to a specific aspect of semantics. Long-term dependencies among the whole document are captured via the intra-attention module. A gate is produced to control the degree to which the retrieved dependency information is fused and the previous token embedding is exposed. Then, in the interaction phase, the context is aligned with the query across different semantic spaces to achieve the information aggregation. Specifically, we compute inter-attention based on a sophisticated feature set. Experiments and ablation studies demonstrate the effectiveness of ATNet.},
  isbn = {978-3-030-16145-3},
  langid = {english},
  keywords = {Inter-attention,Intra-attention,Question answering}
}

@inproceedings{fuEAReaderEnhance2019,
  title = {{{EA Reader}}: {{Enhance Attentive Reader}} for {{Cloze-Style Question Answering}} via {{Multi-Space Context Fusion}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Fu, Chengzhen and Zhang, Yan},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {6375--6382},
  doi = {10.1609/aaai.v33i01.33016375},
  urldate = {2022-04-11},
  abstract = {\&lt;p\&gt;Query-document semantic interactions are essential for the success of many cloze-style question answering models. Recently, researchers have proposed several attention-based methods to predict the answer by focusing on appropriate subparts of the context document. In this paper, we design a novel module to produce the query-aware context vector, named Multi-Space based Context Fusion (MSCF), with the following considerations: (1) interactions are applied across multiple latent semantic spaces; (2) attention is measured at bit level, not at token level. Moreover, we extend MSCF to the multi-hop architecture. This unified model is called Enhanced Attentive Reader (EA Reader). During the iterative inference process, the reader is equipped with a novel memory update rule and maintains the understanding of documents through \&lt;em\&gt;read\&lt;/em\&gt;, \&lt;em\&gt;update\&lt;/em\&gt; and \&lt;em\&gt;write\&lt;/em\&gt; operations. We conduct extensive experiments on four real-world datasets. Our results demonstrate that EA Reader outperforms state-of-the-art models.\&lt;/p\&gt;},
  chapter = {AAAI Technical Track: Natural Language Processing}
}

@misc{ganModelasaServiceMaaSSurvey2023,
  title = {Model-as-a-{{Service}} ({{MaaS}}): {{A Survey}}},
  shorttitle = {Model-as-a-{{Service}} ({{MaaS}})},
  author = {Gan, Wensheng and Wan, Shicheng and Yu, Philip S.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05804},
  eprint = {2311.05804},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.05804},
  urldate = {2024-02-13},
  abstract = {Due to the increased number of parameters and data in the pre-trained model exceeding a certain level, a foundation model (e.g., a large language model) can significantly improve downstream task performance and emerge with some novel special abilities (e.g., deep learning, complex reasoning, and human alignment) that were not present before. Foundation models are a form of generative artificial intelligence (GenAI), and Model-as-a-Service (MaaS) has emerged as a groundbreaking paradigm that revolutionizes the deployment and utilization of GenAI models. MaaS represents a paradigm shift in how we use AI technologies and provides a scalable and accessible solution for developers and users to leverage pre-trained AI models without the need for extensive infrastructure or expertise in model training. In this paper, the introduction aims to provide a comprehensive overview of MaaS, its significance, and its implications for various industries. We provide a brief review of the development history of "X-as-a-Service" based on cloud computing and present the key technologies involved in MaaS. The development of GenAI models will become more democratized and flourish. We also review recent application studies of MaaS. Finally, we highlight several challenges and future issues in this promising area. MaaS is a new deployment and service paradigm for different AI-based models. We hope this review will inspire future research in the field of MaaS.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@misc{gaoCollabCoderGPTPoweredWorkflow2023a,
  title = {{{CollabCoder}}: {{A GPT-Powered Workflow}} for {{Collaborative Qualitative Analysis}}},
  shorttitle = {{{CollabCoder}}},
  author = {Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  year = {2023},
  month = jul,
  number = {arXiv:2304.07366},
  eprint = {2304.07366},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.07366},
  urldate = {2023-09-10},
  abstract = {The Collaborative Qualitative Analysis (CQA) process can be time-consuming and resource-intensive, requiring multiple discussions among team members to refine codes and ideas before reaching a consensus. To address these challenges, we introduce CollabCoder, a system leveraging Large Language Models (LLMs) to support three CQA stages: independent open coding, iterative discussions, and the development of a final codebook. In the independent open coding phase, CollabCoder provides AI-generated code suggestions on demand, and allows users to record coding decision-making information (e.g. keywords and certainty) as support for the process. During the discussion phase, CollabCoder helps to build mutual understanding and productive discussion by sharing coding decision-making information with the team. It also helps to quickly identify agreements and disagreements through quantitative metrics, in order to build a final consensus. During the code grouping phase, CollabCoder employs a top-down approach for primary code group recommendations, reducing the cognitive burden of generating the final codebook. An evaluation involving 16 users confirmed the usability and effectiveness of CollabCoder and offered empirical insights into the LLMs' roles in CQA.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction}
}

@misc{gehrmannGEMBenchmarkNatural2021,
  title = {The {{GEM Benchmark}}: {{Natural Language Generation}}, Its {{Evaluation}} and {{Metrics}}},
  shorttitle = {The {{GEM Benchmark}}},
  author = {Gehrmann, Sebastian and Adewumi, Tosin and Aggarwal, Karmanya and Ammanamanchi, Pawan Sasanka and Anuoluwapo, Aremu and Bosselut, Antoine and Chandu, Khyathi Raghavi and Clinciu, Miruna and Das, Dipanjan and Dhole, Kaustubh D. and Du, Wanyu and Durmus, Esin and Du{\v s}ek, Ond{\v r}ej and Emezue, Chris and Gangal, Varun and Garbacea, Cristina and Hashimoto, Tatsunori and Hou, Yufang and Jernite, Yacine and Jhamtani, Harsh and Ji, Yangfeng and Jolly, Shailza and Kale, Mihir and Kumar, Dhruv and Ladhak, Faisal and Madaan, Aman and Maddela, Mounica and Mahajan, Khyati and Mahamood, Saad and Majumder, Bodhisattwa Prasad and Martins, Pedro Henrique and {McMillan-Major}, Angelina and Mille, Simon and {van Miltenburg}, Emiel and Nadeem, Moin and Narayan, Shashi and Nikolaev, Vitaly and Niyongabo, Rubungo Andre and Osei, Salomey and Parikh, Ankur and {Perez-Beltrachini}, Laura and Rao, Niranjan Ramesh and Raunak, Vikas and Rodriguez, Juan Diego and Santhanam, Sashank and Sedoc, Jo{\~a}o and Sellam, Thibault and Shaikh, Samira and Shimorina, Anastasia and Cabezudo, Marco Antonio Sobrevilla and Strobelt, Hendrik and Subramani, Nishant and Xu, Wei and Yang, Diyi and Yerukola, Akhila and Zhou, Jiawei},
  year = {2021},
  month = apr,
  number = {arXiv:2102.01672},
  eprint = {2102.01672},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2102.01672},
  urldate = {2022-08-24},
  abstract = {We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{geovanan.macarioAnnotatingDataSupport2010,
  title = {Annotating Data to Support Decision-Making: A Case Study},
  shorttitle = {Annotating Data to Support Decision-Making},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Geographic Information Retrieval}}},
  author = {Geovana N. Mac{\'a}rio, Carla and {dos Santos}, Jefersson A. and Medeiros, Claudia Bauzer and {da S. Torres}, Ricardo},
  year = {2010},
  month = feb,
  series = {{{GIR}} '10},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1722080.1722106},
  urldate = {2024-03-27},
  abstract = {Georeferenced data are a key factor in many decision-making systems. However, their interpretation is user and context dependent so that, for each situation, data analysts have to interpret them, a time-consuming task. One approach to alleviate this task, is the use of semantic annotations to store the produced information. Annotating data is however hard to perform and prone to errors, especially when executed manually. This difficulty increases with the amount of data to annotate. Moreover, annotation requires multi-disciplinary collaboration of researchers, with access to heterogeneous and distributed data sources and scientific computations. This paper illustrates our solution to approach this problem by means of a case study in agriculture. It shows how our implementation of a framework to automate the annotation of geospatial data can be used to process real data from remote sensing images and other official Brazilian data sources.},
  isbn = {978-1-60558-826-1},
  keywords = {geospatial data,geospatial standards,remote sensing image classification,semantic annotation}
}

@misc{GetYourFree,
  title = {Get Your Free Score and More},
  journal = {Intuit Credit Karma},
  url = {https://www.creditkarma.com/credit-health/transunion/main},
  urldate = {2024-04-09},
  abstract = {See your full credit report, credit-building tips and more with Intuit Credit Karma ---- all totally free. And it's not like the fake free, but the real 100\% free, free.},
  langid = {english}
}

@article{gilardiChatGPTOutperformsCrowdWorkers2023,
  title = {{{ChatGPT Outperforms Crowd-Workers}} for {{Text-Annotation Tasks}}},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  year = {2023},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {30},
  eprint = {2303.15056},
  primaryclass = {cs},
  pages = {e2305016120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2305016120},
  urldate = {2023-10-26},
  abstract = {Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate = {2023-09-18},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english}
}

@inproceedings{gordonJuryLearningIntegrating2022,
  title = {Jury {{Learning}}: {{Integrating Dissenting Voices}} into {{Machine Learning Models}}},
  shorttitle = {Jury {{Learning}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gordon, Mitchell L. and Lam, Michelle S. and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S.},
  year = {2022},
  month = apr,
  pages = {1--19},
  publisher = {ACM},
  address = {New Orleans LA USA},
  doi = {10.1145/3491102.3502004},
  urldate = {2023-10-16},
  isbn = {978-1-4503-9157-3},
  langid = {english}
}

@misc{goswamiMOMENTFamilyOpen2024,
  title = {{{MOMENT}}: {{A Family}} of {{Open Time-series Foundation Models}}},
  shorttitle = {{{MOMENT}}},
  author = {Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03885},
  eprint = {2402.03885},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.03885},
  urldate = {2024-02-15},
  abstract = {We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time-series models. Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{goyalThinkYouSpeak2023,
  title = {Think before You Speak: {{Training Language Models With Pause Tokens}}},
  shorttitle = {Think before You Speak},
  author = {Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02226},
  eprint = {2310.02226},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.02226},
  urldate = {2023-10-05},
  abstract = {Language models generate responses by producing a series of tokens in immediate succession: the \$(K+1)\^{}\{th\}\$ token is an outcome of manipulating \$K\$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, \$K+10\$ hidden vectors, before it outputs the \$(K+1)\^{}\{th\}\$ token? We operationalize this idea by performing training and inference on language models with a (learnable) \${\textbackslash}textit\{pause\}\$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate \${\textbackslash}textit\{pause-training\}\$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of \$18{\textbackslash}\%\$ EM score on the QA task of SQuAD, \$8{\textbackslash}\%\$ on CommonSenseQA and \$1{\textbackslash}\%\$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{GraphCareEnhancingHealthcare2023,
  title = {{{GraphCare}}: {{Enhancing Healthcare Predictions}} with {{Personalized Knowledge Graphs}}},
  shorttitle = {{{GraphCare}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=tVTN7Zs0ml},
  urldate = {2024-01-11},
  abstract = {Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% and 10.8\% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.},
  langid = {english}
}

@inproceedings{gruverLargeLanguageModels2023,
  title = {Large {{Language Models Are Zero-Shot Time Series Forecasters}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Gruver, Nate and Finzi, Marc Anton and Qiu, Shikai and Wilson, Andrew Gordon},
  year = {2023},
  month = nov,
  url = {https://openreview.net/forum?id=md68e8iZK1},
  urldate = {2024-01-10},
  abstract = {By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.},
  langid = {english}
}

@misc{guanLongTextGeneration2021,
  title = {Long {{Text Generation}} by {{Modeling Sentence-Level}} and {{Discourse-Level Coherence}}},
  author = {Guan, Jian and Mao, Xiaoxi and Fan, Changjie and Liu, Zitao and Ding, Wenbiao and Huang, Minlie},
  year = {2021},
  month = may,
  number = {arXiv:2105.08963},
  eprint = {2105.08963},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.08963},
  urldate = {2022-11-23},
  abstract = {Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{guanLongTextGeneration2021a,
  title = {Long {{Text Generation}} by {{Modeling Sentence-Level}} and {{Discourse-Level Coherence}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Guan, Jian and Mao, Xiaoxi and Fan, Changjie and Liu, Zitao and Ding, Wenbiao and Huang, Minlie},
  year = {2021},
  month = aug,
  pages = {6379--6393},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.499},
  urldate = {2022-11-15},
  abstract = {Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.}
}

@article{guDomainSpecificLanguageModel2021,
  title = {Domain-{{Specific Language Model Pretraining}} for {{Biomedical Natural Language Processing}}},
  author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  year = {2021},
  month = oct,
  journal = {ACM Transactions on Computing for Healthcare},
  volume = {3},
  number = {1},
  pages = {2:1--2:23},
  issn = {2691-1957},
  doi = {10.1145/3458754},
  urldate = {2022-04-12},
  abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
  keywords = {Biomedical,domain-specific pretraining,NLP}
}

@article{GuJiYuShenJingWangLuoDeJiQiYueDuLiJieZongShu2020,
  title = {{}},
  author = {,  and ,  and ,  and ,  and , },
  year = {2020},
  journal = {},
  volume = {31},
  number = {7},
  pages = {2095--2126},
  issn = {1000-9825},
  doi = {10.13328/j.cnki.jos.006048},
  abstract = {,.,.,,,.:,;,,,BERT;,,,;.},
  langid = {chinese},
  keywords = {,attention mechanism,machine reading comprehension,natural language processing,neural reading comprehension model},
  annotation = {26 citations(CNKI)[3-28-2022]{$<$}, EI, CSCD{$>$}}
}

@inproceedings{guoCalibrationModernNeural2017a,
  title = {On Calibration of Modern Neural Networks},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = aug,
  series = {{{ICML}}'17},
  pages = {1321--1330},
  publisher = {JMLR.org},
  address = {Sydney, NSW, Australia},
  urldate = {2024-01-02},
  abstract = {Confidence calibration - the problem of predicting probability estimates representative of the true correctness likelihood - is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling - a single-parameter variant of Platt Scaling - is surprisingly effective at calibrating predictions.}
}

@misc{guoConditionalTextGeneration2020,
  title = {Conditional {{Text Generation}} for {{Harmonious Human-Machine Interaction}}},
  author = {Guo, Bin and Wang, Hao and Ding, Yasan and Wu, Wei and Hao, Shaoyang and Sun, Yueqi and Yu, Zhiwen},
  year = {2020},
  month = dec,
  number = {arXiv:1909.03409},
  eprint = {1909.03409},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1909.03409},
  urldate = {2022-08-24},
  abstract = {In recent years, with the development of deep learning, text generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text generation technology, that is the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that many efforts have been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summary several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{guoContinuousTrainingFinetuning2023,
  title = {Continuous {{Training}} and {{Fine-tuning}} for {{Domain-Specific Language Models}} in {{Medical Question Answering}}},
  author = {Guo, Zhen and Hua, Yining},
  year = {2023},
  month = oct,
  number = {arXiv:2311.00204},
  eprint = {2311.00204},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.00204},
  urldate = {2024-01-20},
  abstract = {Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas where pre-trained models lack the required expertise, such as law, science, and engineering.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{guoImprovingSmallLanguage2023,
  title = {Improving {{Small Language Models}} on {{PubMedQA}} via {{Generative Data Augmentation}}},
  author = {Guo, Zhen and Wang, Peiqi and Wang, Yanwei and Yu, Shangdi},
  year = {2023},
  month = aug,
  number = {arXiv:2305.07804},
  eprint = {2305.07804},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.07804},
  urldate = {2024-01-20},
  abstract = {Large Language Models (LLMs) have made remarkable advancements in the field of natural language processing. However, their increasing size poses challenges in terms of computational cost. On the other hand, Small Language Models (SLMs) are known for their efficiency, but they often struggle with limited capacity and training data, especially in specific domains. In this paper, we introduce a novel method aimed at improving SLMs in the medical domain using LLM-based generative data augmentation. The objective of our approach is to develop more efficient and capable models that are specifically tailored for specialized applications. Through experiments conducted on the PubMedQA dataset, we demonstrate the effectiveness of LLMs in refining and diversifying existing question-answer pairs. This refinement process leads to improved performance in a significantly smaller model after fine-tuning. Notably, our best SLM, with under 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA dataset. Our code and generated data are publicly available to facilitate further explorations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{guoLongT5EfficientTextToText2022,
  title = {{{LongT5}}: {{Efficient Text-To-Text Transformer}} for {{Long Sequences}}},
  shorttitle = {{{LongT5}}},
  author = {Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei},
  year = {2022},
  month = may,
  number = {arXiv:2112.07916},
  eprint = {2112.07916},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.07916},
  urldate = {2023-12-15},
  abstract = {Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call \{{\textbackslash}em Transient Global\} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{guoWhatCanLarge2023a,
  title = {What Can {{Large Language Models}} Do in Chemistry? {{A}} Comprehensive Benchmark on Eight Tasks},
  shorttitle = {What Can {{Large Language Models}} Do in Chemistry?},
  author = {Guo, Taicheng and Guo, Kehan and Nan, Bozhao and Liang, Zhenwen and Guo, Zhichun and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18365},
  eprint = {2305.18365},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18365},
  urldate = {2024-01-14},
  abstract = {Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{gururanganDonStopPretraining2020,
  title = {Don't {{Stop Pretraining}}: {{Adapt Language Models}} to {{Domains}} and {{Tasks}}},
  shorttitle = {Don't {{Stop Pretraining}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  year = {2020},
  pages = {8342--8360},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.740},
  urldate = {2022-04-12},
  abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.}
}

@article{GuYingJieGuiXiaoLinLiDeFuShenYiLiaoDongJiYuShenJingWangLuoDeJiQiYueDuLiJieZongShu2020,
  title = {},
  author = { and {Ying-Jie}, {\relax GU} and {Xiao-Lin}, {\relax GUI} and {De-Fu}, {\relax LI} and Yi, Shen and Dong, Liao},
  year = {2020},
  month = apr,
  journal = {},
  volume = {31},
  number = {7},
  pages = {2095--2126},
  url = {http://www.jos.org.cn/jos/article/abstract/6048},
  urldate = {2021-12-25},
  abstract = {...BERT.;The task of machine reading comprehension is to make the machine understand natural language text and correctly answer text-related questions. Due to the limitation of the dataset scale, most of the early machine reading comprehension methods were modeled based on manual features and traditional machine learning methods. In recent years, with the development of knowledge bases and crowdsourcing, high quality large-scale datasets have been proposed by researchers, which has brought a new opportunity for the advance of neural network models and machine reading comprehension. In this survey, an exhaustive review on the state-of-the-art research efforts on machine reading comprehension based on neural network is made. First, an overview of machine reading comprehension, including development process, problem formulation, and evaluation metric, is given. Then, a comprehensive review is conducted of related technologies in the most fashionable neural reading comprehension framework including the embedding layer, encoder layer, interaction layer, and output layer as well as the latest BERT pre-training model and its advantages are discussed. After that, this paper concludes the recent research progress of machine reading comprehension datasets and neural reading comprehension model, and gives a comparison and analysis of the most representative datasets and neural network models in detail. Finally, the research challenges and future direction of machine reading comprehension are presented.},
  annotation = {00000}
}

@misc{haCloChatUnderstandingHow2024,
  title = {{{CloChat}}: {{Understanding How People Customize}}, {{Interact}}, and {{Experience Personas}} in {{Large Language Models}}},
  shorttitle = {{{CloChat}}},
  author = {Ha, Juhye and Jeon, Hyeon and Han, DaEun and Seo, Jinwook and Oh, Changhoon},
  year = {2024},
  number = {arXiv:2402.15265},
  eprint = {2402.15265},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15265},
  urldate = {2024-04-07},
  abstract = {Large language models (LLMs) have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing LLM-driven conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in LLMs. We conducted a study comparing how participants interact with CloChat and ChatGPT. The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using LLMs.},
  archiveprefix = {arxiv},
  langid = {american},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@inproceedings{hanOpenKEOpenToolkit2018,
  title = {{{OpenKE}}: {{An Open Toolkit}} for {{Knowledge Embedding}}},
  shorttitle = {{{OpenKE}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Han, Xu and Cao, Shulin and Lv, Xin and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Li, Juanzi},
  year = {2018},
  month = nov,
  pages = {139--144},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/d18-2024},
  urldate = {2021-12-06},
  abstract = {We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes operational efficiency to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new models into the framework. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on http://openke.thunlp.org/.}
}

@inproceedings{harshaAutomatedResumeScreener2022,
  title = {Automated {{Resume Screener}} Using {{Natural Language Processing}}({{NLP}})},
  booktitle = {2022 6th {{International Conference}} on {{Trends}} in {{Electronics}} and {{Informatics}} ({{ICOEI}})},
  author = {Harsha, Tumula Mani and Moukthika, Gangaraju Sai and Sai, Dudipalli Siva and Pravallika, Mannuru Naga Rajeswari and Anamalamudi, Satish and Enduri, MuraliKrishna},
  year = {2022},
  month = apr,
  pages = {1772--1777},
  doi = {10.1109/ICOEI53556.2022.9777194},
  abstract = {Resume Screening is the process of evaluating the resume of the job seekers based on a specific requirement. It is used to identify the candidate eligibility for a job by matching all the requirements needed for the offered role with their resume information such as education qualification, skill sets, technical stuff etc. Resume Screening is a crucial stage in candidate's selection for a job role, it is the stage where the decision making is done whether to move the candidate to the next level of hiring process or not. Traditionally, this process is performed manually, but companies often receive thousands of resumes for job applications. In order to reduce the human involvement and errors, many new ways were introduced in this process. This paper discusses about one such process which is very efficient in performing Resume screening. It includes Natural Language Processing (NLP), an automated Machine Learning Algorithm for screening the resumes. This paper explains the end to end working of a python application which efficiently screens the resumes of the candidates based on the organization's requirement.},
  keywords = {Companies,Decision making,Education,Hiring Process,Machine learning algorithms,Market research,Natural language processing,NLP,Resume Screening,Resumes,Skill Set}
}

@incollection{heConstructingKnowledgeGraph2020,
  title = {Constructing {{Knowledge Graph}} for {{Social Networks}} in {{A Deep}} and {{Holistic Way}}},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2020},
  author = {He, Qi and Yang, Jaewon and Shi, Baoxu},
  year = {2020},
  month = apr,
  pages = {307--308},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3366424.3383112},
  urldate = {2022-07-12},
  abstract = {Online social networks such as Facebook and LinkedIn have been an integrated part of everyday life. To improve the user experience and power the products around the social network, Knowledge Graphs (KG) are used as a standard way to extract and organize the knowledge in social networks. This tutorial focuses on how to build KGs for social networks by developing deep NLP models, and holistic optimization of KGs and the social network. Building KG for social networks poses two challenges: 1) input data for each member in the social network is noisy, implicit and in multilingual, so a deep understanding of the input data is needed; 2) KG and the social network influence each other via multiple organic feedback loops, so a holistic view on both networks is needed. We will share the lessons we learned from tackling the above challenges in the past seven years on building the Knowledge Graph for the LinkedIn social network. To address the first challenge of noisy and implicit input data, we present how to train high precision language understanding models by adding small clean data to the noisy data. By doing so, we enhance the-state-of-the-art NLP models such as BERT for building KG. To address multilingual aspect of the input data, we explain how to expand a single-language KG to multilingual KGs by applying transfer learning. For the second challenge of modeling interactions between social network and KG, we launch new products to get explicit feedback on KG from users, and refine KG by learning deep embeddings from the social network. Lastly, we present how we use our KG to empower more than 20+ products at LinkedIn with high business impacts.},
  isbn = {978-1-4503-7024-0},
  keywords = {Knowledge Graph,Knowledge Graph Construction,Social Network}
}

@misc{hendrycksCUADExpertAnnotatedNLP2021,
  title = {{{CUAD}}: {{An Expert-Annotated NLP Dataset}} for {{Legal Contract Review}}},
  shorttitle = {{{CUAD}}},
  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
  year = {2021},
  month = nov,
  number = {arXiv:2103.06268},
  eprint = {2103.06268},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.06268},
  urldate = {2023-10-23},
  abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{hermannTeachingMachinesRead2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html},
  urldate = {2022-01-14},
  annotation = {02578}
}

@inproceedings{hillGoldilocksPrincipleReading2016,
  title = {The {{Goldilocks Principle}}: {{Reading Children}}'s {{Books}} with {{Explicit Memory Representations}}},
  shorttitle = {The {{Goldilocks Principle}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016},
  url = {http://arxiv.org/abs/1511.02301},
  urldate = {2022-01-20},
  annotation = {00549}
}

@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11239},
  urldate = {2023-11-14},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{hortonLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Simulated Economic Agents}}: {{What Can We Learn}} from {{Homo Silicus}}?},
  shorttitle = {Large {{Language Models}} as {{Simulated Economic Agents}}},
  author = {Horton, John J.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.07543},
  eprint = {2301.07543},
  primaryclass = {econ, q-fin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.07543},
  urldate = {2023-06-25},
  abstract = {Newly-developed large language models (LLM) -- because of how they are trained and designed -- are implicit computational models of humans -- a homo silicus. These models can be used the same way economists use homo economicus: they can be given endowments, information, preferences, and so on and then their behavior can be explored in scenarios via simulation. I demonstrate this approach using OpenAI's GPT3 with experiments derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986) and Samuelson and Zeckhauser (1988). The findings are qualitatively similar to the original results, but it is also trivially easy to try variations that offer fresh insights. Departing from the traditional laboratory paradigm, I also create a hiring scenario where an employer faces applicants that differ in experience and wage ask and then analyze how a minimum wage affects realized wages and the extent of labor-labor substitution.},
  archiveprefix = {arxiv},
  keywords = {Economics - General Economics}
}

@article{horvitzReflectionsChallengesPromises,
  title = {Reflections on {{Challenges}} and {{Promises}} of {{Mixed-Initiative Interaction}}},
  author = {Horvitz, Eric},
  abstract = {Research on mixed-initiative interaction and assistance is still in its infancy but is poised to blossom into a wellspring of innovation that promises to change the way we work with computing systems---and the way that computing systems work with us. I share reflections about the opportunities ahead for developing computational systems with the ability to engage people in a deeply collaborative manner, founded on their ability to support fluid mixed-initiative problem solving.},
  langid = {english}
}

@misc{HowACHWorks2014,
  title = {How {{ACH}} Works: {{A}} Developer Perspective - {{Part}} 3},
  shorttitle = {How {{ACH}} Works},
  year = {2014},
  month = jul,
  journal = {Gusto Engineering},
  url = {https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-3/},
  urldate = {2023-10-06},
  abstract = {At Gusto [https://gusto.com], we rely heavily on the ACH network to pay employees and to remit various payroll taxes to federal and state agencies on behalf of our clients. In part 1 [https://engineering.gusto.com/how-ach-works-a-developer-perspective-part-1/] of this post, I outlined the basics of how we originate},
  langid = {english}
}

@misc{huangCalibratingLongformGenerations2024,
  title = {Calibrating {{Long-form Generations}} from {{Large Language Models}}},
  author = {Huang, Yukun and Liu, Yixin and Thirukovalluru, Raghuveer and Cohan, Arman and Dhingra, Bhuwan},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06544},
  eprint = {2402.06544},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.06544},
  urldate = {2024-02-12},
  abstract = {To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{huangChatGPTBetterHuman2023,
  title = {Is {{ChatGPT}} Better than {{Human Annotators}}? {{Potential}} and {{Limitations}} of {{ChatGPT}} in {{Explaining Implicit Hate Speech}}},
  shorttitle = {Is {{ChatGPT}} Better than {{Human Annotators}}?},
  author = {Huang, Fan and Kwak, Haewoon and An, Jisun},
  year = {2023},
  month = mar,
  eprint = {2302.07736},
  primaryclass = {cs},
  doi = {10.1145/3543873.3587368},
  urldate = {2023-03-29},
  abstract = {Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@article{huangImprovedKnowledgeBase2016,
  title = {Improved {{Knowledge Base Completion}} by {{Path-Augmented TransR Model}}},
  author = {Huang, Wenhao and Li, Ge and Jin, Zhi},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.04073 [cs]},
  eprint = {1610.04073},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1610.04073},
  urldate = {2022-02-06},
  abstract = {Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we base PTransR model on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {00011}
}

@phdthesis{HuJiQiYueDuLiJieYuWenBenWenDaJiShuYanJiu2019,
  type = {{}},
  title = {{}},
  author = {, },
  year = {2019},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2022&filename=1021828429.nh&v=},
  abstract = {,,,,,,,,,,,:1);2);3),;4),;5),,,-:,,,,,-,,,,,------,,,,,,+,,,,2.0,,-,--,-,,,,,-,,,,,},
  collaborator = {,  and , },
  langid = {chinese},
  school = {},
  keywords = {,Answer Verification,Attention,Discrete Reasoning,Machine Reading Comprehension,Open-Domain Question Answering,Textual Question Answering},
  annotation = {2 citations(CNKI)[3-24-2022]}
}

@misc{huPlanningorientedAutonomousDriving2023,
  title = {Planning-Oriented {{Autonomous Driving}}},
  author = {Hu, Yihan and Yang, Jiazhi and Chen, Li and Li, Keyu and Sima, Chonghao and Zhu, Xizhou and Chai, Siqi and Du, Senyao and Lin, Tianwei and Wang, Wenhai and Lu, Lewei and Jia, Xiaosong and Liu, Qiang and Dai, Jifeng and Qiao, Yu and Li, Hongyang},
  year = {2023},
  month = mar,
  number = {arXiv:2212.10156},
  eprint = {2212.10156},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10156},
  urldate = {2023-06-23},
  abstract = {Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics}
}

@inproceedings{huTaichiProgrammingLanguage2020,
  title = {The {{Taichi}} Programming Language},
  booktitle = {{{ACM SIGGRAPH}} 2020 {{Courses}}},
  author = {Hu, Yuanming},
  year = {2020},
  month = aug,
  series = {{{SIGGRAPH}} '20},
  pages = {1--50},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3388769.3407493},
  urldate = {2022-02-20},
  isbn = {978-1-4503-7972-4}
}

@misc{idreesFrameworkRealisticSimulation2023,
  title = {A {{Framework}} for {{Realistic Simulation}} of {{Daily Human Activity}}},
  author = {Idrees, Ifrah and Singh, Siddharth and Xu, Kerui and Glas, Dylan F.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.15400},
  eprint = {2311.15400},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.15400},
  urldate = {2024-01-27},
  abstract = {For social robots like Astro which interact with and adapt to the daily movements of users within the home, realistic simulation of human activity is needed for feature development and testing. This paper presents a framework for simulating daily human activity patterns in home environments at scale, supporting manual configurability of different personas or activity patterns, variation of activity timings, and testing on multiple home layouts. We introduce a method for specifying day-to-day variation in schedules and present a bidirectional constraint propagation algorithm for generating schedules from templates. We validate the expressive power of our framework through a use case scenario analysis and demonstrate that our method can be used to generate data closely resembling human behavior from three public datasets and a self-collected dataset. Our contribution supports systematic testing of social robot behaviors at scale, enables procedural generation of synthetic datasets of human movement in different households, and can help minimize bias in training data, leading to more robust and effective robots for home environments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics}
}

@article{ishidaWeNeedZero2021,
  title = {Do {{We Need Zero Training Loss After Achieving Zero Training Error}}?},
  author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  year = {2021},
  month = mar,
  journal = {arXiv:2002.08709 [cs, stat]},
  eprint = {2002.08709},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.08709},
  urldate = {2021-12-12},
  abstract = {Overparameterized deep networks have the capacity to memorize training data with zero {\textbackslash}emph\{training error\}. Even after memorization, the {\textbackslash}emph\{training loss\} continues to approach zero, making the model overconfident and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, it is hard to tune their hyperparameters in order to maintain a fixed/preset level of training loss. We propose a direct solution called {\textbackslash}emph\{flooding\} that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the {\textbackslash}emph\{flood level\}. Our approach makes the loss float around the flood level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flood level. This can be implemented with one line of code and is compatible with any stochastic optimizer and other regularizers. With flooding, the model will continue to "random walk" with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization. We experimentally show that flooding improves performance and, as a byproduct, induces a double descent curve of the test loss.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{izacardFewshotLearningRetrieval2022,
  title = {Few-Shot {{Learning}} with {{Retrieval Augmented Language Models}}},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and {Dwivedi-Yu}, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03299},
  eprint = {2208.03299},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.03299},
  urldate = {2022-08-24},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{jainCoarseTuningModelsCode2023,
  title = {Coarse-{{Tuning Models}} of {{Code}} with {{Reinforcement Learning Feedback}}},
  author = {Jain, Abhinav and Adiole, Chima and Chaudhuri, Swarat and Reps, Thomas and Jermaine, Chris},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18341},
  eprint = {2305.18341},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.18341},
  urldate = {2024-02-20},
  abstract = {Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, these models are trained using next-token prediction, which ignores the syntax and semantics of code. We propose RLCF, that further trains a pre-trained LLM via reinforcement learning, using feedback from a grounding function that scores the quality of the code. The grounding function uses (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM that compares the generated code to a reference code. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF raises the odds that an LLM-generated program compiles, is executable, and produces the right output on tests, often allowing LLMs to match the performance of 2x-8x larger LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages}
}

@inproceedings{jainNoTitleFound2019,
  title = {[{{No}} Title Found]},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  pages = {3543--3556},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1357},
  urldate = {2023-01-30},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful ``explanations'' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.},
  langid = {english}
}

@article{jiangHowCanWe2021,
  title = {How {{Can We Know When Language Models Know}}? {{On}} the {{Calibration}} of {{Language Models}} for {{Question Answering}}},
  shorttitle = {How {{Can We Know When Language Models Know}}?},
  author = {Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  year = {2021},
  month = sep,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {962--977},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00407},
  urldate = {2023-12-20},
  abstract = {Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, ``How can we know when language models know, with confidence, the answer to a particular query?'' We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models---T5, BART, and GPT-2---and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.}
}

@inproceedings{jiaRepresentationJobSkillArtificial2018,
  title = {Representation of {{Job-Skill}} in {{Artificial Intelligence}} with {{Knowledge Graph Analysis}}},
  booktitle = {2018 {{IEEE Symposium}} on {{Product Compliance Engineering}} - {{Asia}} ({{ISPCE-CN}})},
  author = {Jia, Shanshan and Liu, Xiaoan and Zhao, Ping and Liu, Chang and Sun, Lianying and Peng, Tao},
  year = {2018},
  month = dec,
  pages = {1--6},
  doi = {10.1109/ISPCE-CN.2018.8805749},
  abstract = {This study analyses the relationship of different key skills of artificial intelligence (AI) used in the job market. For this, we represent it with a knowledge graph and use a Long Short-term Memory Network to study interactions between these key skills. First, a knowledge graph is build with a rule-based method about these skills in the job markets. Then, the graph is visualized to discover knowledge relationship. Jobs in AI can be classified into two categories: general algorithm jobs and specific focus jobs. Skills of different jobs in AI are very different. Python and Linux are the most necessary key skills for all jobs in AI. Revealing all these key skills in jobs in AI is useful and provide a guideline for job-seekers, companies and universities.},
  keywords = {Artificial Intelligence Knowledge Graph,Data models,Deep learning,Linux,Load modeling,Python,Recruitment}
}

@inproceedings{jiKnowledgeGraphEmbedding2015,
  title = {Knowledge {{Graph Embedding}} via {{Dynamic Mapping Matrix}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
  year = {2015},
  month = jul,
  pages = {687--696},
  publisher = {Association for Computational Linguistics},
  address = {Beijing, China},
  doi = {10.3115/v1/P15-1067},
  urldate = {2021-12-07}
}

@article{jinBiomedicalQuestionAnswering2021,
  title = {Biomedical Question Answering: {{A}} Comprehensive Review},
  shorttitle = {Biomedical Question Answering},
  author = {Jin, Qiao and Yuan, Zheng and Xiong, Guangzhi and Yu, Qianlan and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Liu, Xiaozhong and Yu, Sheng},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.05281},
  eprint = {2102.05281},
  archiveprefix = {arxiv}
}

@inproceedings{jinCogKGEKnowledgeGraph2022,
  title = {{{CogKGE}}: {{A Knowledge Graph Embedding Toolkit}} and {{Benchmark}} for {{Representing Multi-source}} and {{Heterogeneous Knowledge}}},
  shorttitle = {{{CogKGE}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Jin, Zhuoran and Men, Tianyi and Yuan, Hongbang and He, Zhitao and Sui, Dianbo and Wang, Chenhao and Xue, Zhipeng and Chen, Yubo and Zhao, Jun},
  year = {2022},
  month = may,
  pages = {166--173},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-demo.16},
  urldate = {2022-07-20},
  abstract = {In this paper, we propose CogKGE, a knowledge graph embedding (KGE) toolkit, which aims to represent multi-source and heterogeneous knowledge. For multi-source knowledge, unlike existing methods that mainly focus on entity-centric knowledge, CogKGE also supports the representations of event-centric, commonsense and linguistic knowledge. For heterogeneous knowledge, besides structured triple facts, CogKGE leverages additional unstructured information, such as text descriptions, node types and temporal information, to enhance the meaning of embeddings. Designing CogKGE aims to provide a unified programming framework for KGE tasks and a series of knowledge representations for downstream tasks. As a research framework, CogKGE consists of five parts, including core, data, model, knowledge and adapter module. As a knowledge discovery toolkit, CogKGE provides pre-trained embedders to discover new facts, cluster entities and check facts. Furthermore, we construct two benchmark datasets for further research on multi-source heterogeneous KGE tasks: EventKG240K and CogNet360K. We also release an online system to discover knowledge visually. Source code, datasets and pre-trained embeddings are publicly available at GitHub, with a short instruction video.}
}

@article{jinColdstartActiveLearning2022,
  title = {Cold-Start Active Learning for Image Classification},
  author = {Jin, Qiuye and Yuan, Mingzhi and Li, Shiman and Wang, Haoran and Wang, Manning and Song, Zhijian},
  year = {2022},
  month = nov,
  journal = {Information Sciences},
  volume = {616},
  pages = {16--36},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2022.10.066},
  urldate = {2023-12-15},
  abstract = {Active learning (AL) aims to select valuable samples for labeling from an unlabeled sample pool to build a training dataset with minimal annotation cost. Traditional methods always require partially and initially labeled samples to start active selection and then query annotations of samples incrementally through several iterations. However, this scheme is not effective in the deep learning scenario. On the one hand, initially labeled sample sets are not always available in the beginning. On the other hand, the performance of the traditional model is usually poor in the early iterations due to limited training feedback. For the first time, we propose a cold-start AL model based on representative (CALR) sampling, which selects valuable samples without the need for an initial labeled set or the iterative feedback of the target models. Experiments on three image classification datasets, CIFAR-10, CIFAR-100 and Caltech-256, showed that CALR achieved a new state-of-the-art performance of AL in cold-start settings. Especially in low annotation budget conditions, our method can achieve up to a 10\% performance increase compared to traditional methods. Furthermore, CALR can be combined with warm-start methods to improve the start-up efficiency while further breaking the performance ceiling of AL, which makes CALR have a broader application scenario.},
  keywords = {Active learning,Cold start,Image classification}
}

@article{jinWhatDiseaseDoes2020,
  title = {What {{Disease}} Does This {{Patient Have}}? {{A Large-scale Open Domain Question Answering Dataset}} from {{Medical Exams}}},
  shorttitle = {What {{Disease}} Does This {{Patient Have}}?},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.13081 [cs]},
  eprint = {2009.13081},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2009.13081},
  urldate = {2021-11-30},
  abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{jinWhatDiseaseDoes2020a,
  title = {What {{Disease}} Does This {{Patient Have}}? {{A Large-scale Open Domain Question Answering Dataset}} from {{Medical Exams}}},
  shorttitle = {What {{Disease}} Does This {{Patient Have}}?},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = {2020},
  month = sep,
  number = {arXiv:2009.13081},
  eprint = {2009.13081},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.13081},
  urldate = {2023-10-26},
  abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7{\textbackslash}\%, 42.0{\textbackslash}\%, and 70.1{\textbackslash}\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{joySourceCodePlagiarism2010,
  title = {Source Code Plagiarism---a Student Perspective},
  author = {Joy, Mike and Cosma, Georgina and Yau, Jane Yin-Kim and Sinclair, Jane},
  year = {2010},
  journal = {IEEE Transactions on Education},
  volume = {54},
  number = {1},
  pages = {125--132},
  publisher = {IEEE},
  doi = {10.1109/TE.2010.2046664}
}

@inproceedings{kaddariBiomedicalQuestionAnswering2020,
  title = {Biomedical Question Answering: {{A}} Survey of Methods and Datasets},
  shorttitle = {Biomedical Question Answering},
  booktitle = {2020 {{Fourth International Conference On Intelligent Computing}} in {{Data Sciences}} ({{ICDS}})},
  author = {Kaddari, Zakaria and Mellah, Youssef and Berrich, Jamal and Bouchentouf, Toumi and Belkasmi, Mohammed G.},
  year = {2020},
  pages = {1--8},
  publisher = {IEEE},
  doi = {10.1109/icds50568.2020.9268742}
}

@inproceedings{kadlecTextUnderstandingAttention2016,
  title = {Text {{Understanding}} with the {{Attention Sum Reader Network}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kadlec, Rudolf and Schmid, Martin and Bajgar, Ondrej and Kleindienst, Jan},
  year = {2016},
  pages = {908--918},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/P16-1086},
  urldate = {2022-01-09},
  annotation = {00295}
}

@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2001.08361},
  urldate = {2023-05-19},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kasaiLowresourceDeepEntity2019,
  title = {Low-Resource {{Deep Entity Resolution}} with {{Transfer}} and {{Active Learning}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kasai, Jungo and Qian, Kun and Gurajada, Sairam and Li, Yunyao and Popa, Lucian},
  editor = {Korhonen, Anna and Traum, David and M{\`a}rquez, Llu{\'i}s},
  year = {2019},
  month = jul,
  pages = {5851--5861},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1586},
  urldate = {2023-11-13},
  abstract = {Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for knowledge base creation and text mining. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning. We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.}
}

@inproceedings{kePretrainingMetaLearning2021,
  title = {Pre-Training with {{Meta Learning}} for {{Chinese Word Segmentation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ke, Zhen and Shi, Liang and Sun, Songtao and Meng, Erli and Wang, Bin and Qiu, Xipeng},
  year = {2021},
  pages = {5514--5523},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.naacl-main.436},
  urldate = {2022-06-09},
  abstract = {Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.}
}

@inproceedings{keyProceedCareReimagining2021,
  title = {Proceed with {{Care}}: {{Reimagining Home IoT Through}} a {{Care Perspective}}},
  shorttitle = {Proceed with {{Care}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Key, Cayla and Browne, Fiona and Taylor, Nick and Rogers, Jon},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3411764.3445602},
  urldate = {2023-08-31},
  abstract = {As the internet is increasingly embedded in the everyday things in our homes, we notice a need for greater focus on the role care plays in those relationships---and therefore an opportunity to realize unseen potential in reimagining home Internet of Things (IoT). In this paper we report on our inquiry of home dwellers' relationships to caring for their everyday things and homes (referred to as thingcare). Findings from our design ethnography reveal four thematic qualities of their relationships to thingcare: Care Spectacle, Care Liminality, Ontological Braiding, and Care Condition. Using these themes as touchstones, we co-speculated to produce four speculative IoT concepts to explore what care as a design ethic might look like for IoT and reflect on nascent opportunities and challenges for domestic IoT design. We conclude by considering structures of power and privilege embedded within care practices that critically open new design imaginaries for IoT.},
  isbn = {978-1-4503-8096-6},
  keywords = {care,care ethics,design ethnography,home,Internet of things,research-through-design,smart home,things}
}

@misc{kimHealthLLMLargeLanguage2024,
  title = {Health-{{LLM}}: {{Large Language Models}} for {{Health Prediction}} via {{Wearable Sensor Data}}},
  shorttitle = {Health-{{LLM}}},
  author = {Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06866},
  eprint = {2401.06866},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.06866},
  urldate = {2024-01-25},
  abstract = {Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is important. This paper investigates the capacity of LLMs to deliver multi-modal health predictions based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps, GLOBEM, AW\_FB, MIT-BIH \& MIMIC-III). Our experiments cover thirteen consumer health prediction tasks in mental health, activity, metabolic, sleep, and cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable performance to larger models (GPT-3.5 and GPT-4), achieving the best performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness of context enhancement strategies, and generalization capability of the fine-tuned models across training datasets and the size of training samples. Notably, we observe that our context enhancement can yield up to 23.8\% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{kimSelfGeneratedInContextLearning2022,
  title = {Self-{{Generated In-Context Learning}}: {{Leveraging Auto-regressive Language Models}} as a {{Demonstration Generator}}},
  shorttitle = {Self-{{Generated In-Context Learning}}},
  author = {Kim, Hyuhng Joon and Cho, Hyunsoo and Kim, Junyeob and Kim, Taeuk and Yoo, Kang Min and Lee, Sang-goo},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08082},
  eprint = {2206.08082},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.08082},
  urldate = {2023-11-14},
  abstract = {Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@article{kociskyNarrativeQAReadingComprehension2017a,
  title = {The {{NarrativeQA Reading Comprehension Challenge}}},
  author = {Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  year = {2017},
  month = dec,
  eprint = {1712.07040},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1712.07040},
  urldate = {2023-01-20},
  abstract = {Reading comprehension (RC)---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing}
}

@article{kojimaLargeLanguageModels2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {22199--22213},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html},
  urldate = {2023-11-15},
  langid = {english},
  keywords = {No DOI found}
}

@inproceedings{kongCalibratedLanguageModel2020,
  title = {Calibrated {{Language Model Fine-Tuning}} for {{In-}} and {{Out-of-Distribution Data}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Kong, Lingkai and Jiang, Haoming and Zhuang, Yuchen and Lyu, Jie and Zhao, Tuo and Zhang, Chao},
  year = {2020},
  pages = {1326--1340},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.102},
  urldate = {2024-01-08},
  abstract = {Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at https://github.com/Lingkai-Kong/ Calibrated-BERT-Fine-Tuning.},
  langid = {english}
}

@inproceedings{koreedaContractNLIDatasetDocumentlevel2021,
  title = {{{ContractNLI}}: {{A Dataset}} for {{Document-level Natural Language Inference}} for {{Contracts}}},
  shorttitle = {{{ContractNLI}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Koreeda, Yuta and Manning, Christopher},
  year = {2021},
  month = nov,
  pages = {1907--1919},
  publisher = {Association for Computational Linguistics},
  address = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.findings-emnlp.164},
  urldate = {2023-10-12},
  abstract = {Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose ``document-level natural language inference (NLI) for contracts'', a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as ``Some obligations of Agreement may survive termination.'') and a contract, and it is asked to classify whether each hypothesis is ``entailed by'', ``contradicting to'' or ``not mentioned by'' (neutral to) the contract as well as identifying ``evidence'' for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (a) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (b) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.}
}

@inproceedings{krishnaReformulatingUnsupervisedStyle2020,
  title = {Reformulating {{Unsupervised Style Transfer}} as {{Paraphrase Generation}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Krishna, Kalpesh and Wieting, John and Iyyer, Mohit},
  year = {2020},
  month = nov,
  pages = {737--762},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.55},
  urldate = {2023-01-17},
  abstract = {Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate = {2023-09-26},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.}
}

@misc{kuzmanChatGPTBeginningEnd2023,
  title = {{{ChatGPT}}: {{Beginning}} of an {{End}} of {{Manual Linguistic Data Annotation}}? {{Use Case}} of {{Automatic Genre Identification}}},
  shorttitle = {{{ChatGPT}}},
  author = {Kuzman, Taja and Mozeti{\v c}, Igor and Ljube{\v s}i{\'c}, Nikola},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03953},
  eprint = {2303.03953},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2303.03953},
  urldate = {2023-11-13},
  abstract = {ChatGPT has shown strong capabilities in natural language generation tasks, which naturally leads researchers to explore where its abilities end. In this paper, we examine whether ChatGPT can be used for zero-shot text classification, more specifically, automatic genre identification. We compare ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on datasets, manually annotated with genres. The models are compared on test sets in two languages: English and Slovenian. Results show that ChatGPT outperforms the fine-tuned model when applied to the dataset which was not seen before by either of the models. Even when applied on Slovenian language as an under-resourced language, ChatGPT's performance is no worse than when applied to English. However, if the model is fully prompted in Slovenian, the performance drops significantly, showing the current limitations of ChatGPT usage on smaller languages. The presented results lead us to questioning whether this is the beginning of an end of laborious manual annotation campaigns even for smaller languages, such as Slovenian.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{lanALBERTLiteBERT2019,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=H1eA7AEtvS},
  urldate = {2022-04-12},
  abstract = {A new pretraining method that establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.},
  langid = {english}
}

@misc{lanhamMeasuringFaithfulnessChainofThought2023,
  title = {Measuring {{Faithfulness}} in {{Chain-of-Thought Reasoning}}},
  author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and {Telleen-Lawton}, Timothy and Hume, Tristan and {Hatfield-Dodds}, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
  year = {2023},
  month = jul,
  number = {arXiv:2307.13702},
  eprint = {2307.13702},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.13702},
  urldate = {2023-10-11},
  abstract = {Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{lattnerLLVMCompilationFramework2004,
  title = {{{LLVM}}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  year = {2004},
  month = mar,
  pages = {75--86},
  doi = {10.1109/CGO.2004.1281665},
  abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  keywords = {Algorithm design and analysis,Application software,Arithmetic,High level languages,Information analysis,Performance analysis,Program processors,Runtime,Software safety,Virtual machining}
}

@inproceedings{leeAbstractModelingEvaluating2020,
  title = {Abstract: {{Modeling}} and {{Evaluating Intervention Options}} and {{Strategies}} for {{COVID-19 Containment}}: {{A Biological-Behavioral-Logistics Computation Decision Framework}}},
  shorttitle = {Abstract},
  booktitle = {2020 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Lee, Eva},
  year = {2020},
  pages = {1--2},
  doi = {10.1109/BIBM49941.2020.9313325},
  abstract = {SARs, bird flu, H1N1, Ebola crisis in W. Africa, Zika and current SARS-CoV-2 underscore the critical importance of emergency response and medical preparedness. Such needs are wide-spread as globalization and air transportation facilitate rapid disease spread across the world. Computational modeling of infectious disease outbreaks and epidemics offer insights in propagation patterns and facilitate policy makers to synthesize potential interventions. Current models include inclined decay with an exponential adjustment, SEIR (susceptible, exposed, infectious, recovered) compartmental model, discrete time stochastic processes, and transmission tree. While many of these models incorporate contact tracing to predict spread pattern, key elements on optimal usage of scarce resources and effective and efficient process performance (e.g., diagnostics and screening, non-pharmaceutical interventions, trained personnel/robots for treatment, decontamination) have not been included. This is particularly critical in the fight of COVID-19 containment due to lack of testing kits and the prevalence of asymptomatic transmission, and the long period of hospitalization required by severely sick patients.This work focuses on designing a system computational decision modeling framework that simultaneously i) captures disease spread characteristics, ii) incorporates day-to-day hospital and home care processes and resource usage, iii) explores non-pharmaceutical intervention, social and human behavior and iv) allows for system optimization to minimize infection and mortality under time and labor constraints.},
  keywords = {Atmospheric modeling,Computational modeling,COVID-19,Logistics,Medical diagnostic imaging,Planning,Prediction algorithms},
  annotation = {00000}
}

@article{leeBioBERTPretrainedBiomedical2019,
  title = {{{BioBERT}}: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining},
  shorttitle = {{{BioBERT}}},
  author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  editor = {Wren, Jonathan},
  year = {2019},
  month = sep,
  journal = {Bioinformatics},
  pages = {btz682},
  issn = {1367-4803, 1460-2059},
  doi = {10/ggh5qq},
  urldate = {2021-11-30},
  abstract = {Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.},
  langid = {english}
}

@inproceedings{leeDREAMDomainInvariant2022,
  title = {{{DREAM}}: {{Domain Invariant}} and {{Contrastive Representation}} for {{Sleep Dynamics}}},
  shorttitle = {{{DREAM}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Lee, Seungyeon and Pham, Thai-Hoang and Zhang, Ping},
  year = {2022},
  month = nov,
  pages = {1029--1034},
  issn = {2374-8486},
  doi = {10.1109/ICDM54844.2022.00126},
  urldate = {2024-04-05},
  abstract = {Sleep staging is a key challenge in diagnosing and treating sleep-related diseases due to its labor-intensive, time-consuming, costly, and error-prone. With the availability of large-scale sleep signal data, many deep learning methods are proposed for automatic sleep staging. However, these existing methods face several challenges including the heterogeneity of patients' underlying health conditions and the difficulty modeling complex interactions between sleep stages. In this paper, we propose a neural network architecture named DREAM to tackle these issues for automatic sleep staging. DREAM consists of (i) a feature representation network that generates robust representations for sleep signals via the variational auto-encoder framework and contrastive learning and (ii) a sleep stage classification network that explicitly models the interactions between sleep stages in the sequential context at both feature representation and label classification levels via Transformer and conditional random field architectures. Our experimental results indicate that DREAM significantly outperforms existing methods for automatic sleep staging on three sleep signal datasets.},
  keywords = {Analytical models,Brain modeling,contrastive learning,deep learning,domain invariant,EEG analysis,Neural networks,Predictive models,Sleep,sleep dynamics,sleep staging,Training,Transformers}
}

@inproceedings{leEffectiveInterpretablePersonJob2019,
  title = {Towards {{Effective}} and {{Interpretable Person-Job Fitting}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Le, Ran and Hu, Wenpeng and Song, Yang and Zhang, Tao and Zhao, Dongyan and Yan, Rui},
  year = {2019},
  month = nov,
  series = {{{CIKM}} '19},
  pages = {1883--1892},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3357384.3357949},
  urldate = {2022-12-25},
  abstract = {The diversity of job requirements and the complexity of job seekers' abilities put forward higher requirements for the accuracy and interpretability of Person-Job Fit system. Interpretable Person-Job Fit system can show reasons for giving recommendations or not recommending specific jobs to some people, and vice versa. Such reasons help us understand according to what the final decision is made by the system and guarantee a high recommending accuracy. Existing studies on Person-Job Fit have focused on 1) one perspective, without considering the variances of role and psychological motivation between interviewer and job seeker; 2) modeling the matching degree between resume and job requirements directly through a deep neural network without interaction matching modules, which leads to shortage on interpretation. To this end, we propose an Interpretable Person-Job Fit (IPJF) model, which 1) models the Person-Job Fit problem from the perspectives/intentions of employer and job seeker in a multi-tasks optimization fashion to interpretively formulate the Person-Job Fit process; 2) leverages deep interactive representation learning to automatically learn the interdependence between a resume and job requirements without relying on a clear list of job seeker's abilities, and deploys the optimizing problem as a learning to rank problem. Experiments on large real dataset show that the proposed IPJF model outperforms state-of-the-art baselines and also gives promising interpretable recommending reasons.},
  isbn = {978-1-4503-6976-3},
  keywords = {interpretable ranking,multi-tasks optimization,person-job fit}
}

@misc{leeLEANLIFELabelEfficientAnnotation2020,
  title = {{{LEAN-LIFE}}: {{A Label-Efficient Annotation Framework Towards Learning}} from {{Explanation}}},
  shorttitle = {{{LEAN-LIFE}}},
  author = {Lee, Dong-Ho and Khanna, Rahul and Lin, Bill Yuchen and Chen, Jamin and Lee, Seyeon and Ye, Qinyuan and Boschee, Elizabeth and Neves, Leonardo and Ren, Xiang},
  year = {2020},
  month = apr,
  number = {arXiv:2004.07499},
  eprint = {2004.07499},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2004.07499},
  urldate = {2023-08-02},
  abstract = {Successfully training a deep neural network demands a huge corpus of labeled data. However, each label only provides limited information to learn from and collecting the requisite number of labels involves massive human effort. In this work, we introduce LEAN-LIFE, a web-based, Label-Efficient AnnotatioN framework for sequence labeling and classification tasks, with an easy-to-use UI that not only allows an annotator to provide the needed labels for a task, but also enables LearnIng From Explanations for each labeling decision. Such explanations enable us to generate useful additional labeled data from unlabeled instances, bolstering the pool of available training data. On three popular NLP tasks (named entity recognition, relation extraction, sentiment analysis), we find that using this enhanced supervision allows our models to surpass competitive baseline F1 scores by more than 5-10 percentage points, while using 2X times fewer labeled instances. Our framework is the first to utilize this enhanced supervision technique and does so for three important tasks -- thus providing improved annotation recommendations to users and an ability to build datasets of (data, label, explanation) triples instead of the regular (data, label) pair.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{leeLLMCXRInstructionFinetunedLLM2023,
  title = {{{LLM-CXR}}: {{Instruction-Finetuned LLM}} for {{CXR Image Understanding}} and {{Generation}}},
  shorttitle = {{{LLM-CXR}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Lee, Suhyeon and Kim, Won Jun and Chang, Jinho and Ye, Jong Chul},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=BqHaLnans2},
  urldate = {2024-03-18},
  abstract = {Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist of a combination of reasoning based on visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing (encoding or generating) networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our LLM-CXR trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks.},
  langid = {english}
}

@inproceedings{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lester, Brian and {Al-Rfou}, Rami and Constant, Noah},
  year = {2021},
  month = nov,
  pages = {3045--3059},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.243},
  urldate = {2022-08-02},
  abstract = {In this work, we explore ``prompt tuning,'' a simple yet effective mechanism for learning ``soft prompts'' to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ``closes the gap'' and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ``prefix tuning'' of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient ``prompt ensembling.'' We release code and model checkpoints to reproduce our experiments.}
}

@misc{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  month = oct,
  number = {arXiv:1910.13461},
  eprint = {1910.13461},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.13461},
  urldate = {2022-08-16},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{lewisPAQ65Million2021,
  title = {{{PAQ}}: 65 {{Million Probably-Asked Questions}} and {{What You Can Do With Them}}},
  shorttitle = {{{PAQ}}},
  author = {Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini, Pasquale and K{\"u}ttler, Heinrich and Piktus, Aleksandra and Stenetorp, Pontus and Riedel, Sebastian},
  year = {2021},
  month = feb,
  number = {arXiv:2102.07033},
  eprint = {2102.07033},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.07033},
  urldate = {2022-06-29},
  abstract = {Open-domain Question Answering models which directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared to conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models lack the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically-generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5\%, but trail RePAQ by over 15\%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) whilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to ``back-off" to a more expensive state-of-the-art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{liAliMeKGDomainKnowledge2020,
  title = {{{AliMeKG}}: {{Domain Knowledge Graph Construction}} and {{Application}} in {{E-commerce}}},
  shorttitle = {{{AliMeKG}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Li, Feng-Lin and Chen, Hehong and Xu, Guohai and Qiu, Tian and Ji, Feng and Zhang, Ji and Chen, Haiqing},
  year = {2020},
  month = oct,
  series = {{{CIKM}} '20},
  pages = {2581--2588},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3340531.3412685},
  urldate = {2022-07-19},
  abstract = {Pre-\-sales customer service is of importance to E\--commerce plat\-forms as it contributes to optimizing customers? buying process. To better serve users, we propose AliMe KG, a domain knowledge graph in E\--commerce that captures user problems, points of inter\-est (POI), item information and relations thereof. It helps to under\- stand user needs, answer pre\--sales questions and generate explana\-tion texts. We applied AliMe KG to several online business scenar\-ios such as shopping guide, question answering over properties and selling point generation, and gained positive and beneficial business results. In the paper, we systematically introduce how we construct domain knowledge graph from free text, and demonstrate its busi\-ness value with several applications. Our experience shows that min\- ing structured knowledge from free text in vertical domain is prac\-ticable, and can be of substantial value in industrial settings.},
  isbn = {978-1-4503-6859-9},
  keywords = {domain knowledge graph,e-commerce,pre-sales customer service}
}

@inproceedings{liBEHAVIOR1KBenchmarkEmbodied2023,
  title = {{{BEHAVIOR-1K}}: {{A Benchmark}} for {{Embodied AI}} with 1,000 {{Everyday Activities}} and {{Realistic Simulation}}},
  shorttitle = {{{BEHAVIOR-1K}}},
  booktitle = {Proceedings of {{The}} 6th {{Conference}} on {{Robot Learning}}},
  author = {Li, Chengshu and Zhang, Ruohan and Wong, Josiah and Gokmen, Cem and Srivastava, Sanjana and {Mart{\'i}n-Mart{\'i}n}, Roberto and Wang, Chen and Levine, Gabrael and Lingelbach, Michael and Sun, Jiankai and Anvari, Mona and Hwang, Minjune and Sharma, Manasi and Aydin, Arman and Bansal, Dhruva and Hunter, Samuel and Kim, Kyu-Young and Lou, Alan and Matthews, Caleb R. and {Villa-Renteria}, Ivan and Tang, Jerry Huayang and Tang, Claire and Xia, Fei and Savarese, Silvio and Gweon, Hyowon and Liu, Karen and Wu, Jiajun and {Fei-Fei}, Li},
  year = {2023},
  month = mar,
  pages = {80--93},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v205/li23a.html},
  urldate = {2024-02-20},
  abstract = {We present BEHAVIOR-1K, a comprehensive simulation benchmark for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on "what do you want robots to do for you?". The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 5,000 objects annotated with rich physical and semantic properties. The second is OmniGibson, a novel simulation environment that supports these activities via realistic physics simulation and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the simulation-to-reality gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K's human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: https://behavior.stanford.edu.},
  langid = {english}
}

@article{liDIGMNDynamicIntent,
  title = {{{DIGMN}}: {{Dynamic Intent Guided Meta Network}} for {{Differentiated User Engagement Forecasting}} in {{Online Professional Social Platforms}}},
  author = {Li, Feifan and Du, Lun and Fu, Qiang and Han, Shi and Du, Yushu and Lu, Guangming and Li, Zi},
  pages = {10},
  abstract = {User engagement prediction plays a critical role for designing interaction strategies to grow user engagement and increase revenue in online social platforms. Through the in-depth analysis of the real-world data from the world's largest professional social platforms, i.e., LinkedIn, we find that users expose diverse engagement patterns, and a major reason for the differences in user engagement patterns is that users have different intents. That is, people have different intents when using LinkedIn, e.g., applying for jobs, building connections, or checking notifications, which shows quite different engagement patterns. Meanwhile, user intents and the corresponding engagement patterns may change over time. Although such pattern differences and dynamics are essential for user engagement prediction, differentiating user engagement patterns based on user dynamic intents for better user engagement forecasting has not received enough attention in previous works. In this paper, we proposed a Dynamic Intent Guided Meta Network (DIGMN), which can explicitly model user intent varying with time and perform differentiated user engagement forecasting. Specifically, we derive some interpretable basic user intents as prior knowledge from data mining and introduce prior intents in explicitly modeling dynamic user intent. Furthermore, based on the dynamic user intent representations, we propose a meta predictor to perform differentiated user engagement forecasting. Through a comprehensive evaluation on LinkedIn anonymous user data, our method outperforms stateof-the-art baselines significantly, i.e., 2.96\% and 3.48\% absolute error reduction, on coarse-grained and fine-grained user engagement prediction tasks, respectively, demonstrating the effectiveness of our method.},
  langid = {english}
}

@misc{liGuidingLargeLanguage2023,
  title = {Guiding {{Large Language Models}} via {{Directional Stimulus Prompting}}},
  author = {Li, Zekun and Peng, Baolin and He, Pengcheng and Galley, Michel and Gao, Jianfeng and Yan, Xifeng},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11520},
  eprint = {2302.11520},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.11520},
  urldate = {2023-03-13},
  abstract = {We introduce a new framework, Directional Stimulus Prompting, that uses a tuneable language model (LM) to provide guidance for the black-box frozen large language model (LLM) on downstream tasks. Unlike prior work that manually or automatically finds the optimal prompt for each task, we train a policy LM to generate discrete tokens as ``directional stimulus'' of each input, which is a hint/cue such as keywords of an article for summarization. The directional stimulus is then combined with the original input and fed into the LLM to guide its generation toward the desired target. The policy LM can be trained through 1) supervised learning from annotated data and 2) reinforcement learning from offline and online rewards to explore directional stimulus that better aligns LLMs with human preferences. This framework is flexibly applicable to various LMs and tasks. To verify its effectiveness, we apply our framework to summarization and dialogue response generation tasks. Experimental results demonstrate that it can significantly improve LLMs' performance with a small collection of training data: a T5 (780M) trained with 2,000 samples from the CNN/Daily Mail dataset improves Codex (175B)'s performance by 7.2\% in ROUGE-Avg scores; 500 dialogues boost the combined score by 52.5\%, achieving comparable or even better performance than fully trained models on the MultiWOZ dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@article{liLearningKnowledgeGraph2021,
  title = {Learning {{Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}}},
  author = {Li, Zhifei and Liu, Hai and Zhang, Zhaoli and Liu, Tingting and Xiong, Neal N.},
  year = {2021},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--13},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3055147},
  abstract = {Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.},
  keywords = {Aggregates,Computer architecture,Fuses,Graph heterogeneity,Graph neural networks,graph neural networks (GNNs),KGs,knowledge graph (KG) embedding,Learning systems,link prediction.,Semantics,Task analysis}
}

@article{limaCardiotoxicityCancerPatients2022,
  title = {Cardiotoxicity in Cancer Patients Treated with Chemotherapy: {{A}} Systematic Review},
  shorttitle = {Cardiotoxicity in Cancer Patients Treated with Chemotherapy},
  author = {Lima, Maria Adriely Cunha and Brito, Henrique Rodrigues de Almeida and Mitidieri, Gabriel Guimar{\~a}es and {de Souza}, Eduardo Paulo and Sobral, Ana Caroline Gois and Melo, Hemmely Hevelyn Maria Araujo and Vasconcelos, Guilherme Barreto and {de Almeida}, Berila Beatriz Dias and Figueiredo, Thain{\'a} de Ara{\'u}jo Diniz and Filho, Marcello Augusto Anchieta Santos and Santos, Douglas Silva Rosendo and {de Carvalho}, Renan Fontes and Oliveira, Halley Ferraro},
  year = {2022},
  journal = {International Journal of Health Sciences},
  volume = {16},
  number = {6},
  pages = {39--46},
  issn = {1658-3639},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9682875/},
  urldate = {2024-03-28},
  abstract = {Objective: The objective of the study was to assess the incidence of chemotherapy cardiotoxicity. Methods: This is a systematic review carried out through the PubMed, VHL and Scientific Electronic Library Online databases, using the descriptors ``Cardiotoxicity'' and ``Chemotherapy'' associated with the Boolean operator ``AND.'' Initially, 15,090 articles were found between 2015 and 2021. After applying the defined inclusion and exclusion criteria, 80 studies remained, of which 27 underwent complete reading, after which all were included in the study. Results: In total, 32,009 cancer patients were analyzed, of which 27,270 (85.2\%) were female. Breast cancer was the most frequent neoplasm, with 11,145 (34.8\%) cases. Regarding the type of chemotherapy, anthracycline was the most prevalent, analyzed in 18 (66.7\%) studies, followed by trastuzumab, in 9 (33.3\%) studies. Of the studies evaluated, five did not present any case of cardiotoxicity, a total of 2255 (8.3\%) cases were recorded, in addition other outcomes mentioned in patients after chemotherapy were arrhythmia (n = 522), acute coronary syndrome (n = 185), diastolic dysfunction (n = 184), cardiomyopathy (n = 161), and arterial hypertension (n = 89). Conclusion: Post-chemotherapeutic cardiotoxicity was mentioned in most studies, being present in a relevant percentage of the sample. Furthermore, these patients may develop other cardiovascular events.},
  pmcid = {PMC9682875},
  pmid = {36475028}
}

@misc{liMultitaskPretrainingLanguage2022,
  title = {Multi-Task {{Pre-training Language Model}} for {{Semantic Network Completion}}},
  author = {Li, Da and Yang, Sen and Xu, Kele and Yi, Ming and He, Yukai and Wang, Huaimin},
  year = {2022},
  month = apr,
  number = {arXiv:2201.04843},
  eprint = {2201.04843},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.04843},
  urldate = {2022-06-29},
  abstract = {Semantic networks, such as the knowledge graph, can represent the knowledge leveraging the graph structure. Although the knowledge graph shows promising values in natural language processing, it suffers from incompleteness. This paper focuses on knowledge graph completion by predicting linkage between entities, which is a fundamental yet critical task. Semantic matching is a potential solution as it can deal with unseen entities, which the translational distance based methods struggle with. However, to achieve competitive performance as translational distance based methods, semantic matching based methods require large-scale datasets for the training purpose, which are typically unavailable in practical settings. Therefore, we employ the language model and introduce a novel knowledge graph architecture named LP-BERT, which contains two main stages: multi-task pre-training and knowledge graph fine-tuning. In the pre-training phase, three tasks are taken to drive the model to learn the relationship from triples by predicting either entities or relations. While in the fine-tuning phase, inspired by contrastive learning, we design a triple-style negative sampling in a batch, which greatly increases the proportion of negative sampling while keeping the training time almost unchanged. Furthermore, we propose a new data augmentation method utilizing the inverse relationship of triples to improve the performance and robustness of the model. To demonstrate the effectiveness of our method, we conduct extensive experiments on three widely-used datasets, WN18RR, FB15k-237, and UMLS. The experimental results demonstrate the superiority of our methods, and our approach achieves state-of-the-art results on WN18RR and FB15k-237 datasets. Significantly, Hits@10 indicator is improved by 5\% from previous state-of-the-art result on the WN18RR dataset while reaching 100\% on the UMLS dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{linAlpacaTagActiveLearningbased2019a,
  title = {{{AlpacaTag}}: {{An Active Learning-based Crowd Annotation Framework}} for {{Sequence Tagging}}},
  shorttitle = {{{AlpacaTag}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Lin, Bill Yuchen and Lee, Dong-Ho and Xu, Frank F. and Lan, Ouyu and Ren, Xiang},
  year = {2019},
  pages = {58--63},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-3010},
  urldate = {2023-08-02},
  langid = {english}
}

@inproceedings{linLearningEntityRelation2015,
  title = {Learning {{Entity}} and {{Relation Embeddings}} for {{Knowledge Graph Completion}}},
  booktitle = {Twenty-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  year = {2015},
  month = feb,
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571},
  urldate = {2021-12-07},
  abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  langid = {english}
}

@inproceedings{linModelingRelationPaths2015,
  title = {Modeling {{Relation Paths}} for {{Representation Learning}} of {{Knowledge Bases}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lin, Yankai and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong and Rao, Siwei and Liu, Song},
  year = {2015},
  pages = {705--714},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/D15-1082},
  urldate = {2021-12-09},
  abstract = {Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text. The source code of this paper can be obtained from https://github.com/mrlyk423/ relation\_extraction.},
  langid = {english}
}

@misc{linUnlockingSpellBase2023,
  title = {The {{Unlocking Spell}} on {{Base LLMs}}: {{Rethinking Alignment}} via {{In-Context Learning}}},
  shorttitle = {The {{Unlocking Spell}} on {{Base LLMs}}},
  author = {Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and Bhagavatula, Chandra and Choi, Yejin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.01552},
  eprint = {2312.01552},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.01552},
  urldate = {2023-12-13},
  abstract = {The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be "superficial." This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{lippiCLAUDETTEAutomatedDetector2019,
  title = {{{CLAUDETTE}}: An {{Automated Detector}} of {{Potentially Unfair Clauses}} in {{Online Terms}} of {{Service}}},
  shorttitle = {{{CLAUDETTE}}},
  author = {Lippi, Marco and Palka, Przemyslaw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo},
  year = {2019},
  month = jun,
  journal = {Artificial Intelligence and Law},
  volume = {27},
  number = {2},
  eprint = {1805.01217},
  primaryclass = {cs},
  pages = {117--139},
  issn = {0924-8463, 1572-8382},
  doi = {10.1007/s10506-019-09243-2},
  urldate = {2023-11-01},
  abstract = {Terms of service of on-line platforms too often contain clauses that are potentially unfair to the consumer. We present an experimental study where machine learning is employed to automatically detect such potentially unfair clauses. Results show that the proposed system could provide a valuable tool for lawyers and consumers alike.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society}
}

@inproceedings{liuContrastiveAttentionAutomatic2021,
  title = {Contrastive {{Attention}} for {{Automatic Chest X-ray Report Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Liu, Fenglin and Yin, Changchang and Wu, Xian and Ge, Shen and Zhang, Ping and Sun, Xu},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  month = aug,
  pages = {269--280},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.findings-acl.23},
  urldate = {2024-03-18}
}

@misc{liuGEvalNLGEvaluation2023,
  title = {G-{{Eval}}: {{NLG Evaluation}} Using {{GPT-4}} with {{Better Human Alignment}}},
  shorttitle = {G-{{Eval}}},
  author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
  year = {2023},
  month = may,
  number = {arXiv:2303.16634},
  eprint = {2303.16634},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.16634},
  urldate = {2023-11-13},
  abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{LiuJiYuBiLSTMDeShuXueZhuGuanTiZiDongYueJuanFangFa2018,
  title = {{Bi-LSTM}},
  author = {,  and ,  and ,  and , },
  year = {2018},
  journal = {},
  number = {02},
  pages = {109--113},
  issn = {1674-2877},
  url = {https://kns.cnki.net/kcms/detail/frame/list.aspx?dbcode=CJFD&filename=glkw201802046&dbname=CJFDLAST2018&RefType=1&vl=z1ZZ6_Hn98HW9JZgxp45CcyQ5kAhBfGukyc86STLgdtE0IJzj-DaXMBQuvh5JoS-},
  urldate = {2022-02-11},
  abstract = {TF-IDF,,,,83.17\%},
  langid = {chinese},
  keywords = {,Automatic marking,Bi-LSTM,Deep learning,Mathematical subjective questions,Text similarity},
  annotation = {00000}
}

@inproceedings{liuKeywordawareAbstractiveSummarization2021,
  title = {Keyword-Aware {{Abstractive Summarization}} by {{Extracting Set-level Intermediate Summaries}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Liu, Yizhu and Jia, Qi and Zhu, Kenny},
  year = {2021},
  month = apr,
  series = {{{WWW}} '21},
  pages = {3042--3054},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442381.3449906},
  urldate = {2022-07-27},
  abstract = {Abstractive summarization is useful in providing a summary or a digest of news or other web texts and enhancing users reading experience, especially when they are reading on small displays such as mobile phones. However, existing encoder-decoder summarization models have difficulty learning the latent alignment between source documents and summaries because of their vast disparity in length. In this paper, we propose a extractor-abstractor framework in which the keyword-based extractor selects a few sets of salient sentences from the input document and then the abstractor paraphrases these sets of sentences in parallel, which are more aligned to the summary, to generate the final summary. The new extractor and abstractor are pretrained from a set of ``pseudo summaries'' extracted by specially designed heuristics, and then further trained together in a reinforcement learning framework. The results show that the proposed model generates high-quality summaries with faster training speed and less training memory footprint, and outperforms the state-of-the-art models on CNN/Daily Mail, Webis-TLDR-17, Webis-Snippet-20, WikiHow and DUC-2002 datasets.},
  isbn = {978-1-4503-8312-7},
  keywords = {Abstractive Summarization,Alignment,Reinforcement Learning,Set-level Pseudo-summaries}
}

@misc{liuLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Few-Shot Health Learners}}},
  author = {Liu, Xin and McDuff, Daniel and Kovacs, Geza and {Galatzer-Levy}, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak},
  year = {2023},
  month = may,
  number = {arXiv:2305.15525},
  eprint = {2305.15525},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.15525},
  urldate = {2024-01-16},
  abstract = {Large language models (LLMs) can capture rich representations of concepts that are useful for real-world tasks. However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus. We demonstrate that with only few-shot tuning, a large language model is capable of grounding various physiological and behavioral time-series data and making meaningful inferences on numerous health tasks for both clinical and wellness contexts. Using data from wearable and medical sensor recordings, we evaluate these capabilities on the tasks of cardiac signal analysis, physical activity recognition, metabolic calculation (e.g., calories burned), and estimation of stress reports and mental health screeners.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{liuLearningMultiGraphNeural2021,
  title = {Learning {{Multi-Graph Neural Network}} for {{Data-Driven Job Skill Prediction}}},
  booktitle = {2021 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Liu, Liting and Zhang, Wenzheng and Liu, Jie and Shi, Wenxuan and Huang, Yalou},
  year = {2021},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN52387.2021.9533402},
  abstract = {Specifying an appropriate skill set for a job position is critical for talent recruitment. However, it is often more difficult than people think since it needs a great understanding of the role of the position, related technologies, and even the global situation of the job market. To this end, we propose to learn the mapping between the job position description and its required skills in a data-driven manner. This task is challenging due to the complex mapping relationships between job descriptions and skills, which is caused by complex influencing factors. In this paper, we propose a novel Multi-Graph Neural Network based Skill Prediction model (MGNSP) to make skill prediction by learning effective deep semantics matching of job positions and skills. Specifically, to capture the complex heterogeneous relations among the job positions, skills, and meta information, we devise a joint learning approach of graph neural networks for multiple information networks, which are J-Net, S-Net and JS-Net, respectively. After jointly learning complementary semantics of job positions and skills with three multi-layer graph neural networks from these information networks, the skills are predicted by learning to match their representations. Extensive experimental results on a real-world dataset validate the effectiveness of our model.},
  keywords = {Aggregates,Correlation,Graph neural networks,Predictive models,Resumes,Semantics,Task analysis}
}

@inproceedings{liUnifiedDemonstrationRetriever2023,
  title = {Unified {{Demonstration Retriever}} for {{In-Context Learning}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Xiaonan and Lv, Kai and Yan, Hang and Lin, Tianyang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Wang, Xiaoling and Qiu, Xipeng},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {4644--4668},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.256},
  urldate = {2023-11-14},
  abstract = {In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks' training signals into a unified list-wise ranking formulation by language model's feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks' signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR's strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review.}
}

@misc{liuPretrainPromptPredict2021,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2021},
  month = jul,
  number = {arXiv:2107.13586},
  eprint = {2107.13586},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2107.13586},
  urldate = {2022-08-02},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{liuRadiologyGPTLargeLanguage2023,
  title = {Radiology-{{GPT}}: {{A Large Language Model}} for {{Radiology}}},
  shorttitle = {Radiology-{{GPT}}},
  author = {Liu, Zhengliang and Zhong, Aoxiao and Li, Yiwei and Yang, Longtao and Ju, Chao and Wu, Zihao and Ma, Chong and Shu, Peng and Chen, Cheng and Kim, Sekeun and Dai, Haixing and Zhao, Lin and Zhu, Dajiang and Liu, Jun and Liu, Wei and Shen, Dinggang and Li, Xiang and Li, Quanzheng and Liu, Tianming},
  year = {2023},
  month = jun,
  number = {arXiv:2306.08666},
  eprint = {2306.08666},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2306.08666},
  urldate = {2024-01-20},
  abstract = {We introduce Radiology-GPT, a large language model for radiology. Using an instruction tuning approach on an extensive dataset of radiology domain knowledge, Radiology-GPT demonstrates superior performance compared to general language models such as StableLM, Dolly and LLaMA. It exhibits significant versatility in radiological diagnosis, research, and communication. This work serves as a catalyst for future developments in clinical NLP. The successful implementation of Radiology-GPT is indicative of the potential of localizing generative large language models, specifically tailored for distinctive medical specialties, while ensuring adherence to privacy standards such as HIPAA. The prospect of developing individualized, large-scale language models that cater to specific needs of various hospitals presents a promising direction. The fusion of conversational competence and domain-specific knowledge in these models is set to foster future development in healthcare AI. A demo of Radiology-GPT is available at https://huggingface.co/spaces/allen-eric/radiology-gpt.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{liuRetrieveReasonRefine2022,
  title = {Retrieve, {{Reason}}, and {{Refine}}: {{Generating Accurate}} and {{Faithful Patient Instructions}}},
  shorttitle = {Retrieve, {{Reason}}, and {{Refine}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Fenglin and Yang, Bang and You, Chenyu and Wu, Xian and Ge, Shen and Liu, Zhangdaihong and Sun, Xu and Yang, Yang and Clifton, David A.},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=dp0zWsdOV1h},
  urldate = {2024-03-18},
  abstract = {The "Patient Instruction" (PI), which contains critical instructional information provided both to carers and to the patient at the time of discharge, is essential for the patient to manage their condition outside hospital. An accurate and easy-to-follow PI can improve the self-management of patients which can in turn reduce hospital readmission rates. However, writing an appropriate PI can be extremely time consuming for physicians, and is subject to being incomplete or error-prone for (potentially overworked) physicians. Therefore, we propose a new task that can provide an objective means of avoiding incompleteness, while reducing clinical workload: the automatic generation of the PI, which is imagined as being a document that the clinician can review, modify, and approve as necessary (rather than taking the human "out of the loop"). We build a benchmark clinical dataset and propose the Re\$\^{}3\$Writer, which imitates the working patterns of physicians to first retrieve related working experience from historical PIs written by physicians, then reason related medical knowledge. Finally, it refines the retrieved working experience and reasoned medical knowledge to extract useful information, which is used to generate the PI for previously-unseen patient according to their health records during hospitalization. Our experiments show that, using our method, the performance of 6 different models can be substantially boosted across all metrics, with up to 20\%, 11\%, and 19\% relative improvements in BLEU-4, ROUGE-L, and METEOR, respectively. Meanwhile, we show results from human evaluations to measure the effectiveness in terms of its usefulness for clinical practice. The code is available at https://github.com/AI-in-Health/Patient-Instructions.},
  langid = {english}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  primaryclass = {cs},
  url = {https://openreview.net/forum?id=SyxS0T4tvS},
  urldate = {2022-02-11},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {02721}
}

@misc{liuScissorhandsExploitingPersistence2023,
  title = {Scissorhands: {{Exploiting}} the {{Persistence}} of {{Importance Hypothesis}} for {{LLM KV Cache Compression}} at {{Test Time}}},
  shorttitle = {Scissorhands},
  author = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  year = {2023},
  month = may,
  number = {arXiv:2305.17118},
  eprint = {2305.17118},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.17118},
  urldate = {2023-06-23},
  abstract = {Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5X without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20X compression.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{liuWhatMakesGood2022,
  title = {What {{Makes Good In-Context Examples}} for {{GPT-3}}?},
  booktitle = {Proceedings of {{Deep Learning Inside Out}} ({{DeeLIO}} 2022): {{The}} 3rd {{Workshop}} on {{Knowledge Extraction}} and {{Integration}} for {{Deep Learning Architectures}}},
  author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  editor = {Agirre, Eneko and Apidianaki, Marianna and Vuli{\'c}, Ivan},
  year = {2022},
  month = may,
  pages = {100--114},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland and Online},
  doi = {10.18653/v1/2022.deelio-1.10},
  urldate = {2023-11-14},
  abstract = {GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3\% on the ToTTo dataset) and open-domain question answering (45.5\% on the NQ dataset).}
}

@article{LiuZhiYuanSunMaoSongLinYanKaiXieRuoBingZhiShiBiaoShiXueXiYanJiuJinZhan2016,
  title = {{}},
  author = { and Liu Zhiyuan, Sun Maosong},
  year = {2016},
  month = feb,
  journal = {},
  volume = {53},
  number = {2},
  pages = {247},
  issn = {1000-1239},
  doi = {10.7544/issn1000-1239.2016.20160020},
  urldate = {2021-12-07},
  abstract = {...},
  langid = {chinese}
}

@article{luccioniEstimatingCarbonFootprint2023,
  title = {Estimating the {{Carbon Footprint}} of {{BLOOM}}, a {{176B Parameter Language Model}}},
  author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {253},
  pages = {1--15},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v24/23-0069.html},
  urldate = {2024-02-13},
  abstract = {Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of CO2eq if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also carry out an empirical study to measure the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.},
  keywords = {No DOI found}
}

@inproceedings{luContextualEmbeddingModel2022,
  title = {Contextual Embedding and Model Weighting by Fusing Domain Knowledge on Biomedical Question Answering},
  booktitle = {Proceedings of the 13th {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}} and {{Health Informatics}}},
  author = {Lu, Yuxuan and Yan, Jingya and Qi, Zhixuan and Ge, Zhongzheng and Du, Yongping},
  year = {2022},
  month = aug,
  series = {{{BCB}} '22},
  pages = {1--4},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3535508.3545508},
  urldate = {2023-12-15},
  abstract = {Biomedical Question Answering aims to obtain an answer to the given question from the biomedical domain. Due to its high requirement of biomedical domain knowledge, it is difficult for the model to learn domain knowledge from limited training data. We propose a contextual embedding method that combines open-domain QA model AoA Reader and BioBERT model pre-trained on biomedical domain data. We adopt unsupervised pre-training on large biomedical corpus and supervised fine-tuning on biomedical question answering dataset. Additionally, we adopt an MLP-based model weighting layer to automatically exploit the advantages of two models to provide the correct answer. The public dataset biomrc constructed from PubMed corpus is used to evaluate our method. Experimental results show that our model outperforms state-of-the-art system by a large margin.},
  isbn = {978-1-4503-9386-7},
  keywords = {biomedical question answering,contextual embedding,domain knowledge,model weighting}
}

@misc{luDeepSeekVLRealWorldVisionLanguage2024,
  title = {{{DeepSeek-VL}}: {{Towards Real-World Vision-Language Understanding}}},
  shorttitle = {{{DeepSeek-VL}}},
  author = {Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and Sun, Yaofeng and Deng, Chengqi and Xu, Hanwei and Xie, Zhenda and Ruan, Chong},
  year = {2024},
  month = mar,
  number = {arXiv:2403.05525},
  eprint = {2403.05525},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.05525},
  urldate = {2024-03-14},
  abstract = {We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions: We strive to ensure our data is diverse, scalable, and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content, aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model's user experience in practical applications. Considering efficiency and the demands of most real-world scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024), while maintaining a relatively low computational overhead. This design choice ensures the model's ability to capture critical semantic and detailed information across various visual tasks. We posit that a proficient Vision-Language Model should, foremost, possess strong language abilities. To ensure the preservation of LLM capabilities during pretraining, we investigate an effective VL pretraining strategy by integrating LLM training from the beginning and carefully managing the competitive dynamics observed between vision and language modalities. The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a vision-language chatbot in real-world applications, achieving state-of-the-art or competitive performance across a wide range of visual-language benchmarks at the same model size while maintaining robust performance on language-centric benchmarks. We have made both 1.3B and 7B models publicly accessible to foster innovations based on this foundation model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{luFantasticallyOrderedPrompts2022,
  title = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}: {{Overcoming Few-Shot Prompt Order Sensitivity}}},
  shorttitle = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  month = may,
  pages = {8086--8098},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.556},
  urldate = {2023-11-14},
  abstract = {When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are ``fantastic'' and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13\% relative improvement for GPT-family models across eleven different established text classification tasks.}
}

@misc{luHumanStillWins2023,
  title = {Human {{Still Wins}} over {{LLM}}: {{An Empirical Study}} of {{Active Learning}} on {{Domain-Specific Annotation Tasks}}},
  shorttitle = {Human {{Still Wins}} over {{LLM}}},
  author = {Lu, Yuxuan and Yao, Bingsheng and Zhang, Shao and Wang, Yun and Zhang, Peng and Lu, Tun and Li, Toby Jia-Jun and Wang, Dakuo},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09825},
  eprint = {2311.09825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.09825},
  urldate = {2024-02-01},
  abstract = {Large Language Models (LLMs) have demonstrated considerable advances, and several claims have been made about their exceeding human performance. However, in real-world tasks, domain knowledge is often required. Low-resource learning methods like Active Learning (AL) have been proposed to tackle the cost of domain expert annotation, raising this question: Can LLMs surpass compact models trained with expert annotations in domain-specific tasks? In this work, we conduct an empirical experiment on four datasets from three different domains comparing SOTA LLMs with small models trained on expert annotations with AL. We found that small models can outperform GPT-3.5 with a few hundreds of labeled data, and they achieve higher or similar performance with GPT-4 despite that they are hundreds time smaller. Based on these findings, we posit that LLM predictions can be used as a warmup method in real-world applications and human experts remain indispensable in tasks involving data annotation driven by domain-specific knowledge.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@article{LuJiYuBLEUDeShuXueZhuGuanTiZiDongYueJuanDeFangFa2019,
  title = {{BLEU}},
  author = {,  and , },
  year = {2019},
  journal = {},
  number = {03},
  pages = {121--124},
  issn = {1674-2877},
  url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2019&filename=GLKW201903051&uniplatform=NZKPT&v=ix6i2z872Y_kL_nlW-jNIIiz6C2k692i0uIrHQioQSLul4GSRgL1jT1MRXeUvO9-},
  urldate = {2022-02-11},
  abstract = {,,BLEU,,88.11\%},
  langid = {chinese},
  keywords = {,Automatic marking,BLEU,Mathematical subjective questions,Text similarity},
  annotation = {00000}
}

@misc{luLLaMAReviewerAdvancingCode2023,
  title = {{{LLaMA-Reviewer}}: {{Advancing Code Review Automation}} with {{Large Language Models}} through {{Parameter-Efficient Fine-Tuning}}},
  shorttitle = {{{LLaMA-Reviewer}}},
  author = {Lu, Junyi and Yu, Lei and Li, Xiaojia and Yang, Li and Zuo, Chun},
  year = {2023},
  month = sep,
  number = {arXiv:2308.11148},
  eprint = {2308.11148},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2308.11148},
  urldate = {2024-01-20},
  abstract = {The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored. In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1\% of trainable parameters. An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models. The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering}
}

@inproceedings{luoAliCoCo2CommonsenseKnowledge2021,
  title = {{{AliCoCo2}}: {{Commonsense Knowledge Extraction}}, {{Representation}} and {{Application}} in {{E-commerce}}},
  shorttitle = {{{AliCoCo2}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Luo, Xusheng and Bo, Le and Wu, Jinhang and Li, Lin and Luo, Zhiy and Yang, Yonghua and Yang, Keping},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {3385--3393},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3447548.3467203},
  urldate = {2022-07-14},
  abstract = {Commonsense knowledge used by humans while doing online shopping is valuable but difficult to be captured by existing systems running on e-commerce platforms. While construction of common- sense knowledge graphs in e-commerce is non-trivial, representation learning upon such graphs poses unique challenge compared to well-studied open-domain knowledge graphs (e.g., Freebase). By leveraging the commonsense knowledge and representation techniques, various applications in e-commerce can be benefited. Based on AliCoCo, the large-scale e-commerce concept net assisting a series of core businesses in Alibaba, we further enrich it with more commonsense relations and present AliCoCo2, the first commonsense knowledge graph constructed for e-commerce use. We propose a multi-task encoder-decoder framework to provide effective representations for nodes and edges from AliCoCo2. To explore the possibility of improving e-commerce businesses with commonsense knowledge, we apply newly mined commonsense relations and learned embeddings to e-commerce search engine and recommendation system in different ways. Experimental results demonstrate that our proposed representation learning method achieves state-of-the-art performance on the task of knowledge graph completion (KGC), and applications on search and recommendation indicate great potential value of the construction and use of commonsense knowledge graph in e-commerce. Besides, we propose an e-commerce QA task with a new benchmark during the construction of AliCoCo2, for testing machine common sense in e-commerce, which can benefit research community in exploring commonsense reasoning.},
  isbn = {978-1-4503-8332-5},
  keywords = {common sense,e-commerce,knowledge graph embedding}
}

@inproceedings{luoAliCoCoAlibabaEcommerce2020,
  title = {{{AliCoCo}}: {{Alibaba E-commerce Cognitive Concept Net}}},
  shorttitle = {{{AliCoCo}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Luo, Xusheng and Liu, Luxin and Yang, Yonghua and Bo, Le and Cao, Yuanpeng and Wu, Jinghang and Li, Qiang and Yang, Keping and Zhu, Kenny Q.},
  year = {2020},
  month = jun,
  series = {{{SIGMOD}} '20},
  pages = {313--327},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3318464.3386132},
  urldate = {2022-07-12},
  abstract = {One of the ultimate goals of e-commerce platforms is to satisfy various shopping needs for their customers. Much efforts are devoted to creating taxonomies or ontologies in e-commerce towards this goal. However, user needs in e-commerce are still not well defined, and none of the existing ontologies has the enough depth and breadth for universal user needs understanding. The semantic gap in-between prevents shopping experience from being more intelligent. In this paper, we propose to construct a large-scale E-commerce Cognitive Concept Net named "AliCoCo", which is practiced in Alibaba, the largest Chinese e-commerce platform in the world. We formally define user needs in e-commerce, then conceptualize them as nodes in the net. We present details on how AliCoCo is constructed semi-automatically and its successful, ongoing and potential applications in e-commerce.},
  isbn = {978-1-4503-6735-6},
  keywords = {concept net,e-commerce,user needs}
}

@misc{luUILayoutGeneration2023,
  title = {{{UI Layout Generation}} with {{LLMs Guided}} by {{UI Grammar}}},
  author = {Lu, Yuwen and Tong, Ziang and Zhao, Qinyi and Zhang, Chengzhi and Li, Toby Jia-Jun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15455},
  eprint = {2310.15455},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.15455},
  urldate = {2024-04-09},
  abstract = {The recent advances in Large Language Models (LLMs) have stimulated interest among researchers and industry professionals, particularly in their application to tasks concerning mobile user interfaces (UIs). This position paper investigates the use of LLMs for UI layout generation. Central to our exploration is the introduction of UI grammar -- a novel approach we proposed to represent the hierarchical structure inherent in UI screens. The aim of this approach is to guide the generative capacities of LLMs more effectively and improve the explainability and controllability of the process. Initial experiments conducted with GPT-4 showed the promising capability of LLMs to produce high-quality user interfaces via in-context learning. Furthermore, our preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results in specific aspects.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@misc{luUILayoutGeneration2023a,
  title = {{{UI Layout Generation}} with {{LLMs Guided}} by {{UI Grammar}}},
  author = {Lu, Yuwen and Tong, Ziang and Zhao, Qinyi and Zhang, Chengzhi and Li, Toby Jia-Jun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15455},
  eprint = {2310.15455},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.15455},
  urldate = {2024-04-09},
  abstract = {The recent advances in Large Language Models (LLMs) have stimulated interest among researchers and industry professionals, particularly in their application to tasks concerning mobile user interfaces (UIs). This position paper investigates the use of LLMs for UI layout generation. Central to our exploration is the introduction of UI grammar -- a novel approach we proposed to represent the hierarchical structure inherent in UI screens. The aim of this approach is to guide the generative capacities of LLMs more effectively and improve the explainability and controllability of the process. Initial experiments conducted with GPT-4 showed the promising capability of LLMs to produce high-quality user interfaces via in-context learning. Furthermore, our preliminary comparative study suggested the potential of the grammar-based approach in improving the quality of generative results in specific aspects.},
  archiveprefix = {arxiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@inproceedings{maccartneyModelingSemanticContainment2008,
  title = {Modeling {{Semantic Containment}} and {{Exclusion}} in {{Natural Language Inference}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Computational Linguistics}} ({{Coling}} 2008)},
  author = {MacCartney, Bill and Manning, Christopher D.},
  editor = {Scott, Donia and Uszkoreit, Hans},
  year = {2008},
  month = aug,
  pages = {521--528},
  publisher = {Coling 2008 Organizing Committee},
  address = {Manchester, UK},
  url = {https://aclanthology.org/C08-1066},
  urldate = {2023-12-15}
}

@inproceedings{mallariLookCriminalExamining2020,
  title = {Do {{I Look Like}} a {{Criminal}}? {{Examining}} How {{Race Presentation Impacts Human Judgement}} of {{Recidivism}}},
  shorttitle = {Do {{I Look Like}} a {{Criminal}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Mallari, Keri and Inkpen, Kori and Johns, Paul and Tan, Sarah and Ramesh, Divya and Kamar, Ece},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376257},
  urldate = {2024-02-05},
  abstract = {Understanding how racial information impacts human decision making in online systems is critical in today's world. Prior work revealed that race information of criminal defendants, when presented as a text field, had no significant impact on users' judgements of recidivism. We replicated and extended this work to explore how and when race information influences users' judgements, with respect to the saliency of presentation. Our results showed that adding photos to the race labels had a significant impact on recidivism predictions for users who identified as female, but not for those who identified as male. The race of the defendant also impacted these results, with black defendants being less likely to be predicted to recidivate compared to white defendants. These results have strong implications for how system-designers choose to display race information, and cautions researchers to be aware of gender and race effects when using Amazon Mechanical Turk workers.},
  isbn = {978-1-4503-6708-0},
  keywords = {bias recidivism,crowd work,gender,human-ai collaboration,legal,mechanical turk,race}
}

@article{markusLeveragingResearcherDomain2023,
  title = {Leveraging {{Researcher Domain Expertise}} to {{Annotate Concepts Within Imbalanced Data}}},
  author = {Markus, Dror K. and {Mor-Lan}, Guy and Sheafer, Tamir and Shenhav, Shaul R.},
  year = {2023},
  month = jul,
  journal = {Communication Methods and Measures},
  volume = {17},
  number = {3},
  pages = {250--271},
  publisher = {Routledge},
  issn = {1931-2458},
  doi = {10.1080/19312458.2023.2182278},
  urldate = {2024-03-29},
  abstract = {As more computational communication researchers turn to supervised machine learning methods for text classification, we note the challenge in implementing such techniques within an imbalanced dataset. Such issues are critical in our domain, where, in many cases, researchers attempt to identify and study theoretically interesting categories that can be rare in a target corpus. Specifically, imbalanced distributions, with a skewed distribution of texts among the categories, can lead to a lengthy and expensive annotation stage, forcing practitioners to sample and label large numbers of texts to train a classification model. In this paper, we provide an overview of the issue, and describe existing strategies for mitigating such challenges. Noting the pitfalls of previous solutions, we then provide a semi-supervised method -- Expert Initiated Latent Space Sampling -- that complements researcher domain expertise with a systematic, unsupervised exploration of the latent semantic space to overcome such limitations. Utilizing simulations to systematically evaluate our method and compare it to existing approaches, we show that our procedure offers significant advantages in terms of efficiency and accuracy in many classification tasks.}
}

@article{masicMedicalDecisionMaking2022,
  title = {Medical {{Decision Making}} - an {{Overview}}},
  author = {Masic, Izet},
  year = {2022},
  month = sep,
  journal = {Acta Informatica Medica},
  volume = {30},
  number = {3},
  pages = {230--235},
  issn = {0353-8109},
  doi = {10.5455/aim.2022.30.230-235},
  urldate = {2024-01-30},
  abstract = {Background: Medical professionals (doctors and other medical staff) in the field of healthcare everyday must make calculated decisions which have important consequences, impacting patients on the individual level, local (community), national or global level. Healthcare professionals must at times make these choices with limited information, resources, and knowledge, and yet is is expected that these decisions are highly calculated and accurate. It is important to familiarise oneself with the exact definitions regarding medical decision making. Objective: The aim of this study was to describe application of the most important rules to help decision makers to be good or excellent decision makers in medical practice at every level of health care system. Methods: The author used descriptive method of explanation teoretical and practical issues regarding application of od decision making processes in the praxis, based on searchied scientific literature about this topic deposited in online databases. Results and Discussion: The author of this paper discussed about important topics: a) the importance of medical decision in emergency situations; b) the varies of decision making with solving problems by medical professionals; c) the limitations when it comes to medical decison making; and d) what doctors need to follow regarding decision making in the praxis. Two factors that have influenced to the decision process: a) degree of uncertainty about future events; b) usefulness of outcomes in any particular case. The clinical decision problem analysis process demands: a) explicit formalization of a decision making problem or the description of the medical problem decision with a registration of all possible actions which have to be undertaken and registration of all the possible so determined outcomes. b) construction of the decision tree which presents all described actions and outcomes with predictions of the probabilities and the choice of the most optimal action based on the probability outcome and its use. Doing this allows us to delve deeper into more intricate options present within medical decision making. Simple put, a decision is a choice between two options. The person or entity conducting that decision is the decision maker. The exact definition is ``Under the decision should imply some specific action which is selected from several variables or which satisfies the expectation that is previously set''.Many different factors and individuals may be involved in medical decision making, with varying consequences, according to different players and settings. Conclusion: A vital component of medical decision making is evaluation. Decision makers must concisely evaluate situations, in order to make better choices. For example, when examining a health care system, their decisions should consider the following questions, such as, what is the health status of the given population? What economic resources are at the disposal of our patients, and government? How effective is the current healthcare model that is already in place? Does the existing social system pay enough attention to the healthcare protection? Does the organisation structure of the healthcare system satisfy? Are the existing practice and the healthcare technologies secure, effective, and suitable? Are the planning, programming, determination and the choice of priority the adequate to the needs of people? How are the monitoring and evaluation of healthcare system quality organised? These are a few examples of evaluation in medical decision making.},
  pmcid = {PMC9560052},
  pmid = {36311160}
}

@inproceedings{maStateoftheartChineseWord2018,
  title = {State-of-the-Art {{Chinese Word Segmentation}} with {{Bi-LSTMs}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ma, Ji and Ganchev, Kuzman and Weiss, David},
  year = {2018},
  month = oct,
  pages = {4902--4908},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1529},
  urldate = {2022-06-09},
  abstract = {A wide variety of neural-network architectures have been proposed for the task of Chinese word segmentation. Surprisingly, we find that a bidirectional LSTM model, when combined with standard deep learning techniques and best practices, can achieve better accuracy on many of the popular datasets as compared to models based on more complex neuralnetwork architectures. Furthermore, our error analysis shows that out-of-vocabulary words remain challenging for neural-network models, and many of the remaining errors are unlikely to be fixed through architecture changes. Instead, more effort should be made on exploring resources for further improvement.}
}

@inproceedings{mcdonnellEasierHarderDepending2023,
  title = {``{{Easier}} or {{Harder}}, {{Depending}} on {{Who}} the {{Hearing Person Is}}'': {{Codesigning Videoconferencing Tools}} for {{Small Groups}} with {{Mixed Hearing Status}}},
  shorttitle = {``{{Easier}} or {{Harder}}, {{Depending}} on {{Who}} the {{Hearing Person Is}}''},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {McDonnell, Emma J and Moon, Soo Hyun and Jiang, Lucy and Goodman, Steven M. and Kushalnagar, Raja and Froehlich, Jon E. and Findlater, Leah},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3580809},
  urldate = {2024-02-08},
  abstract = {With improvements in automated speech recognition and increased use of videoconferencing, real-time captioning has changed significantly. This shift toward broadly available but less accurate captioning invites exploration of the role hearing conversation partners play in shaping the accessibility of a conversation to d/Deaf and hard of hearing (DHH) captioning users. While recent work has explored DHH individuals' videoconferencing experiences with captioning, we focus on established groups' current practices and priorities for future tools to support more accessible online conversations. Our study consists of three codesign sessions, conducted with four groups (17 participants total, 10 DHH, 7 hearing). We found that established groups crafted social accessibility norms that met their relational contexts. We also identify promising directions for future captioning design, including the need to standardize speaker identification and customization, opportunities to provide behavioral feedback during a conversation, and ways that videoconferencing platforms could enable groups to set and share norms.},
  isbn = {978-1-4503-9421-5},
  keywords = {Accessibility,Captioning,d/Deaf and hard of hearing,Videoconferencing}
}

@misc{menShortGPTLayersLarge2024,
  title = {{{ShortGPT}}: {{Layers}} in {{Large Language Models}} Are {{More Redundant Than You Expect}}},
  shorttitle = {{{ShortGPT}}},
  author = {Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03853},
  eprint = {2403.03853},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.03853},
  urldate = {2024-03-07},
  abstract = {As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy in the model architecture.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{micikeviciusMixedPrecisionTraining2018,
  title = {Mixed {{Precision Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2018},
  month = feb,
  url = {https://openreview.net/forum?id=r1gs9JgRZ},
  urldate = {2022-01-17},
  abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural...},
  langid = {english},
  annotation = {00710}
}

@article{minMetaICLLearningLearn2021,
  title = {{{MetaICL}}: {{Learning}} to {{Learn In Context}}},
  shorttitle = {{{MetaICL}}},
  author = {Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  year = {2021},
  journal = {arXiv preprint arXiv:2110.15943},
  eprint = {2110.15943},
  archiveprefix = {arxiv}
}

@misc{mishraTemplateControllableKeywordstotext2020,
  title = {Template {{Controllable}} Keywords-to-Text {{Generation}}},
  author = {Mishra, Abhijit and Chowdhury, Md Faisal Mahbub and Manohar, Sagar and Gutfreund, Dan and Sankaranarayanan, Karthik},
  year = {2020},
  month = nov,
  number = {arXiv:2011.03722},
  eprint = {2011.03722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.03722},
  urldate = {2022-09-07},
  abstract = {This paper proposes a novel neural model for the understudied task of generating text from keywords. The model takes as input a set of un-ordered keywords, and part-of-speech (POS) based template instructions. This makes it ideal for surface realization in any NLG setup. The framework is based on the encode-attend-decode paradigm, where keywords and templates are encoded first, and the decoder judiciously attends over the contexts derived from the encoded keywords and templates to generate the sentences. Training exploits weak supervision, as the model trains on a large amount of labeled data with keywords and POS based templates prepared through completely automatic means. Qualitative and quantitative performance analyses on publicly available test-data in various domains reveal our system's superiority over baselines, built using state-of-the-art neural machine translation and controllable transfer techniques. Our approach is indifferent to the order of input keywords.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{ModelingRelationalData,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}} {\textbar} {{SpringerLink}}},
  doi = {10.1007/978-3-319-93417-4_38},
  urldate = {2021-12-07}
}

@misc{moonAnyMALEfficientScalable2023,
  title = {{{AnyMAL}}: {{An Efficient}} and {{Scalable Any-Modality Augmented Language Model}}},
  shorttitle = {{{AnyMAL}}},
  author = {Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Nagarajan, Tushar and Smith, Matt and Jain, Shashank and Yeh, Chun-Fu and Murugesan, Prakash and Heidari, Peyman and Liu, Yue and Srinet, Kavya and Damavandi, Babak and Kumar, Anuj},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16058},
  eprint = {2309.16058},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.16058},
  urldate = {2024-01-08},
  abstract = {We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{mouNarrativeQuestionAnswering2021a,
  title = {Narrative {{Question Answering}} with {{Cutting-Edge Open-Domain QA Techniques}}: {{A Comprehensive Study}}},
  shorttitle = {Narrative {{Question Answering}} with {{Cutting-Edge Open-Domain QA Techniques}}},
  author = {Mou, Xiangyang and Yang, Chenghao and Yu, Mo and Yao, Bingsheng and Guo, Xiaoxiao and Potdar, Saloni and Su, Hui},
  editor = {Roark, Brian and Nenkova, Ani},
  year = {2021},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {1032--1046},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00411},
  urldate = {2023-12-15},
  abstract = {Recent advancements in open-domain question answering (ODQA), that is, finding answers from large open-domain corpus like Wikipedia, have led to human-level performance on many datasets. However, progress in QA over book stories (Book QA) lags despite its similar task formulation to ODQA. This work provides a comprehensive and quantitative analysis about the difficulty of Book QA: (1) We benchmark the research on the NarrativeQA dataset with extensive experiments with cutting-edge ODQA techniques. This quantifies the challenges Book QA poses, as well as advances the published state-of-the-art with a {$\sim$}7\% absolute improvement on ROUGE-L. (2) We further analyze the detailed challenges in Book QA through human studies.1 Our findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios.}
}

@article{moussiadesPDetectClusteringApproach2005,
  title = {{{PDetect}}: {{A}} Clustering Approach for Detecting Plagiarism in Source Code Datasets},
  shorttitle = {{{PDetect}}},
  author = {Moussiades, Lefteris and Vakali, Athena},
  year = {2005},
  journal = {The computer journal},
  volume = {48},
  number = {6},
  pages = {651--661},
  publisher = {Oxford University Press},
  doi = {10.1093/comjnl/bxh119}
}

@misc{muennighoffOctoPackInstructionTuning2023,
  title = {{{OctoPack}}: {{Instruction Tuning Code Large Language Models}}},
  shorttitle = {{{OctoPack}}},
  author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and {von Werra}, Leandro and Longpre, Shayne},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07124},
  eprint = {2308.07124},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07124},
  urldate = {2024-01-23},
  abstract = {Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2\% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{murnaneDesigningAmbientNarrativeBased2020,
  title = {Designing {{Ambient Narrative-Based Interfaces}} to {{Reflect}} and {{Motivate Physical Activity}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Murnane, Elizabeth L. and Jiang, Xin and Kong, Anna and Park, Michelle and Shi, Weili and Soohoo, Connor and Vink, Luke and Xia, Iris and Yu, Xin and {Yang-Sammataro}, John and Young, Grace and Zhi, Jenny and Moya, Paula and Landay, James A.},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--14},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376478},
  urldate = {2023-08-31},
  abstract = {Numerous technologies now exist for promoting more active lifestyles. However, while quantitative data representations (e.g., charts, graphs, and statistical reports) typify most health tools, growing evidence suggests such feedback can not only fail to motivate behavior but may also harm self-integrity and fuel negative mindsets about exercise. Our research seeks to devise alternative, more qualitative schemes for encoding personal information. In particular, this paper explores the design of data-driven narratives, given the intuitive and persuasive power of stories. We present WhoIsZuki, a smartphone application that visualizes physical activities and goals as components of a multi-chapter quest, where the main character's progress is tied to the user's. We report on our design process involving online surveys, in-lab studies, and in-the-wild deployments, aimed at refining the interface and the narrative and gaining a deep understanding of people's experiences with this type of feedback. From these insights, we contribute recommendations to guide future development of narrative-based applications for motivating healthy behavior.},
  isbn = {978-1-4503-6708-0},
  keywords = {ambient display,mobile health,narrative feedback}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  year = {2022},
  month = jun,
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.09332},
  urldate = {2022-08-15},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{nallapatiAbstractiveTextSummarization2016,
  title = {Abstractive {{Text Summarization}} Using {{Sequence-to-sequence RNNs}} and {{Beyond}}},
  booktitle = {Proceedings of {{The}} 20th {{SIGNLL Conference}} on {{Computational Natural Language Learning}}},
  author = {Nallapati, Ramesh and Zhou, Bowen and {dos Santos}, Cicero and G{\.u}l{\c c}ehre, {\c C}a{\u g}lar and Xiang, Bing},
  year = {2016},
  month = aug,
  pages = {280--290},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/K16-1028},
  urldate = {2021-12-25},
  langid = {english},
  annotation = {01587}
}

@misc{narasimhanTextStyleTransfer2022,
  title = {On {{Text Style Transfer}} via {{Style Masked Language Models}}},
  author = {Narasimhan, Sharan and Shekar, Pooja and Dey, Suvodip and Desarkar, Maunendra Sankar},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06394},
  eprint = {2210.06394},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2210.06394},
  urldate = {2023-01-30},
  abstract = {Text Style Transfer (TST) is performable through approaches such as latent space disentanglement, cycle-consistency losses, prototype editing etc. The prototype editing approach, which is known to be quite successful in TST, involves two key phases a) Masking of source style-associated tokens and b) Reconstruction of this source-style masked sentence conditioned with the target style. We follow a similar transduction method, in which we transpose the more difficult direct source to target TST task to a simpler Style-Masked Language Model (SMLM) Task, wherein, similar to BERT {\textbackslash}cite\{bert\}, the goal of our model is now to reconstruct the source sentence from its style-masked version. We arrive at the SMLM mechanism naturally by formulating prototype editing/ transduction methods in a probabilistic framework, where TST resolves into estimating a hypothetical parallel dataset from a partially observed parallel dataset, wherein each domain is assumed to have a common latent style-masked prior. To generate this style-masked prior, we use "Explainable Attention" as our choice of attribution for a more precise style-masking step and also introduce a cost-effective and accurate "Attribution-Surplus" method of determining the position of masks from any arbitrary attribution model in O(1) time. We empirically show that this non-generational approach well suites the "content preserving" criteria for a task like TST, even for a complex style like Discourse Manipulation. Our model, the Style MLM, outperforms strong TST baselines and is on par with state-of-the-art TST models, which use complex architectures and orders of more parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@article{narayananEfficientLargeScaleLanguage2021,
  title = {Efficient {{Large-Scale Language Model Training}} on {{GPU Clusters Using Megatron-LM}}},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  year = {2021},
  month = aug,
  journal = {arXiv:2104.04473 [cs]},
  eprint = {2104.04473},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2104.04473},
  urldate = {2022-01-22},
  abstract = {Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52\% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Distributed Parallel and Cluster Computing},
  annotation = {00000}
}

@article{neangOrganizingOceanographicInfrastructure2023,
  title = {Organizing {{Oceanographic Infrastructure}}: {{The Work}} of {{Making}} a {{Software Pipeline Repurposable}}},
  shorttitle = {Organizing {{Oceanographic Infrastructure}}},
  author = {Neang, Andrew B. and Sutherland, Will and Ribes, David and Lee, Charlotte P.},
  year = {2023},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {7},
  number = {CSCW1},
  pages = {1--18},
  issn = {2573-0142},
  doi = {10.1145/3579512},
  urldate = {2023-12-29},
  abstract = {Drawing from a longitudinal case study, we inspect the activities of an expanding team of scientists and their collaborators as they sought to develop a novel software pipeline that worked both for themselves and for their wider community. We argue that these two tasks - making the software work for themselves and also for their wider scientific community - could not be differentiated from each other at the beginning of the software development process. Rather, this division of labor and software capacities emerged, articulated by the actors themselves as they went about their tasks. The activities of making the novel software "work" at all, and the "extra work" of making that software repurposable or reusable could not be distinguished until near the end of the development process - rather than defined or structured in advance. We discuss implications for the trajectory of software development, and the practical work of making software repurposable.},
  langid = {english}
}

@misc{nemaDiversityDrivenAttention2018,
  title = {Diversity Driven {{Attention Model}} for {{Query-based Abstractive Summarization}}},
  author = {Nema, Preksha and Khapra, Mitesh and Laha, Anirban and Ravindran, Balaraman},
  year = {2018},
  month = jul,
  number = {arXiv:1704.08300},
  eprint = {1704.08300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.08300},
  urldate = {2022-08-17},
  abstract = {Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28\% (absolute) in ROUGE-L scores.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{nentidisOverviewBioASQ8a2020,
  title = {Overview of {{BioASQ}} 8a and 8b: {{Results}} of the {{Eighth Edition}} of the {{BioASQ Tasks}} a and b.},
  shorttitle = {Overview of {{BioASQ}} 8a and 8b},
  booktitle = {{{CLEF}} ({{Working Notes}})},
  author = {Nentidis, Anastasios and Krithara, Anastasia and Bougiatiotis, Konstantinos and Paliouras, Georgios},
  year = {2020}
}

@misc{nguyenIncontextExampleSelection2023,
  title = {In-Context {{Example Selection}} with {{Influences}}},
  author = {Nguyen, Tai and Wong, Eric},
  year = {2023},
  month = jun,
  number = {arXiv:2302.11042},
  eprint = {2302.11042},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.11042},
  urldate = {2023-11-01},
  abstract = {In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use \${\textbackslash}textit\{in-context influences\}\$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a \$16.3{\textbackslash}\%\$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{niuEHRKnowGenKnowledgeenhancedMultimodal2024,
  title = {{{EHR-KnowGen}}: {{Knowledge-enhanced}} Multimodal Learning for Disease Diagnosis Generation},
  shorttitle = {{{EHR-KnowGen}}},
  author = {Niu, Shuai and Ma, Jing and Bai, Liang and Wang, Zhihua and Guo, Li and Yang, Xian},
  year = {2024},
  month = feb,
  journal = {Information Fusion},
  volume = {102},
  pages = {102069},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.102069},
  urldate = {2024-01-11},
  abstract = {Electronic health records (EHRs) contain diverse patient information, including medical notes, clinical events, and laboratory test results. Integrating this multimodal data can improve disease diagnoses using deep learning models. However, effectively combining different modalities for diagnosis remains challenging. Previous approaches, such as attention mechanisms and contrastive learning, have attempted to address this but do not fully integrate the modalities into a unified feature space. This paper presents EHR-KnowGen, a multimodal learning model enhanced with external domain knowledge, for improved disease diagnosis generation from diverse patient information in EHRs. Unlike previous approaches, our model integrates different modalities into a unified feature space with soft prompts learning and leverages large language models (LLMs) to generate disease diagnoses. By incorporating external domain knowledge from different levels of granularity, we enhance the extraction and fusion of multimodal information, resulting in more accurate diagnosis generation. Experimental results on real-world EHR datasets demonstrate the superiority of our generative model over comparative methods, providing explainable evidence to enhance the understanding of diagnosis results.},
  keywords = {Disease diagnosis,Generative large language model,Knowledge enhancement,Multimodal electronic health records,Multimodal learning}
}

@misc{noriCanGeneralistFoundation2023,
  title = {Can {{Generalist Foundation Models Outcompete Special-Purpose Tuning}}? {{Case Study}} in {{Medicine}}},
  shorttitle = {Can {{Generalist Foundation Models Outcompete Special-Purpose Tuning}}?},
  author = {Nori, Harsha and Lee, Yin Tat and Zhang, Sheng and Carignan, Dean and Edgar, Richard and Fusi, Nicolo and King, Nicholas and Larson, Jonathan and Li, Yuanzhi and Liu, Weishung and Luo, Renqian and McKinney, Scott Mayer and Ness, Robert Osazuwa and Poon, Hoifung and Qin, Tao and Usuyama, Naoto and White, Chris and Horvitz, Eric},
  year = {2023},
  month = nov,
  number = {arXiv:2311.16452},
  eprint = {2311.16452},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.16452},
  urldate = {2023-12-04},
  abstract = {Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27\% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90\% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,I.2.7}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = dec,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2024-01-10},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{ouyangSQuADBioASQAnalysis,
  title = {{{SQuAD}} to {{BioASQ}}: Analysis of General to Specific},
  author = {Ouyang, Karen},
  pages = {7},
  abstract = {As biomedical information in the form of publications and electronic health records (EHR) increases at an increasingly fast pace, there is clear utility in having systems that can automatically handle information extraction, summarization, and question answering tasks. While there have been significant strides in improving language tasks for general language, addressing domain-specific contexts still remains challenging. In this project, I apply and fine-tune models to the SQuAD dataset and further modify/adapt for biomedical domain-specific question answering. I evaluated and compared performance on the SQuAD dataset and BioASQ, a biomedical literature QA dataset, with the goal of analyzing and developing approaches to leverage unsupervised language models for domain-specific applications. Upon generating various fine-tuned models, the best performance for general language SQuAD QA achieved an F1 score of 76.717, EM score of 73.379, and for biomedical-specific BioASQ QA achieved an F1 score of 70.348 and EM score of 49.902.},
  langid = {english}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02155},
  urldate = {2023-05-19},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{paliourasBioASQChallengeLargescale2012,
  title = {{{BioASQ}}: {{A}} Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering},
  author = {Paliouras, Georgios},
  year = {2012},
  month = nov,
  url = {sites/default/files/PublicDocuments/2012-Paliouras-BioASQ.pdf},
  keywords = {BioASQ public document}
}

@article{pampariEmrQALargeCorpus2018,
  title = {{{emrQA}}: {{A Large Corpus}} for {{Question Answering}} on {{Electronic Medical Records}}},
  shorttitle = {{{emrQA}}},
  author = {Pampari, Anusri and Raghavan, Preethi and Liang, Jennifer and Peng, Jian},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.00732 [cs]},
  eprint = {1809.00732},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1809.00732},
  urldate = {2021-11-30},
  abstract = {We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets{\S}. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ questionanswer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{pappasAUEBNLPBioASQBiomedical2020,
  title = {{{AUEB-NLP}} at {{BioASQ}} 8: {{Biomedical Document}} and {{Snippet Retrieval}}.},
  shorttitle = {{{AUEB-NLP}} at {{BioASQ}} 8},
  booktitle = {{{CLEF}} ({{Working Notes}})},
  author = {Pappas, Dimitris and Stavropoulos, Petros and Androutsopoulos, Ion},
  year = {2020}
}

@inproceedings{pappasBioMRCDatasetBiomedical2020,
  title = {{{BioMRC}}: {{A Dataset}} for {{Biomedical Machine Reading Comprehension}}},
  shorttitle = {{{BioMRC}}},
  booktitle = {Proceedings of the 19th {{SIGBioMed Workshop}} on {{Biomedical Language Processing}}},
  author = {Pappas, Dimitris and Stavropoulos, Petros and Androutsopoulos, Ion and McDonald, Ryan},
  year = {2020},
  month = jul,
  pages = {140--149},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.bionlp-1.15},
  urldate = {2021-12-25},
  abstract = {We introduceBIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural MRC models that had been tested on BIOREAD perform much better on BIOMRC, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to BIOREAD, and biomedical experts perform even better. We also introduce a new BERT-based MRC model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.},
  langid = {english},
  annotation = {00009}
}

@inproceedings{pappasBioReadNewDataset2018,
  title = {{{BioRead}}: {{A New Dataset}} for {{Biomedical Reading Comprehension}}},
  shorttitle = {{{BioRead}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Pappas, Dimitris and Androutsopoulos, Ion and Papageorgiou, Haris},
  year = {2018},
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  address = {Miyazaki, Japan},
  url = {https://aclanthology.org/L18-1439},
  urldate = {2021-12-25},
  langid = {english},
  annotation = {00012}
}

@inproceedings{parikhElimiNetModelEliminating2018,
  title = {{{ElimiNet}}: {{A Model}} for {{Eliminating Options}} for {{Reading Comprehension}} with {{Multiple Choice Questions}}},
  shorttitle = {{{ElimiNet}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Parikh, Soham and Sai, Ananya and Nema, Preksha and Khapra, Mitesh},
  year = {2018},
  month = jul,
  pages = {4272--4278},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Stockholm, Sweden},
  doi = {10.24963/ijcai.2018/594},
  urldate = {2022-04-13},
  abstract = {The task of Reading Comprehension with Multiple Choice Questions, requires a human (or machine) to read a given \{passage, question\} pair and select one of the n given options. The current state of the art model for this task first computes a questionaware representation for the passage and then selects the option which has the maximum similarity with this representation. However, when humans perform this task they do not just focus on option selection but use a combination of elimination and selection. Specifically, a human would first try to eliminate the most irrelevant option and then read the passage again in the light of this new information (and perhaps ignore portions corresponding to the eliminated option). This process could be repeated multiple times till the reader is finally ready to select the correct option. We propose ElimiNet, a neural network-based model which tries to mimic this process. Specifically, it has gates which decide whether an option can be eliminated given the \{passage, question\} pair and if so it tries to make the passage representation orthogonal to this eliminated option (akin to ignoring portions of the passage corresponding to the eliminated option). The model makes multiple rounds of partial elimination to refine the passage representation and finally uses a selection module to pick the best option. We evaluate our model on the recently released large scale RACE dataset and show that it outperforms the current state of the art model on 7 out of the 13 question types in this dataset. Further, we show that taking an ensemble of our elimination-selection based method with a selection based method gives us an improvement of 3.1\% over the best-reported performance on this dataset.},
  isbn = {978-0-9992411-2-7},
  langid = {english}
}

@misc{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.03442},
  eprint = {2304.03442},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.03442},
  urldate = {2023-06-25},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@misc{parkSocialSimulacraCreating2022,
  title = {Social {{Simulacra}}: {{Creating Populated Prototypes}} for {{Social Computing Systems}}},
  shorttitle = {Social {{Simulacra}}},
  author = {Park, Joon Sung and Popowski, Lindsay and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2022},
  month = aug,
  number = {arXiv:2208.04024},
  eprint = {2208.04024},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2208.04024},
  urldate = {2023-06-25},
  abstract = {Social computing prototypes probe the social behaviors that may arise in an envisioned system design. This prototyping practice is currently limited to recruiting small groups of people. Unfortunately, many challenges do not arise until a system is populated at a larger scale. Can a designer understand how a social system might behave when populated, and make adjustments to the design before the system falls prey to such challenges? We introduce social simulacra, a prototyping technique that generates a breadth of realistic social interactions that may emerge when a social computing system is populated. Social simulacra take as input the designer's description of a community's design -- goal, rules, and member personas -- and produce as output an instance of that design with simulated behavior, including posts, replies, and anti-social behaviors. We demonstrate that social simulacra shift the behaviors that they generate appropriately in response to design changes, and that they enable exploration of "what if?" scenarios where community members or moderators intervene. To power social simulacra, we contribute techniques for prompting a large language model to generate thousands of distinct community members and their social interactions with each other; these techniques are enabled by the observation that large language models' training data already includes a wide variety of positive and negative behavior on social media platforms. In evaluations, we show that participants are often unable to distinguish social simulacra from actual community behavior and that social computing designers successfully refine their social computing designs when using social simulacra.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@article{parrANTLRPredicatedLLParser1995,
  title = {{{ANTLR}}: A Predicated-{{{\emph{LL}}}}{\emph{(k)}} Parser Generator},
  shorttitle = {{{ANTLR}}},
  author = {Parr, T. J. and Quong, R. W.},
  year = {1995},
  month = jul,
  journal = {Software---Practice \& Experience},
  volume = {25},
  number = {7},
  pages = {789--810},
  issn = {0038-0644},
  doi = {10.1002/spe.4380250705},
  urldate = {2022-06-06},
  keywords = {compiler,LL(k) parser,parser generator,parsing,predicates}
}

@inproceedings{pascualPlugandPlayMethodControlled2021,
  title = {A {{Plug-and-Play Method}} for {{Controlled Text Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Pascual, Damian and Egressy, Beni and Meister, Clara and Cotterell, Ryan and Wattenhofer, Roger},
  year = {2021},
  month = nov,
  pages = {3973--3997},
  publisher = {Association for Computational Linguistics},
  address = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.findings-emnlp.334},
  urldate = {2022-12-25},
  abstract = {Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{pawelczakBenefitsDrawbacksSource2018,
  title = {Benefits and Drawbacks of Source Code Plagiarism Detection in Engineering Education},
  booktitle = {2018 {{IEEE Global Engineering Education Conference}} ({{EDUCON}})},
  author = {Pawelczak, Dieter},
  year = {2018},
  month = apr,
  pages = {1048--1056},
  issn = {2165-9567},
  doi = {10.1109/EDUCON.2018.8363346},
  abstract = {Source code plagiarism is wide spread in beginners' programming courses. Especially, if programming is a minor subject, as for instance in engineering degrees. It is very tempting for students during a programming assignment to use a working copy of a fellow student rather than struggling with the time-consuming coding by themselves. But as learning programming requires a significant personal commitment, we confirm the results of other studies, that cheating leads to higher failure rates and lower scores in the examination. Automatic plagiarism detection systems are therefore measures against cheating. We analyzed the students' achievements and opinions during the last 5 years of operating an automated assessment system with plagiarism detection. The paper discusses in detail the benefits of such a system, e.g. the equal treatment of all students compared to manual plagiarism checks, and shows also the disadvantages, e.g. code obfuscation, that students perform in order to circumvent the system.},
  keywords = {electronic assessment,Encoding,engineering education,Engineering education,plagiarism,Plagiarism,Programming profession,source code plagiarism,Task analysis,teaching programming,Tools}
}

@article{pennycookShiftingAttentionAccuracy2021,
  title = {Shifting Attention to Accuracy Can Reduce Misinformation Online},
  author = {Pennycook, Gordon and Epstein, Ziv and Mosleh, Mohsen and Arechar, Antonio A. and Eckles, Dean and Rand, David G.},
  year = {2021},
  month = apr,
  journal = {Nature},
  volume = {592},
  number = {7855},
  pages = {590--595},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03344-2},
  urldate = {2023-06-25},
  abstract = {In recent years, there has been a great deal of concern about the proliferation of false and misleading news on social media1--4. Academics and practitioners alike have asked why people share such misinformation, and sought solutions to reduce the sharing of misinformation5--7. Here, we attempt to address both of these questions. First, we find that the veracity of headlines has little effect on sharing intentions, despite having a large effect on judgments of accuracy. This dissociation suggests that sharing does not necessarily indicate belief. Nonetheless, most participants say it is important to share only accurate news. To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show that subtly shifting attention to accuracy increases the quality of news that people subsequently share. Together with additional computational analyses, these findings indicate that people often share misinformation because their attention is focused on factors other than accuracy---and therefore they fail to implement a strongly~held preference for accurate sharing. Our results challenge the popular claim that people value partisanship over accuracy8,9, and provide evidence for scalable attention-based interventions that social media platforms could easily implement to counter misinformation online.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Communication,Decision making,Human behaviour,Technology}
}

@misc{plantYouAreWhat2022,
  title = {You {{Are What You Write}}: {{Preserving Privacy}} in the {{Era}} of {{Large Language Models}}},
  shorttitle = {You {{Are What You Write}}},
  author = {Plant, Richard and Giuffrida, Valerio and Gkatzia, Dimitra},
  year = {2022},
  month = apr,
  number = {arXiv:2204.09391},
  eprint = {2204.09391},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.09391},
  urldate = {2024-02-13},
  abstract = {Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e.g. through adversarial attacks. We present an empirical investigation into the extent of the personal information encoded into pre-trained representations by a range of popular models, and we show a positive correlation between the complexity of a model, the amount of data used in pre-training, and data leakage. In this paper, we present the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender). The results show since larger and more complex models are more prone to leaking private information, use of privacy-preserving methods is highly desirable. We also find that highly privacy-preserving technologies like differential privacy (DP) can have serious model utility effects, which can be ameliorated using hybrid or metric-DP techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{qianITunesPapersRedefining2019,
  title = {Beyond {{iTunes}} for {{Papers}}: {{Redefining}} the {{Unit}} of {{Interaction}} in {{Literature Review Tools}}},
  shorttitle = {Beyond {{iTunes}} for {{Papers}}},
  booktitle = {Companion {{Publication}} of the 2019 {{Conference}} on {{Computer Supported Cooperative Work}} and {{Social Computing}}},
  author = {Qian, Xin and Erhart, Matt J. and Kittur, Aniket and Lutters, Wayne G. and Chan, Joel},
  year = {2019},
  month = nov,
  series = {{{CSCW}} '19 {{Companion}}},
  pages = {341--346},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3311957.3359455},
  urldate = {2024-02-04},
  abstract = {Conducting an effective literature review is an essential step in all scientific work. However, the process is difficult, particularly for interdisciplinary work. Here, we articulate a key avenue for improvement for literature review tools: supporting the appropriate unit of interaction, which we argue is a "grounded claim", a concise statement linked to key contextual details such as evidence. However, there are significant cognitive and interaction costs in creating them. We share insights from our development of a prototype literature review tool, the Knowledge Compressor, that aims to lower these costs.},
  isbn = {978-1-4503-6692-2},
  keywords = {argument diagramming,knowledge reuse,literature review,sensemaking}
}

@article{QualityQuantitySynthetic2023,
  title = {Quality {$>$} {{Quantity}}: {{Synthetic Corpora}} from {{Foundation Models}} for {{Closed-Domain Extractive Question Answering}}},
  shorttitle = {Quality {$>$} {{Quantity}}},
  year = {2023},
  month = jun,
  url = {https://openreview.net/forum?id=OxoP1qFotz&referrer=%5BReviewer%20Console%5D(%2Fgroup%3Fid%3DEMNLP%2F2023%2FConference%2FReviewers%23assigned-papers)},
  urldate = {2023-08-12},
  abstract = {Domain adaptation, the process of training a model in one domain and applying it to another, has been extensively explored in machine learning. While training a domain-specific foundation model (FM) from scratch is an option, recent methods have focused on adapting pre-trained FMs for domain-specific tasks. However, our experiments reveal that either approach does not consistently achieve state-of-the-art (SOTA) results in the target domain. In this work, we study extractive question answering within closed domains and introduce the concept of targeted pre-training. This involves determining and generating relevant data to further pre-train our models, as opposed to the conventional philosophy of utilizing domain-specific FMs trained on a wide range of data. Our proposed framework uses Galactica to generate synthetic, ``targeted'' corpora that align with specific writing styles and topics, such as research papers and radiology reports. This process can be viewed as a form of knowledge distillation. We apply our method to two biomedical extractive question answering datasets, COVID-QA and RadQA, achieving a new benchmark on the former and demonstrating overall improvements on the latter. Code available upon publication.},
  langid = {english}
}

@article{rajpurkarKnowWhatYou2018a,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  shorttitle = {Know {{What You Don}}'t {{Know}}},
  author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.03822 [cs]},
  eprint = {1806.03822},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1806.03822},
  urldate = {2021-12-25},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {00000}
}

@inproceedings{rajpurkarSQuAD1000002016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  pages = {2383--2392},
  publisher = {Association for Computational Linguistics},
  address = {Austin, Texas},
  doi = {10.18653/v1/D16-1264},
  urldate = {2022-01-17},
  annotation = {03991}
}

@article{rajpurkarSQuAD1000002016a,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  month = oct,
  journal = {arXiv:1606.05250 [cs]},
  eprint = {1606.05250},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1606.05250},
  urldate = {2021-12-25},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {00000}
}

@misc{raposoMixtureofDepthsDynamicallyAllocating2024,
  title = {Mixture-of-{{Depths}}: {{Dynamically}} Allocating Compute in Transformer-Based Language Models},
  shorttitle = {Mixture-of-{{Depths}}},
  author = {Raposo, David and Ritter, Sam and Richards, Blake and Lillicrap, Timothy and Humphreys, Peter Conway and Santoro, Adam},
  year = {2024},
  month = apr,
  number = {arXiv:2404.02258},
  eprint = {2404.02258},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.02258},
  urldate = {2024-04-04},
  abstract = {Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens (\$k\$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-\$k\$ routing mechanism. Since \$k\$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the \$k\$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50{\textbackslash}\% faster to step during post-training sampling.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{rasmussenChallengeDataAnnotation2022,
  title = {The {{Challenge}} of {{Data Annotation}} in {{Deep Learning}}---{{A Case Study}} on {{Whole Plant Corn Silage}}},
  author = {Rasmussen, Christoffer B{\o}gelund and Kirk, Kristian and Moeslund, Thomas B.},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {4},
  pages = {1596},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22041596},
  urldate = {2024-02-13},
  abstract = {Recent advances in computer vision are primarily driven by the usage of deep learning, which is known to require large amounts of data, and creating datasets for this purpose is not a trivial task. Larger benchmark datasets often have detailed processes with multiple stages and users with different roles during annotation. However, this can be difficult to implement in smaller projects where resources can be limited. Therefore, in this work we present our processes for creating an image dataset for kernel fragmentation and stover overlengths in Whole Plant Corn Silage. This includes the guidelines for annotating object instances in respective classes and statistics of gathered annotations. Given the challenging image conditions, where objects are present in large amounts of occlusion and clutter, the datasets appear appropriate for training models. However, we experience annotator inconsistency, which can hamper evaluation. Based on this we argue the importance of having an evaluation form independent of the manual annotation where we evaluate our models with physically based sieving metrics. Additionally, instead of the traditional time-consuming manual annotation approach, we evaluate Semi-Supervised Learning as an alternative, showing competitive results while requiring fewer annotations. Specifically, given a relatively large supervised set of around 1400 images we can improve the Average Precision by a number of percentage points. Additionally, we show a significantly large improvement when using an extremely small set of just over 100 images, with over 3{\texttimes} in Average Precision and up to 20 percentage points when estimating the quality.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {annotation,dataset,deep learning,semi-supervised learning,whole plant corn silage}
}

@article{ratnerSnorkelRapidTraining2017,
  title = {Snorkel: {{Rapid Training Data Creation}} with {{Weak Supervision}}},
  shorttitle = {Snorkel},
  author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and R{\'e}, Christopher},
  year = {2017},
  month = nov,
  journal = {Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases},
  volume = {11},
  number = {3},
  pages = {269--282},
  issn = {2150-8097},
  doi = {10.14778/3157794.3157797},
  urldate = {2023-08-02},
  abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8{\texttimes} faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8{\texttimes} speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
  pmcid = {PMC5951191},
  pmid = {29770249}
}

@inproceedings{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = nov,
  pages = {3982--3992},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1410},
  urldate = {2023-06-24},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textbackslash}textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.}
}

@inproceedings{richardsonMCTestChallengeDataset2013,
  title = {{{MCTest}}: {{A Challenge Dataset}} for the {{Open-Domain Machine Comprehension}} of {{Text}}},
  shorttitle = {{{MCTest}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Richardson, Matthew and Burges, Christopher J.C. and Renshaw, Erin},
  year = {2013},
  month = oct,
  pages = {193--203},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, Washington, USA},
  url = {https://aclanthology.org/D13-1020},
  urldate = {2021-12-25},
  annotation = {00610}
}

@inproceedings{rouzegarEnhancingTextClassification2024,
  title = {Enhancing {{Text Classification}} through {{LLM-Driven Active Learning}} and {{Human Annotation}}},
  booktitle = {Proceedings of {{The}} 18th {{Linguistic Annotation Workshop}} ({{LAW-XVIII}})},
  author = {Rouzegar, Hamidreza and Makrehchi, Masoud},
  editor = {Henning, Sophie and Stede, Manfred},
  year = {2024},
  month = mar,
  pages = {98--111},
  publisher = {Association for Computational Linguistics},
  address = {St. Julians, Malta},
  url = {https://aclanthology.org/2024.law-1.10},
  urldate = {2024-04-09},
  abstract = {In the context of text classification, the financial burden of annotation exercises for creating training data is a critical issue. Active learning techniques, particularly those rooted in uncertainty sampling, offer a cost-effective solution by pinpointing the most instructive samples for manual annotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an alternative for automated annotation but come with concerns regarding their reliability. This study introduces a novel methodology that integrates human annotators and LLMs within an Active Learning framework. We conducted evaluations on three public datasets. IMDB for sentiment analysis, a Fake News dataset for authenticity discernment, and a Movie Genres dataset for multi-label classification.The proposed framework integrates human annotation with the output of LLMs, depending on the model uncertainty levels. This strategy achieves an optimal balance between cost efficiency and classification performance. The empirical results show a substantial decrease in the costs associated with data annotation while either maintaining or improving model accuracy.}
}

@inproceedings{rouzegarEnhancingTextClassification2024a,
  title = {Enhancing {{Text Classification}} through {{LLM-Driven Active Learning}} and {{Human Annotation}}},
  booktitle = {Proceedings of {{The}} 18th {{Linguistic Annotation Workshop}} ({{LAW-XVIII}})},
  author = {Rouzegar, Hamidreza and Makrehchi, Masoud},
  editor = {Henning, Sophie and Stede, Manfred},
  year = {2024},
  month = mar,
  pages = {98--111},
  publisher = {Association for Computational Linguistics},
  address = {St. Julians, Malta},
  url = {https://aclanthology.org/2024.law-1.10},
  urldate = {2024-04-09},
  abstract = {In the context of text classification, the financial burden of annotation exercises for creating training data is a critical issue. Active learning techniques, particularly those rooted in uncertainty sampling, offer a cost-effective solution by pinpointing the most instructive samples for manual annotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an alternative for automated annotation but come with concerns regarding their reliability. This study introduces a novel methodology that integrates human annotators and LLMs within an Active Learning framework. We conducted evaluations on three public datasets. IMDB for sentiment analysis, a Fake News dataset for authenticity discernment, and a Movie Genres dataset for multi-label classification.The proposed framework integrates human annotation with the output of LLMs, depending on the model uncertainty levels. This strategy achieves an optimal balance between cost efficiency and classification performance. The empirical results show a substantial decrease in the costs associated with data annotation while either maintaining or improving model accuracy.},
  langid = {american}
}

@inproceedings{rubinLearningRetrievePrompts2022,
  title = {Learning {{To Retrieve Prompts}} for {{In-Context Learning}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  editor = {Carpuat, Marine and {de Marneffe}, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
  year = {2022},
  month = jul,
  pages = {2655--2671},
  publisher = {Association for Computational Linguistics},
  address = {Seattle, United States},
  doi = {10.18653/v1/2022.naacl-main.191},
  urldate = {2023-11-14},
  abstract = {In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2023-09-12},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@inproceedings{sangMBTIPersonalityPrediction2022,
  title = {{{MBTI Personality Prediction}} for {{Fictional Characters Using Movie Scripts}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Sang, Yisi and Mou, Xiangyang and Yu, Mo and Wang, Dakuo and Li, Jing and Stanton, Jeffrey},
  year = {2022},
  month = dec,
  pages = {6715--6724},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.findings-emnlp.500},
  urldate = {2023-09-26},
  abstract = {An NLP model that understands stories should be able to understand the characters in them. To support the development of neural models for this purpose, we construct a benchmark, Story2Personality. The task is to predict a movie character's MBTI or Big 5 personality types based on the narratives of the character. Experiments show that our task is challenging for the existing text classification models, as none is able to largely outperform random guesses. We further proposed a multi-view model for personality prediction using both verbal and non-verbal descriptions, which gives improvement compared to using only verbal descriptions. The uniqueness and challenges in our dataset call for the development of narrative comprehension techniques from the perspective of understanding characters.}
}

@article{satheeshResumeRankingBased2020,
  title = {Resume {{Ranking}} Based on {{Job Description}} Using {{SpaCy NER}} Model},
  author = {Satheesh, Dr K and Jahnavi, A and Iswarya, L and Ayesha, K and Bhanusekhar, G and Hanisha, K},
  year = {2020},
  volume = {07},
  number = {05},
  pages = {5},
  langid = {english}
}

@article{schaferEmpiricalEvaluationUsing2024,
  title = {An {{Empirical Evaluation}} of {{Using Large Language Models}} for {{Automated Unit Test Generation}}},
  author = {Sch{\"a}fer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
  year = {2024},
  month = jan,
  journal = {IEEE Transactions on Software Engineering},
  volume = {50},
  number = {1},
  pages = {85--105},
  issn = {1939-3520},
  doi = {10.1109/TSE.2023.3334955},
  urldate = {2024-01-16},
  abstract = {Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2\% and branch coverage of 52.8\%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3\% statement coverage and 25.6\% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8\% of TestPilot's generated tests have {\textbackslash}leq{$\leq$} 50\% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run TestPilot with two additional LLMs, OpenAI's older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2\% median statement coverage), and somewhat worse results with the latter (54.0\% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.}
}

@inproceedings{schroderRevisitingUncertaintybasedQuery2022,
  title = {Revisiting {{Uncertainty-based Query Strategies}} for {{Active Learning}} with {{Transformers}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Schr{\"o}der, Christopher and Niekler, Andreas and Potthast, Martin},
  year = {2022},
  pages = {2194--2203},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.172},
  urldate = {2023-11-16},
  abstract = {Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (``transformers'') became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.},
  langid = {english}
}

@inproceedings{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = {2018},
  month = feb,
  url = {https://openreview.net/forum?id=H1aIuk-RW},
  urldate = {2023-11-16},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  langid = {english}
}

@inproceedings{seoBidirectionalAttentionFlow2017,
  title = {Bidirectional {{Attention Flow}} for {{Machine Comprehension}}},
  booktitle = {The {{International Conference}} on {{Learning Representations}}},
  author = {Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  year = {2017},
  eprint = {1611.01603},
  url = {http://arxiv.org/abs/1611.01603},
  urldate = {2021-11-30},
  abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bidirectional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{sharmaActiveLearningRationales2015,
  title = {Active {{Learning}} with {{Rationales}} for {{Text Classification}}},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Sharma, Manali and Zhuang, Di and Bilgic, Mustafa},
  editor = {Mihalcea, Rada and Chai, Joyce and Sarkar, Anoop},
  year = {2015},
  month = may,
  pages = {441--451},
  publisher = {Association for Computational Linguistics},
  address = {Denver, Colorado},
  doi = {10.3115/v1/N15-1047},
  urldate = {2023-11-13}
}

@inproceedings{shenDeepActiveLearning2017,
  title = {Deep {{Active Learning}} for {{Named Entity Recognition}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Representation Learning}} for {{NLP}}},
  author = {Shen, Yanyao and Yun, Hyokun and Lipton, Zachary and Kronrod, Yakov and Anandkumar, Animashree},
  editor = {Blunsom, Phil and Bordes, Antoine and Cho, Kyunghyun and Cohen, Shay and Dyer, Chris and Grefenstette, Edward and Hermann, Karl Moritz and Rimell, Laura and Weston, Jason and Yih, Scott},
  year = {2017},
  month = aug,
  pages = {252--256},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/W17-2630},
  urldate = {2023-11-13},
  abstract = {Deep neural networks have advanced the state of the art in named entity recognition. However, under typical training procedures, advantages over classical methods emerge only with large datasets. As a result, deep learning is employed only when large public datasets or a large budget for manually labeling data is available. In this work, we show otherwise: by combining deep learning with active learning, we can outperform classical methods even with a significantly smaller amount of training data.}
}

@inproceedings{shiEvaluationNeuralCode2022,
  title = {On the {{Evaluation}} of {{Neural Code Summarization}}},
  booktitle = {Proceedings of the 44th {{International Conference}} on {{Software Engineering}}},
  author = {Shi, Ensheng and Wang, Yanlin and Du, Lun and Chen, Junjie and Han, Shi and Zhang, Hongyu and Zhang, Dongmei and Sun, Hongbin},
  year = {2022},
  month = may,
  eprint = {2107.07112},
  primaryclass = {cs},
  pages = {1597--1608},
  doi = {10.1145/3510003.3510060},
  urldate = {2023-03-29},
  abstract = {Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from -18{\textbackslash}\% to +25{\textbackslash}\%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important characteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering}
}

@misc{shiLargeLanguageModels2023,
  title = {Large {{Language Models Can Be Easily Distracted}} by {{Irrelevant Context}}},
  author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00093},
  eprint = {2302.00093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00093},
  urldate = {2023-03-13},
  abstract = {Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{shiReconReducingConflicting2023,
  title = {Recon: {{Reducing Conflicting Gradients}} from the {{Root}} for {{Multi-Task Learning}}},
  shorttitle = {Recon},
  author = {Shi, Guangyuan and Li, Qimai and Zhang, Wenlong and Chen, Jiaxin and Wu, Xiao-Ming},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11289},
  eprint = {2302.11289},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.11289},
  urldate = {2023-04-11},
  abstract = {A fundamental challenge for multi-task learning is that different tasks may conflict with each other when they are solved jointly, and a cause of this phenomenon is conflicting gradients during optimization. Recent works attempt to mitigate the influence of conflicting gradients by directly altering the gradients based on some criteria. However, our empirical study shows that ``gradient surgery'' cannot effectively reduce the occurrence of conflicting gradients. In this paper, we take a different approach to reduce conflicting gradients from the root. In essence, we investigate the task gradients w.r.t. each shared network layer, select the layers with high conflict scores, and turn them to task-specific layers. Our experiments show that such a simple approach can greatly reduce the occurrence of conflicting gradients in the remaining shared layers and achieve better performance, with only a slight increase in model parameters in many cases. Our approach can be easily applied to improve various state-of-the-art methods including gradient manipulation methods and branched architecture search methods. Given a network architecture (e.g., ResNet18), it only needs to search for the conflict layers once, and the network can be modified to be used with different methods on the same or even different datasets to gain performance improvement. The source code is available at https://github.com/moukamisama/Recon.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{shoeybiMegatronLMTrainingMultiBillion2020,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  year = {2020},
  month = mar,
  journal = {arXiv:1909.08053 [cs]},
  eprint = {1909.08053},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.08053},
  urldate = {2022-01-19},
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {00333}
}

@inproceedings{smithSkillExtractionDomainSpecific2021,
  title = {Skill {{Extraction}} for {{Domain-Specific Text Retrieval}} in a {{Job-Matching Platform}}},
  booktitle = {Experimental {{IR Meets Multilinguality}}, {{Multimodality}}, and {{Interaction}}},
  author = {Smith, Ellery and Weiler, Andreas and Braschler, Martin},
  editor = {Candan, K. Sel{\c c}uk and Ionescu, Bogdan and Goeuriot, Lorraine and Larsen, Birger and M{\"u}ller, Henning and Joly, Alexis and Maistro, Maria and Piroi, Florina and Faggioli, Guglielmo and Ferro, Nicola},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {116--128},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-85251-1_10},
  abstract = {We discuss a domain-specific retrieval application for matching job seekers with open positions that uses a novel syntactic method of extracting skill-terms from the text of natural language job advertisements. Our new method is contrasted with two word embeddings methods, using word2vec. We define the notion of a skill headword, and present an algorithm that learns syntactic dependency patterns to recognize skill-terms. In all metrics, our syntactic method outperforms both word embeddings methods. Moreover, the word embeddings approaches were unable to model a meaningful distinction between skill-terms and non-skill-terms, while our syntactic approach was able to perform this successfully. We also show how these extracted skills can be used to automatically construct a semantic job-skills ontology, and facilitate a job-to-candidate matching system.},
  isbn = {978-3-030-85251-1},
  langid = {english},
  keywords = {Domain-specific retrieval,Term extraction}
}

@misc{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.03585},
  urldate = {2023-11-14},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning}
}

@article{sparckSTATISTICALINTERPRETATIONTERM1972,
  title = {A {{STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL}}},
  author = {SPARCK, JONES KAREN},
  year = {1972},
  month = jan,
  journal = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  publisher = {MCB UP Ltd},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  urldate = {2023-06-24},
  abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.}
}

@misc{spathisFirstStepHardest2023,
  title = {The First Step Is the Hardest: {{Pitfalls}} of {{Representing}} and {{Tokenizing Temporal Data}} for {{Large Language Models}}},
  shorttitle = {The First Step Is the Hardest},
  author = {Spathis, Dimitris and Kawsar, Fahim},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06236},
  eprint = {2309.06236},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.06236},
  urldate = {2024-03-18},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this "modality gap". While the capability of language models to generalize to other modalities with minimal or no finetuning is exciting, this paper underscores the fact that their outputs cannot be meaningful if they stumble over input nuances.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{srivastavaNICEOptimizeInContext2024a,
  title = {{{NICE}}: {{To Optimize In-Context Examples}} or {{Not}}?},
  shorttitle = {{{NICE}}},
  author = {Srivastava, Pragya and Golechha, Satvik and Deshpande, Amit and Sharma, Amit},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06733},
  eprint = {2402.06733},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.06733},
  urldate = {2024-02-13},
  abstract = {Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called {\textbackslash}metriclong\{\} ({\textbackslash}metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added details, we validate our hypothesis empirically by computing {\textbackslash}metric with query-dependent bins of examples, comparing different instructions with ICE selection methods, and performing label perturbation experiments. We conclude that tasks can be divided into two broad classes based on the {\textbackslash}metric metric, where the returns on ICE optimization follow predictable trends when instructions are provided in the prompt.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{steyversCalibrationGapModel2024,
  title = {The {{Calibration Gap}} between {{Model}} and {{Human Confidence}} in {{Large Language Models}}},
  author = {Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas and Smyth, Padhraic},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13835},
  eprint = {2401.13835},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.13835},
  urldate = {2024-01-28},
  abstract = {For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@article{sturgeonPopulationbasedStudyCardiovascular2019,
  title = {A Population-Based Study of Cardiovascular Disease Mortality Risk in {{US}} Cancer Patients},
  author = {Sturgeon, Kathleen M and Deng, Lei and Bluethmann, Shirley M and Zhou, Shouhao and Trifiletti, Daniel M and Jiang, Changchuan and Kelly, Scott P and Zaorsky, Nicholas G},
  year = {2019},
  month = dec,
  journal = {European Heart Journal},
  volume = {40},
  number = {48},
  pages = {3889--3897},
  issn = {0195-668X},
  doi = {10.1093/eurheartj/ehz766},
  urldate = {2024-03-29},
  abstract = {This observational study characterized cardiovascular disease (CVD) mortality risk for multiple cancer sites, with respect to the following: (i) continuous calendar year, (ii) age at diagnosis, and (iii) follow-up time after diagnosis.The Surveillance, Epidemiology, and End Results program was used to compare the US general population to 3 234 256 US cancer survivors (1973--2012). Standardized mortality ratios (SMRs) were calculated using coded cause of death from CVDs (heart disease, hypertension, cerebrovascular disease, atherosclerosis, and aortic aneurysm/dissection). Analyses were adjusted by age, race, and sex. Among 28 cancer types, 1 228 328 patients (38.0\%) died from cancer and 365 689 patients (11.3\%) died from CVDs. Among CVDs, 76.3\% of deaths were due to heart disease. In eight cancer sites, CVD mortality risk surpassed index-cancer mortality risk in at least one calendar year. Cardiovascular disease mortality risk was highest in survivors diagnosed at \&lt;35\,years of age. Further, CVD mortality risk is highest (SMR 3.93, 95\% confidence interval 3.89--3.97) within the first year after cancer diagnosis, and CVD mortality risk remains elevated throughout follow-up compared to the general population.The majority of deaths from CVD occur in patients diagnosed with breast, prostate, or bladder cancer. We observed that from the point of cancer diagnosis forward into survivorship cancer patients (all sites) are at elevated risk of dying from CVDs compared to the general US population. In endometrial cancer, the first year after diagnosis poses a very high risk of dying from CVDs, supporting early involvement of cardiologists in such patients.}
}

@inproceedings{suCAiRECOVIDQuestionAnswering2020,
  title = {{{CAiRE-COVID}}: {{A Question Answering}} and {{Query-focused Multi-Document Summarization System}} for {{COVID-19 Scholarly Information Management}}},
  shorttitle = {{{CAiRE-COVID}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{NLP}} for {{COVID-19}} ({{Part}} 2) at {{EMNLP}} 2020},
  author = {Su, Dan and Xu, Yan and Yu, Tiezheng and Siddique, Farhad Bin and Barezi, Elham and Fung, Pascale},
  year = {2020},
  month = dec,
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.nlpcovid19-2.14},
  urldate = {2022-08-17},
  abstract = {We present CAiRE-COVID, a real-time question answering (QA) and multi-document summarization system, which won one of the 10 tasks in the Kaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our system aims to tackle the recent challenge of mining the numerous scientific articles being published on COVID-19 by answering high priority questions from the community and summarizing salient question-related information. It combines information extraction with state-of-the-art QA and query-focused multi-document summarization techniques, selecting and highlighting evidence snippets from existing literature given a query. We also propose query-focused abstractive and extractive multi-document summarization methods, to provide more relevant information related to the question. We further conduct quantitative experiments that show consistent improvements on various metrics for each module. We have launched our website CAiRE-COVID for broader use by the medical community, and have open-sourced the code for our system, to bootstrap further study by other researches.}
}

@misc{suImproveQueryFocused2021,
  title = {Improve {{Query Focused Abstractive Summarization}} by {{Incorporating Answer Relevance}}},
  author = {Su, Dan and Yu, Tiezheng and Fung, Pascale},
  year = {2021},
  month = may,
  number = {arXiv:2105.12969},
  eprint = {2105.12969},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2105.12969},
  urldate = {2022-09-06},
  abstract = {Query focused summarization (QFS) models aim to generate summaries from source documents that can answer the given query. Most previous work on QFS only considers the query relevance criterion when producing the summary. However, studying the effect of answer relevance in the summary generating process is also important. In this paper, we propose QFS-BART, a model that incorporates the explicit answer relevance of the source documents given the query via a question answering model, to generate coherent and answer-related summaries. Furthermore, our model can take advantage of large pre-trained models which improve the summarization performance significantly. Empirical results on the Debatepedia dataset show that the proposed model achieves the new state-of-the-art performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{suInfoEntropyLossMitigate2023,
  title = {{{InfoEntropy Loss}} to {{Mitigate Bias}} of {{Learning Difficulties}} for {{Generative Language Models}}},
  author = {Su, Zhenpeng and Wu, Xing and Bai, Xue and Lin, Zijia and Chen, Hui and Ding, Guiguang and Zhou, Wei and Hu, Songlin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.19531},
  eprint = {2310.19531},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.19531},
  urldate = {2023-10-31},
  abstract = {Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose an Information Entropy Loss (InfoEntropy Loss) function. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 436M, 1.1B, and 6.7B parameters. Experiments reveal that models incorporating the proposed InfoEntropy Loss can gain consistent performance improvement on downstream benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{suRoFormerEnhancedTransformer2023,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.09864},
  urldate = {2024-02-14},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{suRoFormerEnhancedTransformer2023a,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.09864},
  urldate = {2024-03-07},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{sushilComparativeStudyZeroshot2024,
  title = {A Comparative Study of Zero-Shot Inference with Large Language Models and Supervised Modeling in Breast Cancer Pathology Classification},
  author = {Sushil, Madhumita and Zack, Travis and Mandair, Divneet and Zheng, Zhiwei and Wali, Ahmed and Yu, Yan-Ning and Quan, Yuwei and Butte, Atul J.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13887},
  eprint = {2401.13887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.13887},
  urldate = {2024-01-29},
  abstract = {Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the differences were more prominent. Frequent sources of GPT-4 errors included inferences from multiple samples and complex task design. On complex tasks where large annotated datasets cannot be easily collected, LLMs can reduce the burden of large-scale data labeling. However, if the use of LLMs is prohibitive, the use of simpler supervised models with large annotated datasets can provide comparable results. LLMs demonstrated the potential to speed up the execution of clinical NLP studies by reducing the need for curating large annotated datasets. This may result in an increase in the utilization of NLP-based variables and outcomes in observational clinical studies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{swayamdiptaDatasetCartographyMapping2020,
  title = {Dataset {{Cartography}}: {{Mapping}} and {{Diagnosing Datasets}} with {{Training Dynamics}}},
  shorttitle = {Dataset {{Cartography}}},
  author = {Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A. and Choi, Yejin},
  year = {2020},
  month = oct,
  number = {arXiv:2009.10795},
  eprint = {2009.10795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.10795},
  urldate = {2023-12-01},
  abstract = {Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{szafraniecCodeTranslationCompiler2023,
  title = {Code {{Translation}} with {{Compiler Representations}}},
  author = {Szafraniec, Marc and Roziere, Baptiste and Leather, Hugh and Charton, Francois and Labatut, Patrick and Synnaeve, Gabriel},
  year = {2023},
  month = apr,
  number = {arXiv:2207.03578},
  eprint = {2207.03578},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.03578},
  urldate = {2024-02-20},
  abstract = {In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11\% on average, and up to 79\% for the Java -{$>$} Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages}
}

@inproceedings{takatsuPersonalizedExtractiveSummarization2021,
  title = {Personalized {{Extractive Summarization}} for a {{News Dialogue System}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Takatsu, Hiroaki and Okuda, Mayu and Matsuyama, Yoichi and Honda, Hiroshi and Fujie, Shinya and Kobayashi, Tetsunori},
  year = {2021},
  month = jan,
  pages = {1044--1051},
  doi = {10.1109/SLT48900.2021.9383568},
  abstract = {In modern society, people's interests and preferences are diversifying. Along with this, the demand for personalized summarization technology is increasing. In this study, we propose a method for generating summaries tailored to each user's interests using profile features obtained from questionnaires administered to users of our spoken-dialogue news delivery system. We propose a method that collects and uses the obtained user profile features to generate a summary tailored to each user's interests, specifically, the sentence features obtained by BERT and user profile features obtained from the questionnaire result. In addition, we propose a method for extracting sentences by solving an integer linear programming problem that considers redundancy and context coherence, using the degree of interest in sentences estimated by the model. The results of our experiments confirmed that summaries generated based on the degree of interest in sentences estimated using user profile information can transmit information more efficiently than summaries based solely on the importance of sentences.},
  keywords = {automatic text summarization,Conferences,Context modeling,Data mining,Estimation,Feature extraction,Integer linear programming,personalization,Redundancy,spoken dialogue system}
}

@article{Talk2CareFacilitatingAsynchronous2024,
  title = {{{Talk2Care}}: {{Facilitating Asynchronous Patient-Provider Communication}} with {{Large-Language-Model}}},
  year = {2024},
  langid = {english}
}

@article{tangRapidlyBootstrappingQuestion2020,
  title = {Rapidly {{Bootstrapping}} a {{Question Answering Dataset}} for {{COVID-19}}},
  author = {Tang, Raphael and Nogueira, Rodrigo and Zhang, Edwin and Gupta, Nikhil and Cam, Phuong and Cho, Kyunghyun and Lin, Jimmy},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.11339 [cs]},
  eprint = {2004.11339},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.11339},
  urldate = {2021-12-25},
  abstract = {We present CovidQA, the beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is the first publicly available resource of its type, and intended as a stopgap measure for guiding research until more substantial evaluation resources become available. While this dataset, comprising 124 question-article pairs as of the present version 0.1 release, does not have sufficient examples for supervised machine learning, we believe that it can be helpful for evaluating the zero-shot or transfer capabilities of existing models on topics specifically related to COVID-19. This paper describes our methodology for constructing the dataset and presents the effectiveness of a number of baselines, including term-based techniques and various transformer-based models. The dataset is available at http://covidqa.ai/},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {00047}
}

@article{tianImprovingBiomedicalNamed2020,
  title = {Improving Biomedical Named Entity Recognition with Syntactic Information},
  author = {Tian, Yuanhe and Shen, Wang and Song, Yan and Xia, Fei and He, Min and Li, Kenli},
  year = {2020},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {539},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03834-6},
  urldate = {2022-04-13},
  abstract = {Background:{\enspace} Biomedical named entity recognition (BioNER) is an important task for understanding biomedical texts, which can be challenging due to the lack of large-scale labeled training data and domain knowledge. To address the challenge, in addition to using powerful encoders (e.g., biLSTM and BioBERT), one possible method is to leverage extra knowledge that is easy to obtain. Previous studies have shown that auto-processed syntactic information can be a useful resource to improve model performance, but their approaches are limited to directly concatenating the embeddings of syntactic information to the input word embeddings. Therefore, such syntactic information is leveraged in an inflexible way, where inaccurate one may hurt model performance. Results:{\enspace} In this paper, we propose BioKMNER, a BioNER model for biomedical texts with key-value memory networks (KVMN) to incorporate auto-processed syntactic information. We evaluate BioKMNER on six English biomedical datasets, where our method with KVMN outperforms the strong baseline method, namely, BioBERT, from the previous study on all datasets. Specifically, the F1 scores of our best performing model are 85.29\% on BC2GM, 77.83\% on JNLPBA, 94.22\% on BC5CDR-chemical, 90.08\% on NCBI-disease, 89.24\% on LINNAEUS, and 76.33\% on Species-800, where state-of-theart performance is obtained on four of them (i.e., BC2GM, BC5CDR-chemical, NCBIdisease, and Species-800). Conclusion:{\enspace} The experimental results on six English benchmark datasets demonstrate that auto-processed syntactic information can be a useful resource for BioNER and our method with KVMN can appropriately leverage such information to improve model performance.},
  langid = {english}
}

@inproceedings{TimeLLMTimeSeries2023,
  title = {Time-{{LLM}}: {{Time Series Forecasting}} by {{Reprogramming Large Language Models}}},
  shorttitle = {Time-{{LLM}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=Unb5CVPtae},
  urldate = {2024-01-16},
  abstract = {Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that {\textbackslash}method is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.},
  langid = {english}
}

@misc{tornbergChatGPT4OutperformsExperts2023,
  title = {{{ChatGPT-4 Outperforms Experts}} and {{Crowd Workers}} in {{Annotating Political Twitter Messages}} with {{Zero-Shot Learning}}},
  author = {T{\"o}rnberg, Petter},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06588},
  eprint = {2304.06588},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06588},
  urldate = {2023-10-26},
  abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Social and Information Networks}
}

@article{toshevskaReviewTextStyle2022,
  title = {A {{Review}} of {{Text Style Transfer}} Using {{Deep Learning}}},
  author = {Toshevska, Martina and Gievska, Sonja},
  year = {2022},
  month = oct,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {3},
  number = {5},
  eprint = {2109.15144},
  primaryclass = {cs},
  pages = {669--684},
  issn = {2691-4581},
  doi = {10.1109/TAI.2021.3115992},
  urldate = {2023-01-17},
  abstract = {Style is an integral component of a sentence indicated by the choice of words a person makes. Different people have different ways of expressing themselves, however, they adjust their speaking and writing style to a social context, an audience, an interlocutor or the formality of an occasion. Text style transfer is defined as a task of adapting and/or changing the stylistic manner in which a sentence is written, while preserving the meaning of the original sentence. A systematic review of text style transfer methodologies using deep learning is presented in this paper. We point out the technological advances in deep neural networks that have been the driving force behind current successes in the fields of natural language understanding and generation. The review is structured around two key stages in the text style transfer process, namely, representation learning and sentence generation in a new style. The discussion highlights the commonalities and differences between proposed solutions as well as challenges and opportunities that are expected to direct and foster further research in the field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-03-14},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.09288},
  urldate = {2023-10-16},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{tranBioInstructInstructionTuning2023,
  title = {{{BioInstruct}}: {{Instruction Tuning}} of {{Large Language Models}} for {{Biomedical Natural Language Processing}}},
  shorttitle = {{{BioInstruct}}},
  author = {Tran, Hieu and Yang, Zhichao and Yao, Zonghai and Yu, Hong},
  year = {2023},
  month = nov,
  number = {arXiv:2310.19975},
  eprint = {2310.19975},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.19975},
  urldate = {2024-01-20},
  abstract = {To enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multi-task learning principles. We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 \& 2, 7B \& 13B version). The instructions were created by prompting the GPT-4 language model with three-seed samples randomly drawn from an 80 human curated instructions. We employed Low-Rank Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three major categories: question answering(QA), information extraction(IE), and text generation(GEN). We also examined whether categories(e.g., QA, IE, and generation) of instructions impact model performance. Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3\% in QA, 5.7\% in IE, and 96\% in Generation tasks. Our 7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with vast domain-specific data or a variety of tasks. Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks. Our findings align with the observations of multi-task learning, suggesting the synergies between two tasks. The BioInstruct dataset serves as a valuable resource and instruction tuned LLMs lead to the best performing BioNLP applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{triantafillouMetaDatasetDatasetDatasets2020,
  title = {Meta-{{Dataset}}: {{A Dataset}} of {{Datasets}} for {{Learning}} to {{Learn}} from {{Few Examples}}},
  shorttitle = {Meta-{{Dataset}}},
  author = {Triantafillou, Eleni and Zhu, Tyler and Dumoulin, Vincent and Lamblin, Pascal and Evci, Utku and Xu, Kelvin and Goroshin, Ross and Gelada, Carles and Swersky, Kevin and Manzagol, Pierre-Antoine and Larochelle, Hugo},
  year = {2020},
  month = apr,
  number = {arXiv:1903.03096},
  eprint = {1903.03096},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.03096},
  urldate = {2022-06-29},
  abstract = {Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{trischlerNewsQAMachineComprehension2017,
  title = {{{NewsQA}}: {{A Machine Comprehension Dataset}}},
  shorttitle = {{{NewsQA}}},
  author = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
  year = {2017},
  month = feb,
  journal = {arXiv:1611.09830 [cs]},
  eprint = {1611.09830},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1611.09830},
  urldate = {2021-12-25},
  abstract = {We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {00520}
}

@article{tsatsaronisOverviewBIOASQLargescale2015,
  title = {An Overview of the {{BIOASQ}} Large-Scale Biomedical Semantic Indexing and Question Answering Competition},
  author = {Tsatsaronis, George and Balikas, Georgios and Malakasiotis, Prodromos and Partalas, Ioannis and Zschunke, Matthias and Alvers, Michael R. and Weissenborn, Dirk and Krithara, Anastasia and Petridis, Sergios and Polychronopoulos, Dimitris and Almirantis, Yannis and Pavlopoulos, John and Baskiotis, Nicolas and Gallinari, Patrick and Arti{\'e}res, Thierry and Ngomo, Axel-Cyrille Ngonga and Heino, Norman and Gaussier, Eric and {Barrio-Alvers}, Liliana and Schroeder, Michael and Androutsopoulos, Ion and Paliouras, Georgios},
  year = {2015},
  month = apr,
  journal = {BMC Bioinformatics},
  volume = {16},
  number = {1},
  pages = {138},
  issn = {1471-2105},
  doi = {10.1186/s12859-015-0564-6},
  urldate = {2021-12-25},
  abstract = {This article provides an overview of the first BioASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March and September 2013. BioASQ assesses the ability of systems to semantically index very large numbers of biomedical scientific articles, and to return concise and user-understandable answers to given natural language questions by combining information from biomedical articles and ontologies.},
  langid = {english},
  keywords = {BioASQ Competition,Hierarchical Text Classification,Information retrieval,Multi-document text summarization,Passage retrieval,Question answering,Semantic indexing},
  annotation = {00354}
}

@article{turingCOMPUTINGMACHINERYINTELLIGENCE1950a,
  title = {I.---{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind, New Series},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {1460-2113, 0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  abstract = {I propose to consider the question, ``Can machines think?''{$\clubsuit$} This should begin with definitions of the meaning of the terms ``machine'' and ``think''. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words ``machine'' and ``think'' are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, ``Can machines think?'' is to be sought in a statistical survey such as a Gallup poll.},
  langid = {english}
}

@misc{ulcarTrainingDatasetDictionary2021,
  title = {Training Dataset and Dictionary Sizes Matter in {{BERT}} Models: The Case of {{Baltic}} Languages},
  shorttitle = {Training Dataset and Dictionary Sizes Matter in {{BERT}} Models},
  author = {Ul{\v c}ar, Matej and {Robnik-{\v S}ikonja}, Marko},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10553},
  eprint = {2112.10553},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10553},
  urldate = {2022-08-09},
  abstract = {Large pretrained masked language models have become state-of-the-art solutions for many NLP problems. While studies have shown that monolingual models produce better results than multilingual models, the training datasets must be sufficiently large. We trained a trilingual LitLat BERT-like model for Lithuanian, Latvian, and English, and a monolingual Est-RoBERTa model for Estonian. We evaluate their performance on four downstream tasks: named entity recognition, dependency parsing, part-of-speech tagging, and word analogy. To analyze the importance of focusing on a single language and the importance of a large training set, we compare created models with existing monolingual and multilingual BERT models for Estonian, Latvian, and Lithuanian. The results show that the newly created LitLat BERT and Est-RoBERTa models improve the results of existing models on all tested tasks in most situations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{vandewaterAnotherICUBenchmark2023,
  title = {Yet {{Another ICU Benchmark}}: {{A Flexible Multi-Center Framework}} for {{Clinical ML}}},
  shorttitle = {Yet {{Another ICU Benchmark}}},
  author = {{van de Water}, Robin and Schmidt, Hendrik and Elbers, Paul and Thoral, Patrick and Arnrich, Bert and Rockenschaub, Patrick},
  year = {2023},
  month = aug,
  number = {arXiv:2306.05109},
  eprint = {2306.05109},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.05109},
  urldate = {2024-01-14},
  abstract = {Medical applications of machine learning (ML) have experienced a surge in popularity in recent years. The intensive care unit (ICU) is a natural habitat for ML given the abundance of available data from electronic health records. Models have been proposed to address numerous ICU prediction tasks like the early detection of complications. While authors frequently report state-of-the-art performance, it is challenging to verify claims of superiority. Datasets and code are not always published, and cohort definitions, preprocessing pipelines, and training setups are difficult to reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular framework that allows researchers to define reproducible and comparable clinical ML experiments; we offer an end-to-end solution from cohort definition to model evaluation. The framework natively supports most open-access ICU datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future ICU datasets. Combined with a transparent preprocessing pipeline and extensible training code for multiple ML and deep learning models, YAIB enables unified model development. Our benchmark comes with five predefined established prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and length of stay) developed in collaboration with clinicians. Adding further tasks is straightforward by design. Using YAIB, we demonstrate that the choice of dataset, cohort definition, and preprocessing have a major impact on the prediction performance - often more so than model class - indicating an urgent need for YAIB as a holistic benchmarking tool. We provide our work to the clinical ML community to accelerate method development and enable real-world clinical implementations. Software Repository: https://github.com/rvandewater/YAIB.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2022-01-20},
  annotation = {34483}
}

@misc{vatsSurveyHumanAITeaming2024,
  title = {A {{Survey}} on {{Human-AI Teaming}} with {{Large Pre-Trained Models}}},
  author = {Vats, Vanshika and Nizam, Marzia Binta and Liu, Minghao and Wang, Ziyuan and Ho, Richard and Prasad, Mohnish Sai and Titterton, Vincent and Malreddy, Sai Venkat and Aggarwal, Riya and Xu, Yanwen and Ding, Lei and Mehta, Jay and Grinnell, Nathan and Liu, Li and Zhong, Sijia and Gandamani, Devanathan Nallur and Tang, Xinyi and Ghosalkar, Rohan and Shen, Celeste and Shen, Rachel and Hussain, Nafisa and Ravichandran, Kesav and Davis, James},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04931},
  eprint = {2403.04931},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.04931},
  urldate = {2024-03-13},
  abstract = {In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@inproceedings{vaudauxPretrainedLanguageModels2023,
  title = {Pretrained {{Language Models}} v. {{Court Ruling Predictions}}: {{A Case Study}} on a {{Small Dataset}} of {{French Court}} of {{Appeal Rulings}}},
  shorttitle = {Pretrained {{Language Models}} v. {{Court Ruling Predictions}}},
  booktitle = {Proceedings of the {{Natural Legal Language Processing Workshop}} 2023},
  author = {Vaudaux, Olivia and Bazzoli, Caroline and Coavoux, Maximin and Vial, G{\'e}raldine and Verg{\`e}s, {\'E}tienne},
  editor = {{Preo{\textbackslash}textcommabelowtiuc-Pietro}, Daniel and Goanta, Catalina and Chalkidis, Ilias and Barrett, Leslie and Spanakis, Gerasimos (Jerry) and Aletras, Nikolaos},
  year = {2023},
  month = dec,
  pages = {38--43},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  url = {https://aclanthology.org/2023.nllp-1.5},
  urldate = {2023-12-07},
  abstract = {NLP systems are increasingly used in the law domain, either by legal institutions or by the industry. As a result there is a pressing need to characterize their strengths and weaknesses and understand their inner workings. This article presents a case study on the task of judicial decision prediction, on a small dataset from French Courts of Appeal. Specifically, our dataset of around 1000 decisions is about the habitual place of residency of children from divorced parents. The task consists in predicting, from the facts and reasons of the documents, whether the court rules that children should live with their mother or their father. Instead of feeding the whole document to a classifier, we carefully construct the dataset to make sure that the input to the classifier does not contain any `spoilers' (it is often the case in court rulings that information all along the document mentions the final decision). Our results are mostly negative: even classifiers based on French pretrained language models (Flaubert, JuriBERT) do not classify the decisions with a reasonable accuracy. However, they can extract the decision when it is part of the input. With regards to these results, we argue that there is a strong caveat when constructing legal NLP datasets automatically.}
}

@misc{vikramCanLargeLanguage2023,
  title = {Can {{Large Language Models Write Good Property-Based Tests}}?},
  author = {Vikram, Vasudev and Lemieux, Caroline and Padhye, Rohan},
  year = {2023},
  month = jul,
  number = {arXiv:2307.04346},
  eprint = {2307.04346},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.04346},
  urldate = {2024-01-16},
  abstract = {Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for property-based tests. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we explore the potential of using LLMs to synthesize property-based tests. We call our approach PBT-GPT, and propose three different strategies of prompting the LLM for PBT. We characterize various failure modes of PBT-GPT and detail an evaluation methodology for automatically synthesized property-based tests. PBT-GPT achieves promising results in our preliminary studies on sample Python library APIs in \${\textbackslash}texttt\{numpy\}\$, \${\textbackslash}texttt\{networkx\}\$, and \${\textbackslash}texttt\{datetime\}\$.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Software Engineering}
}

@inproceedings{wangAutomatedConcatenationEmbeddings2021,
  title = {Automated {{Concatenation}} of {{Embeddings}} for {{Structured Prediction}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Xinyu and Jiang, Yong and Bach, Nguyen and Wang, Tao and Huang, Zhongqiang and Huang, Fei and Tu, Kewei},
  year = {2021},
  month = aug,
  pages = {2643--2660},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.206},
  urldate = {2022-07-19},
  abstract = {Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.}
}

@misc{wangBenchmarkingGeneralizationInContext2022,
  title = {Benchmarking {{Generalization}} via {{In-Context Instructions}} on 1,600+ {{Language Tasks}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  year = {2022},
  month = apr,
  number = {arXiv:2204.07705},
  eprint = {2204.07705},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.07705},
  urldate = {2022-06-29},
  abstract = {How can we measure the generalization of models to a variety of unseen tasks when provided with their language instructions? To facilitate progress in this goal, we introduce Natural-Instructions v2, a benchmark of 1,600+ diverse language tasks and their expert-written instructions. It covers 70+ distinct task types, such as tagging, in-filling, and rewriting. These tasks are collected with contributions of NLP practitioners in the community and through an iterative peer review process to ensure their quality. With this large and diverse collection of tasks, we are able to rigorously benchmark cross-task generalization of models -- training on a subset of tasks and evaluating on the remaining unseen ones. For instance, we quantify generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances, and model sizes. Based on these insights, we introduce Tk-Instruct, an encoder-decoder Transformer that is trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) which outperforms existing larger models on our benchmark. We hope this benchmark facilitates future progress toward more general-purpose language understanding models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{wangCalibratingImbalancedClassifiers2022a,
  title = {Calibrating {{Imbalanced Classifiers}} with {{Focal Loss}}: {{An Empirical Study}}},
  shorttitle = {Calibrating {{Imbalanced Classifiers}} with {{Focal Loss}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{Industry Track}}},
  author = {Wang, Cheng and Balazs, Jorge and Szarvas, Gy{\"o}rgy and Ernst, Patrick and Poddar, Lahari and Danchenko, Pavel},
  year = {2022},
  month = dec,
  pages = {145--153},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, UAE},
  url = {https://aclanthology.org/2022.emnlp-industry.14},
  urldate = {2023-06-20},
  abstract = {Imbalanced data distribution is a practical and common challenge in building production-level machine learning (ML) models in industry, where data usually exhibits long-tail distributions. For instance, in virtual AI Assistants, such as Google Assistant, Amazon Alexa and Apple Siri, the ``play music'' or ``set timer'' utterance is exposed to an order of magnitude more traffic than other skills. This can easily cause trained models to overfit to the majority classes, categories or intents, lead to model miscalibration. The uncalibrated models output unreliable (mostly overconfident) predictions, which are at high risk of affecting downstream decision-making systems. In this work, we study the calibration of production models in the industry use-case of predicting product return reason codes in customer service conversations of an online retail store; The returns reasons also exhibit class imbalance.To alleviate the resulting miscalibration in the production ML model, we streamline the model development and deployment using focal loss (CITATION).We empirically show the effectiveness of model training with focal loss in learning better calibrated models, as compared to standard cross-entropy loss. Better calibration, in turn, enables better control of the precision-recall trade-off for the models deployed in production.}
}

@misc{wangCoCoSumContextualCode2021,
  title = {{{CoCoSum}}: {{Contextual Code Summarization}} with {{Multi-Relational Graph Neural Network}}},
  shorttitle = {{{CoCoSum}}},
  author = {Wang, Yanlin and Shi, Ensheng and Du, Lun and Yang, Xiaodi and Hu, Yuxuan and Han, Shi and Zhang, Hongyu and Zhang, Dongmei},
  year = {2021},
  month = jul,
  number = {arXiv:2107.01933},
  eprint = {2107.01933},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.01933},
  urldate = {2022-07-18},
  abstract = {Source code summaries are short natural language descriptions of code snippets that help developers better understand and maintain source code. There has been a surge of work on automatic code summarization to reduce the burden of writing summaries manually. However, most contemporary approaches mainly leverage the information within the boundary of the method being summarized (i.e., local context), and ignore the broader context that could assist with code summarization. This paper explores two global contexts, namely intra-class and inter-class contexts, and proposes the model CoCoSUM: Contextual Code Summarization with Multi-Relational Graph Neural Networks. CoCoSUM first incorporates class names as the intra-class context to generate the class semantic embeddings. Then, relevant Unified Modeling Language (UML) class diagrams are extracted as inter-class context and are encoded into the class relational embeddings using a novel Multi-Relational Graph Neural Network (MRGNN). Class semantic embeddings and class relational embeddings, together with the outputs from code token encoder and AST encoder, are passed to a decoder armed with a two-level attention mechanism to generate high-quality, context-aware code summaries. We conduct extensive experiments to evaluate our approach and compare it with other automatic code summarization models. The experimental results show that CoCoSUM is effective and outperforms state-of-the-art methods. Our source code and experimental data are available in the supplementary materials and will be made publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Software Engineering}
}

@article{wangCoKEContextualizedKnowledge2020,
  title = {{{CoKE}}: {{Contextualized Knowledge Graph Embedding}}},
  shorttitle = {{{CoKE}}},
  author = {Wang, Quan and Huang, Pingping and Wang, Haifeng and Dai, Songtai and Jiang, Wenbin and Liu, Jing and Lyu, Yajuan and Zhu, Yong and Wu, Hua},
  year = {2020},
  month = apr,
  journal = {arXiv:1911.02168 [cs]},
  eprint = {1911.02168},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.02168},
  urldate = {2022-03-17},
  abstract = {Knowledge graph embedding, which projects symbolic entities and relations into continuous vector spaces, is gaining increasing attention. Previous methods allow a single static embedding for each entity or relation, ignoring their intrinsic contextual nature, i.e., entities and relations may appear in different graph contexts, and accordingly, exhibit different properties. This work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm that takes into account such contextual nature, and learns dynamic, flexible, and fully contextualized entity and relation embeddings. Two types of graph contexts are studied: edges and paths, both formulated as sequences of entities and relations. CoKE takes a sequence as input and uses a Transformer encoder to obtain contextualized representations. These representations are hence naturally adaptive to the input, capturing contextual meanings of entities and relations therein. Evaluation on a wide variety of public benchmarks verifies the superiority of CoKE in link prediction and path query answering. It performs consistently better than, or at least equally well as current state-of-the-art in almost every case, in particular offering an absolute improvement of 21.0\% in H@10 on path query answering. Our code is available at {\textbackslash}url\{https://github.com/PaddlePaddle/Research/tree/master/KG/CoKE\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{WangDaShuJuShiDaiDeJiQiXueXiReDianGuoJiJiQiXueXiDaHuiICML2013CanHuiGanXiang2013,
  title = {{ ICML2013}},
  author = {, },
  year = {2013},
  journal = {},
  number = {009},
  pages = {3},
  langid = {chinese},
  annotation = {00000}
}

@article{wangDRGLLaMATuningLLaMA2024,
  title = {{{DRG-LLaMA}} : Tuning {{LLaMA}} Model to Predict Diagnosis-Related Group for Hospitalized Patients},
  shorttitle = {{{DRG-LLaMA}}},
  author = {Wang, Hanyin and Gao, Chufan and Dantona, Christopher and Hull, Bryan and Sun, Jimeng},
  year = {2024},
  month = jan,
  journal = {npj Digital Medicine},
  volume = {7},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00989-3},
  urldate = {2024-01-23},
  abstract = {In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is pivotal, but its assignment process is inefficient. The study introduces DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes to enhance DRGs assignment. Utilizing LLaMA as the foundational model and optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries, our DRG-LLaMA -7B model exhibited a noteworthy macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0\%, and a macro-averaged Area Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This model surpassed the performance of prior leading models in DRG prediction, showing a relative improvement of 40.3\% and 35.7\% in macro-averaged F1 score compared to ClinicalBERT and CAML, respectively. Applied to base DRG and complication or comorbidity (CC)/major complication or comorbidity (MCC) prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8\% and 67.5\%, respectively. Additionally, our findings indicate that DRG-LLaMA 's performance correlates with increased model parameters and input context lengths.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational models,Data processing}
}

@inproceedings{wangExecutionBasedEvaluationOpenDomain2023,
  title = {Execution-{{Based Evaluation}} for {{Open-Domain Code Generation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Wang, Zhiruo and Zhou, Shuyan and Fried, Daniel and Neubig, Graham},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {1271--1290},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.89},
  urldate = {2024-01-11},
  abstract = {To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.}
}

@inproceedings{wangGatedSelfMatchingNetworks2017,
  title = {Gated {{Self-Matching Networks}} for {{Reading Comprehension}} and {{Question Answering}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wang, Wenhui and Yang, Nan and Wei, Furu and Chang, Baobao and Zhou, Ming},
  year = {2017},
  pages = {189--198},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1018},
  urldate = {2022-04-13},
  abstract = {In this paper, we present the gated selfmatching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3\% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9\%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.},
  langid = {english}
}

@article{wangHumanAICollaborationData2019,
  title = {Human-{{AI Collaboration}} in {{Data Science}}: {{Exploring Data Scientists}}' {{Perceptions}} of {{Automated AI}}},
  shorttitle = {Human-{{AI Collaboration}} in {{Data Science}}},
  author = {Wang, Dakuo and Weisz, Justin D. and Muller, Michael and Ram, Parikshit and Geyer, Werner and Dugan, Casey and Tausczik, Yla and Samulowitz, Horst and Gray, Alexander},
  year = {2019},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {3},
  number = {CSCW},
  eprint = {1909.02309},
  primaryclass = {cs},
  pages = {1--24},
  issn = {2573-0142},
  doi = {10.1145/3359313},
  urldate = {2023-09-18},
  abstract = {The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@article{wangKnowledgeGraphEmbedding2014,
  title = {Knowledge {{Graph Embedding}} by {{Translating}} on {{Hyperplanes}}},
  author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  year = {2014},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/8870},
  urldate = {2021-12-07},
  abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {TransH}
}

@inproceedings{wangMediTabScalingMedical2023,
  title = {{{MediTab}}: {{Scaling Medical Tabular Data Predictors}} via {{Data Consolidation}}, {{Enrichment}}, and {{Refinement}}},
  shorttitle = {{{MediTab}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Wang, Zifeng and Gao, Chufan and Xiao, Cao and Sun, Jimeng},
  year = {2023},
  month = oct,
  eprint = {2305.12081},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.12081},
  urldate = {2024-02-01},
  abstract = {Tabular data prediction has been employed in medical applications such as patient health risk prediction. However, existing methods usually revolve around the algorithm design while overlooking the significance of data engineering. Medical tabular datasets frequently exhibit significant heterogeneity across different sources, with limited sample sizes per source. As such, previous predictors are often trained on manually curated small datasets that struggle to generalize across different tabular datasets during inference. This paper proposes to scale medical tabular data predictors (MediTab) to various tabular inputs with varying features. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with distinct schema. It also aligns out-domain data with the target task using a "learn, annotate, and refinement" pipeline. The expanded training data then enables the pre-trained MediTab to infer for arbitrary tabular input in the domain without fine-tuning, resulting in significant improvements over supervised baselines: it reaches an average ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3 trial outcome prediction datasets, respectively. In addition, MediTab exhibits impressive zero-shot performances: it outperforms supervised XGBoost models by 8.9\% and 17.2\% on average in two prediction tasks, respectively. The code is available at https://github.com/RyanWangZf/MediTab.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,LLM_for_clinical_diagnose}
}

@inproceedings{wangMiniLMDeepSelfAttention2020,
  title = {{{MiniLM}}: {{Deep Self-Attention Distillation}} for {{Task-Agnostic Compression}} of {{Pre-Trained Transformers}}},
  shorttitle = {{{MiniLM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  year = {2020},
  volume = {33},
  pages = {5776--5788},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2023-11-14},
  abstract = {Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.}
}

@misc{wangMultimodalRiskPrediction2023,
  title = {Multimodal {{Risk Prediction}} with {{Physiological Signals}}, {{Medical Images}} and {{Clinical Notes}}},
  author = {Wang, Yuanlong and Yin, Changchang and Zhang, Ping},
  year = {2023},
  month = may,
  pages = {2023.05.18.23290207},
  publisher = {medRxiv},
  doi = {10.1101/2023.05.18.23290207},
  urldate = {2024-01-11},
  abstract = {The broad adoption of electronic health records (EHRs) provides great opportunities to conduct healthcare research and solve various clinical problems in medicine. With recent advances and success, methods based on machine learning and deep learning have become increasingly popular in medical informatics. Combining data from multiple modalities may help in predictive tasks. To assess the expectations of multimodal data, we introduce a comprehensive fusion framework designed to integrate temporal variables, medical images, and clinical notes in Electronic Health Record (EHR) for enhanced performance in down-stream predictive tasks. Early, joint, and late fusion strategies were employed to effectively combine data from various modalities. Model performance and contribution scores show that multimodal models outperform uni-modal models in various tasks. Additionally, temporal signs contain more information than CXR images and clinical notes in three explored predictive tasks. Therefore, models integrating different data modalities can work better in predictive tasks.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@misc{wangSelfInstructAligningLanguage2022,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10560},
  urldate = {2023-03-15},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{wangUnderstandingChainofThoughtPrompting2023,
  title = {Towards {{Understanding Chain-of-Thought Prompting}}: {{An Empirical Study}} of {{What Matters}}},
  shorttitle = {Towards {{Understanding Chain-of-Thought Prompting}}},
  author = {Wang, Boshi and Min, Sewon and Deng, Xiang and Shen, Jiaming and Wu, You and Zettlemoyer, Luke and Sun, Huan},
  year = {2023},
  month = jun,
  number = {arXiv:2212.10001},
  eprint = {2212.10001},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10001},
  urldate = {2023-06-10},
  abstract = {Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90\% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{wangUserBehaviorSimulation2024,
  title = {User {{Behavior Simulation}} with {{Large Language Model}} Based {{Agents}}},
  author = {Wang, Lei and Zhang, Jingsen and Yang, Hao and Chen, Zhiyuan and Tang, Jiakai and Zhang, Zeyu and Chen, Xu and Lin, Yankai and Song, Ruihua and Zhao, Wayne Xin and Xu, Jun and Dou, Zhicheng and Wang, Jun and Wen, Ji-Rong},
  year = {2024},
  month = feb,
  number = {arXiv:2306.02552},
  eprint = {2306.02552},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2306.02552},
  urldate = {2024-02-25},
  abstract = {Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval}
}

@article{WangXiaoJieJiQiYueDuLiJieDeYanJiuJinZhan2019,
  title = {{}},
  author = { and {Xiao-jie}, {\relax WANG} and  and {Zi-wei}, B. A. I. and  and Ke, L. I. and  and {Cai-xia}, {\relax YUAN}},
  year = {2019},
  month = dec,
  publisher = {},
  doi = {10.13190/j.jbupt.2019-111},
  urldate = {2021-12-25},
  abstract = {4.--42.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {cn},
  annotation = {00000}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2023-01-19},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{weiFinetunedLanguageModels2021,
  title = {Finetuned {{Language Models}} Are {{Zero-Shot Learners}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2021},
  month = oct,
  url = {https://openreview.net/forum?id=gEZrGCozdqR},
  urldate = {2023-11-14},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning---finetuning language models on a collection of datasets described via instructions---substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@article{weissenbornMakingNeuralQA2017,
  title = {Making {{Neural QA}} as {{Simple}} as {{Possible}} but Not {{Simpler}}},
  author = {Weissenborn, Dirk and Wiese, Georg and Seiffe, Laura},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.04816 [cs]},
  eprint = {1703.04816},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1703.04816},
  urldate = {2021-12-25},
  abstract = {Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  annotation = {00188}
}

@article{westonAICompleteQuestionAnswering2015,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and {van Merri{\"e}nboer}, Bart and Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  month = dec,
  journal = {arXiv:1502.05698 [cs, stat]},
  eprint = {1502.05698},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1502.05698},
  urldate = {2021-12-25},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning},
  annotation = {00961}
}

@article{wilsonAutomatedEssayEvaluation2016,
  title = {Automated Essay Evaluation Software in {{English Language Arts}} Classrooms: {{Effects}} on Teacher Feedback, Student Motivation, and Writing Quality},
  shorttitle = {Automated Essay Evaluation Software in {{English Language Arts}} Classrooms},
  author = {Wilson, Joshua and Czik, Amanda},
  year = {2016},
  month = sep,
  journal = {Computers \& Education},
  volume = {100},
  pages = {94--109},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2016.05.004},
  urldate = {2022-07-13},
  abstract = {Automated Essay Evaluation (AEE) systems are being increasingly adopted in the United States to support writing instruction. AEE systems are expected to assist teachers in providing increased higher-level feedback and expediting the feedback process, while supporting gains in students' writing motivation and writing quality. The current study explored these claims using a quasi-experimental study. Four eighth-grade English Language Arts (ELA) classes were assigned to a combined feedback condition in which they received feedback on their writing from their teacher and from an automated essay evaluation (AEE) system called PEG Writing{\textregistered}. Four other eighth-grade ELA classes were assigned to a teacher feedback-only condition, in which they received feedback from their teacher via GoogleDocs. Results indicated that teachers gave the same median amount feedback to students in both condition, but gave proportionately more feedback on higher-level writing skills to students in the combined PEG~+~Teacher Feedback condition. Teachers also agreed that PEG assisted them in saving one-third to half the time it took to provide feedback when they were the sole source of feedback (i.e., the GoogleDocs condition). At the conclusion of the study, students in the combined feedback condition demonstrated increases in writing persistence, though there were no differences between groups with regard to final-draft writing quality.},
  langid = {english},
  keywords = {Automated essay evaluation,English Language Arts,Interactive learning environments,Writing}
}

@misc{workshopBLOOM176BParameterOpenAccess2022,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c c}ois and Gall{\'e}, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno{\^i}t and Muennighoff, Niklas and {del Moral}, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and {McMillan-Major}, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren{\c c}on, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and {van Strien}, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz{\'a}lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G{\'e}rard and Kruszewski, Germ{\'a}n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and {Gonzalez-Dios}, Itziar and {de la Rosa}, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J{\"o}rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu{\~n}oz, Manuel Romero and Masoud, Maraim and Grandury, Mar{\'i}a and {\v S}a{\v s}ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and {de Gibert}, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L{\'o}pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta{\c s}ar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and {Al-shaibani}, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and {Ben-David}, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and {von Platen}, Patrick and Cornette, Pierre and Lavall{\'e}e, Pierre Fran{\c c}ois and Lacroix, R{\'e}mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St{\'e}phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N{\'e}v{\'e}ol, Aur{\'e}lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and {van der Wal}, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zden{\v e}k and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu{\~n}oz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and {Miranda-Escalada}, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl{\'e}mentine and Peri{\~n}{\'a}n, Daniel Le{\'o}n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and {de Bykhovetz}, Madeleine Hahn and Takeuchi, Maiko and P{\`a}mies, Marc and Castillo, Maria A. and Nezhurina, Marianna and S{\"a}nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and {Sang-aroonsiri}, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th{\'e}o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  year = {2022},
  month = dec,
  number = {arXiv:2211.05100},
  eprint = {2211.05100},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2211.05100},
  urldate = {2023-01-17},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@article{wornowEHRSHOTEHRBenchmark2023,
  title = {{{EHRSHOT}}: {{An EHR Benchmark}} for {{Few-Shot Evaluation}} of {{Foundation Models}}},
  shorttitle = {{{EHRSHOT}}},
  author = {Wornow, Michael and Thapa, Rahul and Steinberg, Ethan and Fries, Jason and Shah, Nigam},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {67125--67137},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/d42db1f74df54cb992b3956eb7f15a6f-Abstract-Datasets_and_Benchmarks.html},
  urldate = {2024-03-13},
  langid = {english},
  keywords = {No DOI found}
}

@article{wuAI4VISSurveyArtificial2022,
  title = {{{AI4VIS}}: {{Survey}} on {{Artificial Intelligence Approaches}} for {{Data Visualization}}},
  shorttitle = {{{AI4VIS}}},
  author = {Wu, Aoyu and Wang, Yun and Shu, Xinhuan and Moritz, Dominik and Cui, Weiwei and Zhang, Haidong and Zhang, Dongmei and Qu, Huamin},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {12},
  pages = {5049--5070},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3099002},
  urldate = {2023-10-12},
  abstract = {Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes. We make the list of surveyed papers and related material available online at.}
}

@article{wuGoogleNeuralMachine2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1609.08144},
  url = {http://arxiv.org/abs/1609.08144},
  urldate = {2022-01-20},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {05182}
}

@misc{wuLanguageModelsPlan2024,
  title = {Do Language Models Plan Ahead for Future Tokens?},
  author = {Wu, Wilson and Morris, John X. and Levine, Lionel},
  year = {2024},
  month = mar,
  number = {arXiv:2404.00859},
  eprint = {2404.00859},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.00859},
  urldate = {2024-04-05},
  abstract = {Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at \$t\$ that is then used in future forward passes \$t+{\textbackslash}tau\$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at \$t\$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step \$t\$ are already the same as those that would most benefit inference at time \$t+{\textbackslash}tau\$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{wuPMCLLaMABuildingOpensource2023,
  title = {{{PMC-LLaMA}}: {{Towards Building Open-source Language Models}} for {{Medicine}}},
  shorttitle = {{{PMC-LLaMA}}},
  author = {Wu, Chaoyi and Lin, Weixiong and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  year = {2023},
  month = aug,
  number = {arXiv:2304.14454},
  eprint = {2304.14454},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.14454},
  urldate = {2024-01-20},
  abstract = {Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{wuPrecedentEnhancedLegalJudgment2023,
  title = {Precedent-{{Enhanced Legal Judgment Prediction}} with {{LLM}} and {{Domain-Model Collaboration}}},
  author = {Wu, Yiquan and Zhou, Siying and Liu, Yifei and Lu, Weiming and Liu, Xiaozhong and Zhang, Yating and Sun, Changlong and Wu, Fei and Kuang, Kun},
  year = {2023},
  month = oct,
  url = {https://www.semanticscholar.org/paper/Precedent-Enhanced-Legal-Judgment-Prediction-with-Wu-Zhou/a80546c9847710af1ba8d5f8dca9386e7a520d0a},
  urldate = {2023-10-16},
  abstract = {Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains.}
}

@misc{wuPrecedentEnhancedLegalJudgment2023a,
  title = {Precedent-{{Enhanced Legal Judgment Prediction}} with {{LLM}} and {{Domain-Model Collaboration}}},
  author = {Wu, Yiquan and Zhou, Siying and Liu, Yifei and Lu, Weiming and Liu, Xiaozhong and Zhang, Yating and Sun, Changlong and Wu, Fei and Kuang, Kun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.09241},
  eprint = {2310.09241},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.09241},
  urldate = {2023-10-16},
  abstract = {Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{xiaoCAIL2018LargeScaleLegal2018,
  title = {{{CAIL2018}}: {{A Large-Scale Legal Dataset}} for {{Judgment Prediction}}},
  shorttitle = {{{CAIL2018}}},
  author = {Xiao, Chaojun and Zhong, Haoxi and Guo, Zhipeng and Tu, Cunchao and Liu, Zhiyuan and Sun, Maosong and Feng, Yansong and Han, Xianpei and Hu, Zhen and Wang, Heng and Xu, Jianfeng},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02478},
  eprint = {1807.02478},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1807.02478},
  urldate = {2023-10-16},
  abstract = {In this paper, we introduce the {\textbackslash}textbf\{C\}hinese {\textbackslash}textbf\{AI\} and {\textbackslash}textbf\{L\}aw challenge dataset (CAIL2018), the first large-scale Chinese legal dataset for judgment prediction. {\textbackslash}dataset contains more than \$2.6\$ million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction. Moreover, the annotations of judgment results are more detailed and rich. It consists of applicable law articles, charges, and prison terms, which are expected to be inferred according to the fact descriptions of cases. For comparison, we implement several conventional text classification baselines for judgment prediction and experimental results show that it is still a challenge for current models to predict the judgment results of legal cases, especially on prison terms. To help the researchers make improvements on legal judgment prediction, both {\textbackslash}dataset and baselines will be released after the CAIL competition{\textbackslash}footnote\{http://cail.cipsc.org.cn/\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{xiaoEfficientStreamingLanguage2023,
  title = {Efficient {{Streaming Language Models}} with {{Attention Sinks}}},
  author = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17453},
  eprint = {2309.17453},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.17453},
  urldate = {2023-10-02},
  abstract = {Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{xiaoFreeALHumanFreeActive2023,
  title = {{{FreeAL}}: {{Towards Human-Free Active Learning}} in the {{Era}} of {{Large Language Models}}},
  shorttitle = {{{FreeAL}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Xiao, Ruixuan and Dong, Yiwen and Zhao, Junbo and Wu, Runze and Lin, Minmin and Chen, Gang and Wang, Haobo},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {14520--14535},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.896},
  urldate = {2024-02-11},
  abstract = {Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision.}
}

@misc{xiaoSmartInmailDemo2020,
  title = {Smart {{Inmail Demo}}},
  author = {Xiao, Charles},
  year = {2020},
  month = dec,
  url = {https://docs.google.com/presentation/d/1IeINOeVhQn1rnoVJ3_IKfUkntdnBgdPQWFxOWL_lxNk},
  urldate = {2022-08-25}
}

@misc{xiaoUncertaintyQuantificationPretrained2022a,
  title = {Uncertainty {{Quantification}} with {{Pre-trained Language Models}}: {{A Large-Scale Empirical Analysis}}},
  shorttitle = {Uncertainty {{Quantification}} with {{Pre-trained Language Models}}},
  author = {Xiao, Yuxin and Liang, Paul Pu and Bhatt, Umang and Neiswanger, Willie and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2022},
  month = oct,
  number = {arXiv:2210.04714},
  eprint = {2210.04714},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.04714},
  urldate = {2023-12-20},
  abstract = {Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{xiaShearedLLaMAAccelerating2023,
  title = {Sheared {{LLaMA}}: {{Accelerating Language Model Pre-training}} via {{Structured Pruning}}},
  shorttitle = {Sheared {{LLaMA}}},
  author = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06694},
  eprint = {2310.06694},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06694},
  urldate = {2023-10-12},
  abstract = {The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{xuBaizeOpenSourceChat2023,
  title = {Baize: {{An Open-Source Chat Model}} with {{Parameter-Efficient Tuning}} on {{Self-Chat Data}}},
  shorttitle = {Baize},
  author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01196},
  eprint = {2304.01196},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.01196},
  urldate = {2023-04-08},
  abstract = {Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. The Baize models and data are released for research purposes only at https://github.com/project-baize/baize. An online demo is also available at https://huggingface.co/spaces/project-baize/baize-lora-7B.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{xuFantasticQuestionsWhere2022,
  title = {Fantastic {{Questions}} and {{Where}} to {{Find Them}}: {{FairytaleQA}} -- {{An Authentic Dataset}} for {{Narrative Comprehension}}},
  shorttitle = {Fantastic {{Questions}} and {{Where}} to {{Find Them}}},
  author = {Xu, Ying and Wang, Dakuo and Yu, Mo and Ritchie, Daniel and Yao, Bingsheng and Wu, Tongshuang and Zhang, Zheng and Li, Toby Jia-Jun and Bradford, Nora and Sun, Branda and Hoang, Tran Bao and Sang, Yisi and Hou, Yufang and Ma, Xiaojuan and Yang, Diyi and Peng, Nanyun and Yu, Zhou and Warschauer, Mark},
  year = {2022},
  month = mar,
  number = {arXiv:2203.13947},
  eprint = {2203.13947},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.13947},
  urldate = {2023-11-08},
  abstract = {Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing on the reading education research, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations. Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models' fine-grained learning skills. Second, the dataset supports question generation (QG) task in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{xuMentalLLMLeveragingLarge2023,
  title = {Mental-{{LLM}}: {{Leveraging Large Language Models}} for {{Mental Health Prediction}} via {{Online Text Data}}},
  shorttitle = {Mental-{{LLM}}},
  author = {Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K. and Wang, Dakuo},
  year = {2023},
  month = sep,
  number = {arXiv:2307.14385},
  eprint = {2307.14385},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2307.14385},
  urldate = {2023-10-12},
  abstract = {Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on various mental health prediction tasks via online text data. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for the mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9\% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8\%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on the mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.},
  archiveprefix = {arxiv},
  keywords = {68U35,Computer Science - Computation and Language,H.5.2,I.2.m}
}

@misc{xuNECENarrativeEvent2023,
  title = {{{NECE}}: {{Narrative Event Chain Extraction Toolkit}}},
  shorttitle = {{{NECE}}},
  author = {Xu, Guangxuan and Isaza, Paulina Toro and Li, Moshi and Oloko, Akintoye and Yao, Bingsheng and Sanctos, Cassia and Adebiyi, Aminat and Hou, Yufang and Peng, Nanyun and Wang, Dakuo},
  year = {2023},
  month = feb,
  number = {arXiv:2208.08063},
  eprint = {2208.08063},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2208.08063},
  urldate = {2023-05-11},
  abstract = {To understand a narrative, it is essential to comprehend its main characters and the associated major events; however, this can be challenging with lengthy and unstructured narrative texts. To address this, we introduce NECE, an open-access, document-level toolkit that automatically extracts and aligns narrative events in the temporal order of their occurrence using sliding window method. Through extensive human evaluations, we have confirmed the high quality of the NECE toolkit, and external validation has demonstrated its potential for application in downstream tasks such as question answering and bias analysis. The NECE toolkit includes both a Python library and a user-friendly web interface; the latter offers custom visualizations of event chains and easy navigation between graphics and text to improve reading efficiency and experience.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval}
}

@misc{xuParameterEfficientFineTuningMethods2023,
  title = {Parameter-{{Efficient Fine-Tuning Methods}} for {{Pretrained Language Models}}: {{A Critical Review}} and {{Assessment}}},
  shorttitle = {Parameter-{{Efficient Fine-Tuning Methods}} for {{Pretrained Language Models}}},
  author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
  year = {2023},
  month = dec,
  number = {arXiv:2312.12148},
  eprint = {2312.12148},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.12148},
  urldate = {2024-02-13},
  abstract = {With the continuous growth in the number of parameters of transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, we conduct experiments using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{xuRAMEHRRetrievalAugmentation2024,
  title = {{{RAM-EHR}}: {{Retrieval Augmentation Meets Clinical Predictions}} on {{Electronic Health Records}}},
  shorttitle = {{{RAM-EHR}}},
  author = {Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Jin, Bowen and Wang, May D. and Ho, Joyce C. and Yang, Carl},
  year = {2024},
  month = feb,
  number = {arXiv:2403.00815},
  eprint = {2403.00815},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.00815},
  urldate = {2024-04-05},
  abstract = {We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4\% gain in AUROC and 7.2\% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at {\textbackslash}url\{https://github.com/ritaranx/RAM-EHR\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Quantitative Biology - Other Quantitative Biology}
}

@inproceedings{xuTransformerReasoningNetwork2021,
  title = {Transformer {{Reasoning Network}} for {{Personalized Review Summarization}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Xu, Hongyan and Liu, Hongtao and Jiao, Pengfei and Wang, Wenjun},
  year = {2021},
  month = jul,
  series = {{{SIGIR}} '21},
  pages = {1452--1461},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3404835.3462854},
  urldate = {2022-07-24},
  abstract = {Review summarization aims to generate condensed text for online product reviews, and has attracted more and more attention in E-commerce platforms. In addition to the input review, the quality of generated summaries is highly related to the characteristics of users and products, e.g., their historical summaries, which could provide useful clues for the target summary generation. However, most previous works ignore the underlying interaction between the given input review and the corresponding historical summaries. Therefore, we aim to explore how to effectively incorporate the history information into the summary generation. In this paper, we propose a novel transformer-based reasoning framework for personalized review summarization. We design an elaborately adapted transformer network containing an encoder and a decoder, to fully infer the important and informative parts among the historical summaries in terms of the input review to generate more comprehensive summaries. In the encoder of our approach, we develop an inter- and intra-attention to involve the history information selectively to learn the personalized representation of the input review. In the decoder part, we propose to incorporate the constructed reasoning memory learning from historical summaries into the original transformer decoder, and design a memory-decoder attention module to retrieve more useful information for the final summary generation. Extensive experiments are conducted and the results show our approach could generate more reasonable summaries for recommendation, and outperform many competitive baseline methods.},
  isbn = {978-1-4503-8037-9},
  keywords = {personalized review summarization,reasoning network,recommender system,transformer}
}

@misc{yamadaLUKEDeepContextualized2020,
  title = {{{LUKE}}: {{Deep Contextualized Entity Representations}} with {{Entity-aware Self-attention}}},
  shorttitle = {{{LUKE}}},
  author = {Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Matsumoto, Yuji},
  year = {2020},
  month = oct,
  number = {arXiv:2010.01057},
  eprint = {2010.01057},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.01057},
  urldate = {2022-06-29},
  abstract = {Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{yangBERTMeetsChinese2019,
  title = {{{BERT Meets Chinese Word Segmentation}}},
  author = {Yang, Haiqin},
  year = {2019},
  month = sep,
  number = {arXiv:1909.09292},
  eprint = {1909.09292},
  primaryclass = {cs, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1909.09292},
  urldate = {2022-06-09},
  abstract = {Chinese word segmentation (CWS) is a fundamental task for Chinese language understanding. Recently, neural network-based models have attained superior performance in solving the in-domain CWS task. Last year, Bidirectional Encoder Representation from Transformers (BERT), a new language representation model, has been proposed as a backbone model for many natural language tasks and redefined the corresponding performance. The excellent performance of BERT motivates us to apply it to solve the CWS task. By conducting intensive experiments in the benchmark datasets from the second International Chinese Word Segmentation Bake-off, we obtain several keen observations. BERT can slightly improve the performance even when the datasets contain the issue of labeling inconsistency. When applying sufficiently learned features, Softmax, a simpler classifier, can attain the same performance as that of a more complicated classifier, e.g., Conditional Random Field (CRF). The performance of BERT usually increases as the model size increases. The features extracted by BERT can be also applied as good candidates for other neural network models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{yangEmpiricalStudyGPT32022,
  title = {An {{Empirical Study}} of {{GPT-3}} for {{Few-Shot Knowledge-Based VQA}}},
  author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  year = {2022},
  month = sep,
  number = {arXiv:2109.05014},
  eprint = {2109.05014},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2109.05014},
  urldate = {2023-06-10},
  abstract = {Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3's power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{yangLegalNLINaturalLanguage2022,
  title = {{{LegalNLI}}: Natural Language Inference for Legal Compliance Inspection},
  shorttitle = {{{LegalNLI}}},
  booktitle = {International {{Conference}} on {{Advanced Algorithms}} and {{Neural Networks}} ({{AANN}} 2022)},
  author = {Yang, Zhanye},
  year = {2022},
  month = jun,
  volume = {12285},
  pages = {144--150},
  publisher = {SPIE},
  doi = {10.1117/12.2637107},
  urldate = {2023-10-12},
  abstract = {Legal compliance inspection (LCI) plays an essential role in the judicial applications of laws and rules but has been neglected in the development of legal artificial intelligence (LegalAI). Currently, few methods in LegalAI can be utilized to solve this task. In this work, we propose to use natural language inference (NLI) framework to solve the LCI problems based on a fundamental fact that the legal judgment should be subject to judicial syllogisms. Specifically, we present LegalNLI -- a specially constructed dataset reformatted from the Chinese legal datasets for other problems. The proposed LegalNLI is a document-level NLI dataset in the legal domain whose premises and hypotheses vary from hundreds to thousands of words in length. In addition, there are few artifacts in LegalNLI that are some clues so as to possibly identify the label by looking only at the hypothesis without observing the premise. Therefore, it is more effective to solve the LCI task by adopting the NLI framework instead of direct text classification methods. Finally, we provide some experiments for evaluating some existing state-of-the-art systems of sentence-level NLI task on the LegalNLI dataset and find it is challenging.}
}

@inproceedings{yangModelingTwoWaySelection2022,
  title = {Modeling {{Two-Way Selection Preference}} for {{Person-Job Fit}}},
  booktitle = {Sixteenth {{ACM Conference}} on {{Recommender Systems}}},
  author = {Yang, Chen and Hou, Yupeng and Song, Yang and Zhang, Tao and Wen, Ji-Rong and Zhao, Wayne Xin},
  year = {2022},
  month = sep,
  eprint = {2208.08612},
  primaryclass = {cs},
  pages = {102--112},
  doi = {10.1145/3523227.3546752},
  urldate = {2023-01-28},
  abstract = {Person-job fit is the core technique of online recruitment platforms, which can improve the efficiency of recruitment by accurately matching the job positions with the job seekers. Existing works mainly focus on modeling the unidirectional process or overall matching. However, recruitment is a two-way selection process, which means that both candidate and employer involved in the interaction should meet the expectation of each other, instead of unilateral satisfaction. In this paper, we propose a dual-perspective graph representation learning approach to model directed interactions between candidates and jobs. To model the two-way selection preference from the dual-perspective of job seekers and employers, we incorporate two different nodes for each candidate (or job) and characterize both successful matching and failed matching via a unified dual-perspective interaction graph. To learn dual-perspective node representations effectively, we design an effective optimization algorithm, which involves a quadruple-based loss and a dual-perspective contrastive learning loss. Extensive experiments on three large real-world recruitment datasets have shown the effectiveness of our approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval}
}

@techreport{yangPerformanceMultimodalGPT4V2023,
  type = {Preprint},
  title = {Performance of {{Multimodal GPT-4V}} on {{USMLE}} with {{Image}}: {{Potential}} for {{Imaging Diagnostic Support}} with {{Explanations}}},
  shorttitle = {Performance of {{Multimodal GPT-4V}} on {{USMLE}} with {{Image}}},
  author = {Yang, Zhichao and Yao, Zonghai and Tasmin, Mahbuba and Vashisht, Parth and Jang, Won Seok and Wang, Beining and Berlowitz, Dan and Yu, Hong},
  year = {2023},
  month = oct,
  institution = {{Radiology and Imaging}},
  doi = {10.1101/2023.10.26.23297629},
  urldate = {2023-10-30},
  abstract = {Objective This study aimed to evaluate the performance of GPT-4V on medical licensing examination questions with images, as well as to analyze interpretability. Design, Setting, and Participants We used 3 sets of multiple-choice questions with images to evaluate GPT-4V's performance. The first set was the United States Medical Licensing Examination (USMLE) from the National Board of Medical Examiners (NBME) sample questions in step1, step2CK, and step3. The second set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The third set was the Diagnostic Radiology Qualifying Core Exam (DRQCE) from the American Board of Radiology. The study (including data analysis) was conducted from September to October 2023. Main Outcomes and Measures The choice accuracy of GPT-4V was compared to two other large language models, GPT-4 and ChatGPT. The GPT-4V explanation was evaluated across 4 qualitative metrics: image misunderstanding, text hallucination, reasoning error, and non-medical error. Results Of the 3 exams with images, NBME, AMBOSS, and DRQCE, GPT-4V achieved accuracies of 86.2\%, 62.0\%, and 73.1\%, respectively. GPT-4V outperformed ChatGPT and GPT-4 by 131.8\% and 64.5\% on average across various data sets. The model demonstrated a decreasing trend in performance as question difficulty increased in the AMBOSS dataset. GPT-4V achieves an accuracy of 90.7\% in the full USMLE exam, outperforming the passing threshold of about 60\% accuracy. Among the incorrect answers, 75.9\% of responses included misinterpretation of the image. However, 39.0\% of them could be easily solved with a short hint. Conclusion In this cross-sectional study, GPT-4V achieved a high accuracy of USMLE that was in the 70th 80th percentile with AMBOSS users preparing for the exam. The results suggest the potential of},
  langid = {english}
}

@misc{yangQuokkaOpensourceLarge2024,
  title = {Quokka: {{An Open-source Large Language Model ChatBot}} for {{Material Science}}},
  shorttitle = {Quokka},
  author = {Yang, Xianjun and Wilson, Stephen D. and Petzold, Linda},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01089},
  eprint = {2401.01089},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.01089},
  urldate = {2024-01-20},
  abstract = {This paper presents the development of a specialized chatbot for materials science, leveraging the Llama-2 language model, and continuing pre-training on the expansive research articles in the materials science domain from the S2ORC dataset. The methodology involves an initial pretraining phase on over one million domain-specific papers, followed by an instruction-tuning process to refine the chatbot's capabilities. The chatbot is designed to assist researchers, educators, and students by providing instant, context-aware responses to queries in the field of materials science. We make the four trained checkpoints (7B, 13B, with or without chat ability) freely available to the research community at https://github.com/Xianjun-Yang/Quokka.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computational Engineering Finance and Science}
}

@inproceedings{yaoItAITurn2022,
  title = {It Is {{AI}}'s {{Turn}} to {{Ask Humans}} a {{Question}}: {{Question-Answer Pair Generation}} for {{Children}}'s {{Story Books}}},
  shorttitle = {It Is {{AI}}'s {{Turn}} to {{Ask Humans}} a {{Question}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yao, Bingsheng and Wang, Dakuo and Wu, Tongshuang and Zhang, Zheng and Li, Toby and Yu, Mo and Xu, Ying},
  year = {2022},
  month = may,
  pages = {731--744},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.54},
  urldate = {2023-01-18},
  abstract = {Existing question answering (QA) techniques are created mainly to answer questions asked by humans. But in educational applications, teachers often need to decide what questions they should ask, in order to help students to improve their narrative understanding capabilities. We design an automated question-answer generation (QAG) system for this education scenario: given a story book at the kindergarten to eighth-grade level as input, our system can automatically generate QA pairs that are capable of testing a variety of dimensions of a student's comprehension skills. Our proposed QAG model architecture is demonstrated using a new expert-annotated FairytaleQA dataset, which has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and human evaluations show that our model outperforms state-of-the-art QAG baseline systems. On top of our QAG system, we also start to build an interactive story-telling application for the future real-world deployment in this educational scenario.}
}

@misc{yaoLabelsEmpoweringHuman2023,
  title = {Beyond {{Labels}}: {{Empowering Human Annotators}} with {{Natural Language Explanations}} through a {{Novel Active-Learning Architecture}}},
  shorttitle = {Beyond {{Labels}}},
  author = {Yao, Bingsheng and Jindal, Ishan and Popa, Lucian and Katsis, Yannis and Ghosh, Sayan and He, Lihong and Lu, Yuxuan and Srivastava, Shashank and Li, Yunyao and Hendler, James and Wang, Dakuo},
  year = {2023},
  month = oct,
  number = {arXiv:2305.12710},
  eprint = {2305.12710},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.12710},
  urldate = {2023-11-13},
  abstract = {Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Additional ablation studies illustrate the potential of our AL architecture for transfer learning, generalizability, and integration with large language models (LLMs). While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{yaoLabelsEmpoweringHuman2023a,
  title = {Beyond {{Labels}}: {{Empowering Human Annotators}} with {{Natural Language Explanations}} through a {{Novel Active-Learning Architecture}}},
  shorttitle = {Beyond {{Labels}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Yao, Bingsheng and Jindal, Ishan and Popa, Lucian and Katsis, Yannis and Ghosh, Sayan and He, Lihong and Lu, Yuxuan and Srivastava, Shashank and Li, Yunyao and Hendler, James and Wang, Dakuo},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {11629--11643},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.778},
  urldate = {2024-03-29},
  abstract = {Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Additional ablation studies illustrate the potential of our AL architecture for transfer learning, generalizability, and integration with large language models (LLMs). While LLMs exhibit exceptional explanation-generation capabilities for relatively simple tasks, their effectiveness in complex real-world tasks warrants further in-depth study.}
}

@misc{yasunagaLinkBERTPretrainingLanguage2022,
  title = {{{LinkBERT}}: {{Pretraining Language Models}} with {{Document Links}}},
  shorttitle = {{{LinkBERT}}},
  author = {Yasunaga, Michihiro and Leskovec, Jure and Liang, Percy},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15827},
  eprint = {2203.15827},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.15827},
  urldate = {2023-10-16},
  abstract = {Language model (LM) pretraining can learn various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5\% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7\% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data at https://github.com/michiyasunaga/LinkBERT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{yeSimpleEffectivePluggable2022,
  title = {A {{Simple}} but {{Effective Pluggable Entity Lookup Table}} for {{Pre-trained Language Models}}},
  author = {Ye, Deming and Lin, Yankai and Li, Peng and Sun, Maosong and Liu, Zhiyuan},
  year = {2022},
  month = may,
  number = {arXiv:2202.13392},
  eprint = {2202.13392},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.13392},
  urldate = {2022-06-21},
  abstract = {Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity's output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2\%-5\% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{yildirimMultimodalHealthcareAI2024,
  title = {Multimodal {{Healthcare AI}}: {{Identifying}} and {{Designing Clinically Relevant Vision-Language Applications}} for {{Radiology}}},
  shorttitle = {Multimodal {{Healthcare AI}}},
  author = {Yildirim, Nur and Richardson, Hannah and Wetscherek, Maria T. and Bajwa, Junaid and Jacob, Joseph and Pinnock, Mark A. and Harris, Stephen and {de Castro}, Daniel Coelho and Bannur, Shruthi and Hyland, Stephanie L. and Ghosh, Pratik and Ranjit, Mercy and Bouzid, Kenza and Schwaighofer, Anton and {P{\'e}rez-Garc{\'i}a}, Fernando and Sharma, Harshita and Oktay, Ozan and Lungren, Matthew and {Alvarez-Valle}, Javier and Nori, Aditya and Thieme, Anja},
  year = {2024},
  month = feb,
  eprint = {2402.14252},
  primaryclass = {cs},
  doi = {10.1145/3613904.3642013},
  urldate = {2024-02-28},
  abstract = {Recent advances in AI combine large language models (LLMs) with vision encoders that bring forward unprecedented technical capabilities to leverage for a wide range of healthcare applications. Focusing on the domain of radiology, vision-language models (VLMs) achieve good performance results for tasks such as generating radiology findings based on a patient's medical image, or answering visual questions (e.g., 'Where are the nodules in this chest X-ray?'). However, the clinical utility of potential applications of these capabilities is currently underexplored. We engaged in an iterative, multidisciplinary design process to envision clinically relevant VLM interactions, and co-designed four VLM use concepts: Draft Report Generation, Augmented Report Review, Visual Search and Querying, and Patient Imaging History Highlights. We studied these concepts with 13 radiologists and clinicians who assessed the VLM concepts as valuable, yet articulated many design considerations. Reflecting on our findings, we discuss implications for integrating VLM capabilities in radiology, and for healthcare AI more generally.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@article{yoonSequenceTaggingBiomedical2021,
  title = {Sequence {{Tagging}} for {{Biomedical Extractive Question Answering}}},
  author = {Yoon, Wonjin and Jackson, Richard and Kang, Jaewoo and Lagerberg, Aron},
  year = {2021},
  journal = {arXiv preprint arXiv:2104.07535},
  eprint = {2104.07535},
  archiveprefix = {arxiv}
}

@misc{yuanBatchEvalHumanlikeText2023,
  title = {{{BatchEval}}: {{Towards Human-like Text Evaluation}}},
  shorttitle = {{{BatchEval}}},
  author = {Yuan, Peiwen and Feng, Shaoxiong and Li, Yiwei and Wang, Xinglin and Pan, Boyuan and Wang, Heda and Li, Kan},
  year = {2023},
  month = dec,
  number = {arXiv:2401.00437},
  eprint = {2401.00437},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.00437},
  urldate = {2024-01-02},
  abstract = {Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5\% on Pearson correlations with only 64\% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{zamaniJointModelingOptimization2018,
  title = {Joint {{Modeling}} and {{Optimization}} of {{Search}} and {{Recommendation}}},
  author = {Zamani, Hamed and Croft, W. Bruce},
  year = {2018},
  month = jul,
  number = {arXiv:1807.05631},
  eprint = {1807.05631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.05631},
  urldate = {2022-08-31},
  abstract = {Despite the somewhat different techniques used in developing search engines and recommender systems, they both follow the same goal: helping people to get the information they need at the right time. Due to this common goal, search and recommendation models can potentially benefit from each other. The recent advances in neural network technologies make them effective and easily extendable for various tasks, including retrieval and recommendation. This raises the possibility of jointly modeling and optimizing search ranking and recommendation algorithms, with potential benefits to both. In this paper, we present theoretical and practical reasons to motivate joint modeling of search and recommendation as a research direction. We propose a general framework that simultaneously learns a retrieval model and a recommendation model by optimizing a joint loss function. Our preliminary results on a dataset of product data indicate that the proposed joint modeling substantially outperforms the retrieval and recommendation models trained independently. We list a number of future directions for this line of research that can potentially lead to development of state-of-the-art search and recommendation models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval}
}

@inproceedings{zamaniLearningJointSearch2020,
  title = {Learning a {{Joint Search}} and {{Recommendation Model}} from {{User-Item Interactions}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Zamani, Hamed and Croft, W. Bruce},
  year = {2020},
  month = jan,
  series = {{{WSDM}} '20},
  pages = {717--725},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3336191.3371818},
  urldate = {2022-08-29},
  abstract = {Existing learning to rank models for information retrieval are trained based on explicit or implicit query-document relevance information. In this paper, we study the task of learning a retrieval model based on user-item interactions. Our model has potential applications to the systems with rich user-item interaction data, such as browsing and recommendation, in which having an accurate search engine is desired. This includes media streaming services and e-commerce websites among others. Inspired by the neural approaches to collaborative filtering and the language modeling approaches to information retrieval, our model is jointly optimized to predict user-item interactions and reconstruct the item textual descriptions. In more details, our model learns user and item representations such that they can accurately predict future user-item interactions, while generating an effective unigram language model for each item. Our experiments on four diverse datasets in the context of movie and product search and recommendation demonstrate that our model substantially outperforms competitive retrieval baselines, in addition to providing comparable performance to state-of-the-art hybrid recommendation models.},
  isbn = {978-1-4503-6822-3},
  keywords = {collaborative filtering,joint search and recommendation,language models,representation learning}
}

@misc{zengEvaluatingLargeLanguage2023,
  title = {Evaluating {{Large Language Models}} at {{Evaluating Instruction Following}}},
  author = {Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07641},
  eprint = {2310.07641},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07641},
  urldate = {2023-10-12},
  abstract = {As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these "LLM evaluators", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{zhangActiveExampleSelection2022,
  title = {Active {{Example Selection}} for {{In-Context Learning}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Yiming and Feng, Shi and Tan, Chenhao},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {9134--9148},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.622},
  urldate = {2023-11-14},
  abstract = {With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8\% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.}
}

@misc{zhangAlpaCareInstructiontunedLarge2023,
  title = {{{AlpaCare}}:{{Instruction-tuned Large Language Models}} for {{Medical Application}}},
  shorttitle = {{{AlpaCare}}},
  author = {Zhang, Xinlu and Tian, Chenxin and Yang, Xianjun and Chen, Lichang and Li, Zekun and Petzold, Linda Ruth},
  year = {2023},
  month = oct,
  number = {arXiv:2310.14558},
  eprint = {2310.14558},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.14558},
  urldate = {2024-01-20},
  abstract = {Large Language Models (LLMs) have demonstrated significant enhancements in instruction-following abilities through instruction tuning, achieving notable performances across various tasks. Previous research has focused on fine-tuning medical domain-specific LLMs using an extensive array of medical-specific data, incorporating millions of pieces of biomedical literature to augment their medical capabilities. However, existing medical instruction-tuned LLMs have been constrained by the limited scope of tasks and instructions available, restricting the efficacy of instruction tuning and adversely affecting performance in the general domain. In this paper, we fine-tune LLaMA-series models using 52k diverse, machine-generated, medical instruction-following data, MedInstruct-52k, resulting in the model AlpaCare. Comprehensive experimental results on both general and medical-specific domain free-form instruction evaluations showcase AlpaCare's strong medical proficiency and generalizability compared to previous instruction-tuned models in both medical and general domains. We provide public access to our MedInstruct-52k dataset and a clinician-crafted free-form instruction test set, MedInstruct-test, along with our codebase, to foster further research and development. Our project page is available at https://github.com/XZhang97666/AlpaCare.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{zhangBioWordVecImprovingBiomedical2019,
  title = {{{BioWordVec}}, Improving Biomedical Word Embeddings with Subword Information and {{MeSH}}},
  author = {Zhang, Yijia and Chen, Qingyu and Yang, Zhihao and Lin, Hongfei and Lu, Zhiyong},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {52},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0055-0},
  urldate = {2021-11-30},
  langid = {english}
}

@inproceedings{zhangConversationalSearchRecommendation2018,
  title = {Towards {{Conversational Search}} and {{Recommendation}}: {{System Ask}}, {{User Respond}}},
  shorttitle = {Towards {{Conversational Search}} and {{Recommendation}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Zhang, Yongfeng and Chen, Xu and Ai, Qingyao and Yang, Liu and Croft, W. Bruce},
  year = {2018},
  month = oct,
  series = {{{CIKM}} '18},
  pages = {177--186},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3269206.3271776},
  urldate = {2022-08-29},
  abstract = {Conversational search and recommendation based on user-system dialogs exhibit major differences from conventional search and recommendation tasks in that 1) the user and system can interact for multiple semantically coherent rounds on a task through natural language dialog, and 2) it becomes possible for the system to understand the user needs or to help users clarify their needs by asking appropriate questions from the users directly. We believe the ability to ask questions so as to actively clarify the user needs is one of the most important advantages of conversational search and recommendation. In this paper, we propose and evaluate a unified conversational search/recommendation framework, in an attempt to make the research problem doable under a standard formalization. Specifically, we propose a System Ask -- User Respond (SAUR) paradigm for conversational search, define the major components of the paradigm, and design a unified implementation of the framework for product search and recommendation in e-commerce. To accomplish this, we propose the Multi-Memory Network (MMN) architecture, which can be trained based on large-scale collections of user reviews in e-commerce. The system is capable of asking aspect-based questions in the right order so as to understand the user needs, while (personalized) search is conducted during the conversation, and results are provided when the system feels confident. Experiments on real-world user purchasing data verified the advantages of conversational search and recommendation against conventional search and recommendation algorithms in terms of standard evaluation measures such as NDCG.},
  isbn = {978-1-4503-6014-2},
  keywords = {conversational recommendation,conversational search,dialog systems,memory networks,personalized agent,product search}
}

@inproceedings{zhangCroAnoCrowdAnnotation2021,
  title = {{{CroAno}} : {{A Crowd Annotation Platform}} for {{Improving Label Consistency}} of {{Chinese NER Dataset}}},
  shorttitle = {{{CroAno}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Zhang, Baoli and Li, Zhucong and Gan, Zhen and Chen, Yubo and Wan, Jing and Liu, Kang and Zhao, Jun and Liu, Shengping and Shi, Yafei},
  year = {2021},
  month = nov,
  pages = {275--282},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-demo.32},
  urldate = {2023-08-02},
  abstract = {In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and data management, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator: CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency Detector: CroAno employs a detector to locate corpus-level label inconsistency and provides users an interface to correct inconsistent entities in batches. 3) Prediction Error Analyzer: We deconstruct the entity prediction error of the model to six fine-grained entity error types. Users can employ this error system to detect corpus-level inconsistency from a model perspective. To validate the effectiveness of our platform, we use CroAno to revise two public datasets. In the two revised datasets, we get an improvement of +1.96\% and +2.57\% F1 respectively in model performance.}
}

@misc{zhangGraphlessNeuralNetworks2022,
  title = {Graph-Less {{Neural Networks}}: {{Teaching Old MLPs New Tricks}} via {{Distillation}}},
  shorttitle = {Graph-Less {{Neural Networks}}},
  author = {Zhang, Shichang and Liu, Yozen and Sun, Yizhou and Shah, Neil},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08727},
  eprint = {2110.08727},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.08727},
  urldate = {2022-11-16},
  abstract = {Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36\% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{zhangLanguageModelAnnotator2023,
  title = {Language {{Model}} as an {{Annotator}}: {{Unsupervised Context-aware Quality Phrase Generation}}},
  shorttitle = {Language {{Model}} as an {{Annotator}}},
  author = {Zhang, Zhihao and Zuo, Yuan and Lin, Chenghua and Wu, Junjie},
  year = {2023},
  month = dec,
  number = {arXiv:2312.17349},
  eprint = {2312.17349},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.17349},
  urldate = {2024-01-01},
  abstract = {Phrase mining is a fundamental text mining task that aims to identify quality phrases from context. Nevertheless, the scarcity of extensive gold labels datasets, demanding substantial annotation efforts from experts, renders this task exceptionally challenging. Furthermore, the emerging, infrequent, and domain-specific nature of quality phrases presents further challenges in dealing with this task. In this paper, we propose LMPhrase, a novel unsupervised context-aware quality phrase mining framework built upon large pre-trained language models (LMs). Specifically, we first mine quality phrases as silver labels by employing a parameter-free probing technique called Perturbed Masking on the pre-trained language model BERT (coined as Annotator). In contrast to typical statistic-based or distantly-supervised methods, our silver labels, derived from large pre-trained language models, take into account rich contextual information contained in the LMs. As a result, they bring distinct advantages in preserving informativeness, concordance, and completeness of quality phrases. Secondly, training a discriminative span prediction model heavily relies on massive annotated data and is likely to face the risk of overfitting silver labels. Alternatively, we formalize phrase tagging task as the sequence generation problem by directly fine-tuning on the Sequence-to-Sequence pre-trained language model BART with silver labels (coined as Generator). Finally, we merge the quality phrases from both the Annotator and Generator as the final predictions, considering their complementary nature and distinct characteristics. Extensive experiments show that our LMPhrase consistently outperforms all the existing competitors across two different granularity phrase mining tasks, where each task is tested on two different domain datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{zhangLatentPromptTuning2022,
  title = {Latent {{Prompt Tuning}} for {{Text Summarization}}},
  author = {Zhang, Yubo and Zhang, Xingxing and Wang, Xun and Chen, Si-qing and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2211.01837},
  eprint = {2211.01837},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.01837},
  urldate = {2023-03-13},
  abstract = {Prompts with different control signals (e.g., length, keywords, etc.) can be used to control text summarization. When control signals are available, they can control the properties of generated summaries and potentially improve summarization quality (since more information are given). Unfortunately, control signals are not already available during inference time. In this paper, we propose Lotus (shorthand for Latent Prompt Tuning for Summarization), which is a single model that can be applied in both controlled and uncontrolled (without control signals) modes. During training, Lotus learns latent prompt representations from prompts with gold control signals using a contrastive learning objective. Experiments show Lotus in uncontrolled mode consistently improves upon strong (uncontrollable) summarization models across four different summarization datasets. We also demonstrate generated summaries can be controlled using prompts with user specified control tokens.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{zhangLLMaAAMakingLarge2023,
  title = {{{LLMaAA}}: {{Making Large Language Models}} as {{Active Annotators}}},
  shorttitle = {{{LLMaAA}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Zhang, Ruoyu and Li, Yanzeng and Ma, Yongliang and Zhou, Ming and Zou, Lei},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {13088--13103},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.872},
  urldate = {2024-02-11},
  abstract = {Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.}
}

@misc{zhangMACSumControllableSummarization2022,
  title = {{{MACSum}}: {{Controllable Summarization}} with {{Mixed Attributes}}},
  shorttitle = {{{MACSum}}},
  author = {Zhang, Yusen and Liu, Yang and Yang, Ziyi and Fang, Yuwei and Chen, Yulong and Radev, Dragomir and Zhu, Chenguang and Zeng, Michael and Zhang, Rui},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05041},
  eprint = {2211.05041},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.05041},
  urldate = {2023-03-14},
  abstract = {Controllable summarization allows users to generate customized summaries with specified attributes. However, due to the lack of designated annotations of controlled summaries, existing works have to craft pseudo datasets by adapting generic summarization benchmarks. Furthermore, most research focuses on controlling single attributes individually (e.g., a short summary or a highly abstractive summary) rather than controlling a mix of attributes together (e.g., a short and highly abstractive summary). In this paper, we propose MACSum, the first human-annotated summarization dataset for controlling mixed attributes. It contains source texts from two domains, news articles and dialogues, with human-annotated summaries controlled by five designed attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We propose two simple and effective parameter-efficient approaches for the new task of mixed controllable summarization based on hard prompt tuning and soft prefix tuning. Results and analysis demonstrate that hard prompt models yield the best performance on all metrics and human evaluations. However, mixed-attribute control is still challenging for summarization tasks. Our dataset and code are available at https://github.com/psunlpgroup/MACSum.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{zhangMultimodalChainofThoughtReasoning2023,
  title = {Multimodal {{Chain-of-Thought Reasoning}} in {{Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00923},
  eprint = {2302.00923},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.00923},
  urldate = {2023-02-24},
  abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16 percentage points (75.17\%-{$>$}91.68\% accuracy) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available available at https://github.com/amazon-science/mm-cot.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{zhangOPTOpenPretrained2022,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2205.01068},
  urldate = {2022-07-27},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{zhangPEANUTHumanAICollaborative2023,
  title = {{{PEANUT}}: {{A Human-AI Collaborative Tool}} for {{Annotating Audio-Visual Data}}},
  shorttitle = {{{PEANUT}}},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Zhang, Zheng and Ning, Zheng and Xu, Chenliang and Tian, Yapeng and Li, Toby Jia-Jun},
  year = {2023},
  month = oct,
  pages = {1--18},
  publisher = {ACM},
  address = {San Francisco CA USA},
  doi = {10.1145/3586183.3606776},
  urldate = {2023-12-10},
  isbn = {9798400701320},
  langid = {english}
}

@inproceedings{zhangStoryBuddyHumanAICollaborative2022,
  title = {{{StoryBuddy}}: {{A Human-AI Collaborative Chatbot}} for {{Parent-Child Interactive Storytelling}} with {{Flexible Parental Involvement}}},
  shorttitle = {{{StoryBuddy}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Zheng and Xu, Ying and Wang, Yanhao and Yao, Bingsheng and Ritchie, Daniel and Wu, Tongshuang and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
  year = {2022},
  month = apr,
  eprint = {2202.06205},
  primaryclass = {cs},
  pages = {1--21},
  doi = {10.1145/3491102.3517479},
  urldate = {2023-01-18},
  abstract = {Despite its benefits for children's skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy's design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy's usability and suggested design insights for future parent-AI collaboration systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@inproceedings{zhangStoryBuddyHumanAICollaborative2022a,
  title = {{{StoryBuddy}}: {{A Human-AI Collaborative Chatbot}} for {{Parent-Child Interactive Storytelling}} with {{Flexible Parental Involvement}}},
  shorttitle = {{{StoryBuddy}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Zheng and Xu, Ying and Wang, Yanhao and Yao, Bingsheng and Ritchie, Daniel and Wu, Tongshuang and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
  year = {2022},
  month = apr,
  eprint = {2202.06205},
  primaryclass = {cs},
  pages = {1--21},
  doi = {10.1145/3491102.3517479},
  urldate = {2023-01-18},
  abstract = {Despite its benefits for children's skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy's design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy's usability and suggested design insights for future parent-AI collaboration systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@misc{zhaoEducationalQuestionGeneration2022,
  title = {Educational {{Question Generation}} of {{Children Storybooks}} via {{Question Type Distribution Learning}} and {{Event-Centric Summarization}}},
  author = {Zhao, Zhenjie and Hou, Yufang and Wang, Dakuo and Yu, Mo and Liu, Chengzhong and Ma, Xiaojuan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.14187},
  eprint = {2203.14187},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.14187},
  urldate = {2023-01-18},
  abstract = {Generating educational questions of fairytales or storybooks is vital for improving children's literacy ability. However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness. In this paper, we propose a novel question generation method that first learns the question type distribution of an input story paragraph, and then summarizes salient events which can be used to generate high-cognitive-demand questions. To train the event-centric summarizer, we finetune a pre-trained transformer-based sequence-to-sequence model using silver samples composed by educational question-answer pairs. On a newly proposed educational question answering dataset FairytaleQA, we show good performance of our method on both automatic and human evaluation metrics. Our work indicates the necessity of decomposing question type distribution learning and event-centric summary generation for educational question generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{zhaoOuroborosSpeculativeDecoding2024,
  title = {Ouroboros: {{Speculative Decoding}} with {{Large Model Enhanced Drafting}}},
  shorttitle = {Ouroboros},
  author = {Zhao, Weilin and Huang, Yuxiang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan and Sun, Maosong},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13720},
  eprint = {2402.13720},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.13720},
  urldate = {2024-02-26},
  abstract = {Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model. Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts. The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively. The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{zhaoPyHealthPythonLibrary2021a,
  title = {{{PyHealth}}: {{A Python Library}} for {{Health Predictive Models}}},
  shorttitle = {{{PyHealth}}},
  author = {Zhao, Yue and Qiao, Zhi and Xiao, Cao and Glass, Lucas and Sun, Jimeng},
  year = {2021},
  month = jan,
  number = {arXiv:2101.04209},
  eprint = {2101.04209},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.04209},
  urldate = {2024-01-14},
  abstract = {Despite the explosion of interest in healthcare AI research, the reproducibility and benchmarking of those research works are often limited due to the lack of standard benchmark datasets and diverse evaluation metrics. To address this reproducibility challenge, we develop PyHealth, an open-source Python toolbox for developing various predictive models on healthcare data. PyHealth consists of data preprocessing module, predictive modeling module, and evaluation module. The target users of PyHealth are both computer science researchers and healthcare data scientists. With PyHealth, they can conduct complex machine learning pipelines on healthcare datasets with fewer than ten lines of code. The data preprocessing module enables the transformation of complex healthcare datasets such as longitudinal electronic health records, medical images, continuous signals (e.g., electrocardiogram), and clinical notes into machine learning friendly formats. The predictive modeling module provides more than 30 machine learning models, including established ensemble trees and deep neural network-based approaches, via a unified but extendable API designed for both researchers and practitioners. The evaluation module provides various evaluation strategies (e.g., cross-validation and train-validation-test split) and predictive model metrics. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, and interactive examples are introduced in the library's development. PyHealth can be installed through the Python Package Index (PyPI) or https://github.com/yzhao062/PyHealth .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{zhaoPyTorchFSDPExperiences2023,
  title = {{{PyTorch FSDP}}: {{Experiences}} on {{Scaling Fully Sharded Data Parallel}}},
  shorttitle = {{{PyTorch FSDP}}},
  author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao, Yuchen and Mathews, Ajit and Li, Shen},
  year = {2023},
  month = sep,
  number = {arXiv:2304.11277},
  eprint = {2304.11277},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.11277},
  urldate = {2024-02-13},
  abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance}
}

@inproceedings{zhaoTalkPapersBringing2020,
  title = {Talk to {{Papers}}: {{Bringing Neural Question Answering}} to {{Academic Search}}},
  shorttitle = {Talk to {{Papers}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Zhao, Tiancheng and Lee, Kyusong},
  editor = {Celikyilmaz, Asli and Wen, Tsung-Hsien},
  year = {2020},
  month = jul,
  pages = {30--36},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-demos.5},
  urldate = {2024-02-04},
  abstract = {We introduce Talk to Papers, which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search. It's designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. We present a large improvement over classic search engine baseline on several standard QA datasets and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort.}
}

@article{zhengDGLKETrainingKnowledge2020,
  title = {{{DGL-KE}}: {{Training Knowledge Graph Embeddings}} at {{Scale}}},
  shorttitle = {{{DGL-KE}}},
  author = {Zheng, Da and Song, Xiang and Ma, Chao and Tan, Zeyuan and Ye, Zihao and Dong, Jin and Xiong, Hao and Zhang, Zheng and Karypis, George},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.08532 [cs]},
  eprint = {2004.08532},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2004.08532},
  urldate = {2022-01-25},
  abstract = {Knowledge graphs have emerged as a key abstraction for organizing information in diverse domains and their embeddings are increasingly used to harness their information in various information retrieval and machine learning tasks. However, the ever growing size of knowledge graphs requires computationally efficient algorithms capable of scaling to graphs with millions of nodes and billions of edges. This paper presents DGL-KE, an open-source package to efficiently compute knowledge graph embeddings. DGL-KE introduces various novel optimizations that accelerate training on knowledge graphs with millions of nodes and billions of edges using multi-processing, multi-GPU, and distributed parallelism. These optimizations are designed to increase data locality, reduce communication overhead, overlap computations with memory accesses, and achieve high operation efficiency. Experiments on knowledge graphs consisting of over 86M nodes and 338M edges show that DGL-KE can compute embeddings in 100 minutes on a EC2 instance with 8 GPUs and 30 minutes on an EC2 cluster with 4 machines with 48 cores/machine. These results represent a 2{\texttimes} {$\sim$} 5{\texttimes} speedup over the best competing approaches. DGL-KE is available on https://github.com/awslabs/dgl-ke.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing}
}

@misc{zhengTopicGuidedAbstractiveText2021,
  title = {Topic-{{Guided Abstractive Text Summarization}}: A {{Joint Learning Approach}}},
  shorttitle = {Topic-{{Guided Abstractive Text Summarization}}},
  author = {Zheng, Chujie and Zhang, Kunpeng and Wang, Harry Jiannan and Fan, Ling and Wang, Zhe},
  year = {2021},
  month = aug,
  number = {arXiv:2010.10323},
  eprint = {2010.10323},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.10323},
  urldate = {2022-07-20},
  abstract = {We introduce a new approach for abstractive text summarization, Topic-Guided Abstractive Summarization, which calibrates long-range dependencies from topic-level features with globally salient content. The idea is to incorporate neural topic modeling with a Transformer-based sequence-to-sequence (seq2seq) model in a joint learning framework. This design can learn and preserve the global semantics of the document, which can provide additional contextual guidance for capturing important ideas of the document, thereby enhancing the generation of summary. We conduct extensive experiments on two datasets and the results show that our proposed model outperforms many extractive and abstractive systems in terms of both ROUGE measurements and human evaluation. Our code is available at: https://github.com/chz816/tas.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{zhengWhenDoesPretraining2021a,
  title = {When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the {{CaseHOLD}} Dataset of 53,000+ Legal Holdings},
  shorttitle = {When Does Pretraining Help?},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Law}}},
  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R. and Henderson, Peter and Ho, Daniel E.},
  year = {2021},
  month = jul,
  series = {{{ICAIL}} '21},
  pages = {159--168},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3462757.3466088},
  urldate = {2023-12-15},
  abstract = {While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case {$<$}u{$>$}H{$<$}/u{$>$}oldings {$<$}u{$>$}O{$<$}/u{$>$}n {$<$}u{$>$}L{$<$}/u{$>$}egal {$<$}u{$>$}D{$<$}/u{$>$}ecisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (on a corpus of {$\approx$}3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2\% on F1, representing a 12\% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage in resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.},
  isbn = {978-1-4503-8526-8},
  keywords = {benchmark dataset,law,natural language processing,pretraining}
}

@misc{zhongQMSumNewBenchmark2021,
  title = {{{QMSum}}: {{A New Benchmark}} for {{Query-based Multi-domain Meeting Summarization}}},
  shorttitle = {{{QMSum}}},
  author = {Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and Radev, Dragomir},
  year = {2021},
  month = apr,
  number = {arXiv:2104.05938},
  eprint = {2104.05938},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.05938},
  urldate = {2022-07-27},
  abstract = {Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at {\textbackslash}url\{https://github.com/Yale-LILY/QMSum\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{zhouLeveragingLargeLanguage2023,
  title = {Leveraging {{Large Language Models}} for {{Enhanced Product Descriptions}} in {{eCommerce}}},
  author = {Zhou, Jianghong and Liu, Bo and Hong, Jhalak Nilesh Acharya Yao and Lee, Kuang-chih and Wen, Musen},
  year = {2023},
  month = oct,
  number = {arXiv:2310.18357},
  eprint = {2310.18357},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.18357},
  urldate = {2024-01-20},
  abstract = {In the dynamic field of eCommerce, the quality and comprehensiveness of product descriptions are pivotal for enhancing search visibility and customer engagement. Effective product descriptions can address the 'cold start' problem, align with market trends, and ultimately lead to increased click-through rates. Traditional methods for crafting these descriptions often involve significant human effort and may lack both consistency and scalability. This paper introduces a novel methodology for automating product description generation using the LLAMA 2.0 7B language model. We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms. The model is then fine-tuned for domain-specific language features and eCommerce nuances to enhance its utility in sales and user engagement. We employ multiple evaluation metrics, including NDCG, customer click-through rates, and human assessments, to validate the effectiveness of our approach. Our findings reveal that the system is not only scalable but also significantly reduces the human workload involved in creating product descriptions. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platforms, offering significant business impact, including improved search functionality and increased sales.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,H.3.3,H.3.5,I.2.7,K.4.4}
}

@misc{zhouLIMALessMore2023,
  title = {{{LIMA}}: {{Less Is More}} for {{Alignment}}},
  shorttitle = {{{LIMA}}},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  year = {2023},
  month = may,
  number = {arXiv:2305.11206},
  eprint = {2305.11206},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.11206},
  urldate = {2024-01-23},
  abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{zhouTransformerbasedRepresentationlearningModel2023,
  title = {A Transformer-Based Representation-Learning Model with Unified Processing of Multimodal Input for Clinical Diagnostics},
  author = {Zhou, Hong-Yu and Yu, Yizhou and Wang, Chengdi and Zhang, Shu and Gao, Yuanxu and Pan, Jia and Shao, Jun and Lu, Guangming and Zhang, Kang and Li, Weimin},
  year = {2023},
  month = jun,
  journal = {Nature Biomedical Engineering},
  volume = {7},
  number = {6},
  pages = {743--755},
  publisher = {Nature Publishing Group},
  issn = {2157-846X},
  doi = {10.1038/s41551-023-01045-x},
  urldate = {2024-01-25},
  abstract = {During the diagnostic process, clinicians leverage multimodal information, such as the chief complaint, medical images and laboratory test results. Deep-learning models for aiding diagnosis have yet to meet this requirement of leveraging multimodal information. Here we report a transformer-based representation-learning model as a clinical diagnostic aid that processes multimodal input in a unified manner. Rather than learning modality-specific features, the model leverages embedding layers to convert images and unstructured and structured text into visual tokens and text tokens, and uses bidirectional blocks with intramodal and intermodal attention to learn holistic representations of radiographs, the unstructured chief complaint and clinical history, and structured clinical information such as laboratory test results and patient demographic information. The unified model outperformed an image-only model and non-unified multimodal diagnosis models in the identification of pulmonary disease (by 12\% and 9\%, respectively) and in the prediction of adverse clinical outcomes in patients with COVID-19 (by 29\% and 7\%, respectively). Unified multimodal transformer-based models may help streamline the triaging of patients and facilitate the clinical decision-making process.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Biomedical engineering,Health care}
}

@article{zhuCalibrationLargeLanguage2023,
  title = {On the {{Calibration}} of {{Large Language Models}} and {{Alignment}}},
  author = {Zhu, Chiwei and Xu, Benfeng and Wang, Quan and Zhang, Yongdong and Mao, Zhendong},
  year = {2023},
  month = nov,
  journal = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  eprint = {2311.13240},
  primaryclass = {cs},
  pages = {9778--9795},
  doi = {10.18653/v1/2023.findings-emnlp.654},
  urldate = {2023-12-20},
  abstract = {As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{zhuJudgeLMFinetunedLarge2023,
  title = {{{JudgeLM}}: {{Fine-tuned Large Language Models}} Are {{Scalable Judges}}},
  shorttitle = {{{JudgeLM}}},
  author = {Zhu, Lianghui and Wang, Xinggang and Wang, Xinlong},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17631},
  eprint = {2310.17631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.17631},
  urldate = {2023-10-30},
  abstract = {Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90\% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{zhuPromptingLargeLanguage2024,
  title = {Prompting {{Large Language Models}} for {{Zero-Shot Clinical Prediction}} with {{Structured Longitudinal Electronic Health Record Data}}},
  author = {Zhu, Yinghao and Wang, Zixiang and Gao, Junyi and Tong, Yuning and An, Jingkun and Liao, Weibin and Harrison, Ewen M. and Ma, Liantao and Pan, Chengwei},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01713},
  eprint = {2402.01713},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01713},
  urldate = {2024-02-16},
  abstract = {The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with our elaborately designed prompting framework, LLMs can improve prediction performance in key tasks such as mortality, length-of-stay, and 30-day readmission by about 35{\textbackslash}\%, surpassing ML models in few-shot settings. Our research underscores the potential of LLMs in enhancing clinical decision-making, especially in urgent healthcare situations like the outbreak of emerging diseases with no labeled data. The code is publicly available at https://github.com/yhzhu99/llm4healthcare for reproducibility.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{zhuSurveyModelCompression2023,
  title = {A {{Survey}} on {{Model Compression}} for {{Large Language Models}}},
  author = {Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  year = {2023},
  month = sep,
  number = {arXiv:2308.07633},
  eprint = {2308.07633},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07633},
  urldate = {2024-02-13},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{zhuTransformingWikipediaAugmented2022,
  title = {Transforming {{Wikipedia}} into {{Augmented Data}} for {{Query-Focused Summarization}}},
  author = {Zhu, Haichao and Dong, Li and Wei, Furu and Qin, Bing and Liu, Ting},
  year = {2022},
  month = jul,
  number = {arXiv:1911.03324},
  eprint = {1911.03324},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1911.03324},
  urldate = {2022-07-27},
  abstract = {The limited size of existing query-focused summarization datasets renders training data-driven summarization models challenging. Meanwhile, the manual construction of a query-focused summarization corpus is costly and time-consuming. In this paper, we use Wikipedia to automatically collect a large query-focused summarization dataset (named WIKIREF) of more than 280, 000 examples, which can serve as a means of data augmentation. We also develop a BERT-based query-focused summarization model (Q-BERT) to extract sentences from the documents as summaries. To better adapt a huge model containing millions of parameters to tiny benchmarks, we identify and fine-tune only a sparse subnetwork, which corresponds to a small fraction of the whole model parameters. Experimental results on three DUC benchmarks show that the model pre-trained on WIKIREF has already achieved reasonable performance. After fine-tuning on the specific benchmark datasets, the model with data augmentation outperforms strong comparison systems. Moreover, both our proposed Q-BERT model and subnetwork fine-tuning further improve the model performance. The dataset is publicly available at https://aka.ms/wikiref.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language}
}
